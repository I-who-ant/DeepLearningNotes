"""
ç¤ºä¾‹2: æ·»åŠ éªŒè¯é›†å’Œæ¨¡å‹è¯„ä¼°

åœ¨ç¤ºä¾‹1çš„åŸºç¡€ä¸Šæ·»åŠ :
- è®­ç»ƒé›†/éªŒè¯é›†åˆ’åˆ†
- éªŒè¯é›†å‡†ç¡®ç‡è¯„ä¼°
- model.train() å’Œ model.eval() æ¨¡å¼åˆ‡æ¢
- TensorBoard å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
- è¿‡æ‹Ÿåˆæ£€æµ‹

è¿è¡Œ: python src/experiments/model_train/02_with_validation.py
æŸ¥çœ‹TensorBoard: tensorboard --logdir=/home/seeback/PycharmProjects/DeepLearning/tensor_board
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, transforms
import os


# ============================================================
# 1. å®šä¹‰ç›¸åŒçš„CNNæ¨¡å‹ (å¤ç”¨ç¤ºä¾‹1çš„æ¨¡å‹)
# ============================================================
class SimpleCNN(nn.Module):
    """ç®€å•çš„3å±‚å·ç§¯ç¥ç»ç½‘ç»œ"""
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)      # 32x32x3 -> 32x32x32
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)     # 16x16x32 -> 16x16x64
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)    # 8x8x64 -> 8x8x128
        self.pool = nn.MaxPool2d(2, 2)                   # ä¸‹é‡‡æ ·
        self.fc1 = nn.Linear(128 * 4 * 4, 256)           # å…¨è¿æ¥å±‚
        self.fc2 = nn.Linear(256, 10)                    # è¾“å‡ºå±‚ (10ç±»)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))    # -> 16x16x32
        x = self.pool(self.relu(self.conv2(x)))    # -> 8x8x64
        x = self.pool(self.relu(self.conv3(x)))    # -> 4x4x128
        x = x.view(x.size(0), -1)                  # å±•å¹³
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x


# ============================================================
# 2. å‡†å¤‡æ•°æ® - æ–°å¢: è®­ç»ƒé›†/éªŒè¯é›†åˆ’åˆ†
# ============================================================
def prepare_data(val_ratio=0.2):
    """
    åŠ è½½CIFAR-10æ•°æ®é›†å¹¶åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†

    å‚æ•°:
        val_ratio: éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤20%)

    è¿”å›:
        train_loader: è®­ç»ƒé›†DataLoader
        val_loader: éªŒè¯é›†DataLoader
    """
    print("æ­£åœ¨åŠ è½½ CIFAR-10 æ•°æ®é›†...")

    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    # åŠ è½½å®Œæ•´çš„è®­ç»ƒé›†
    full_train_dataset = datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )

    # è®¡ç®—è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„å¤§å°
    total_size = len(full_train_dataset)
    val_size = int(total_size * val_ratio)
    train_size = total_size - val_size

    # éšæœºåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_dataset, val_dataset = random_split(
        full_train_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)  # å›ºå®šéšæœºç§å­,ä¿è¯å¯å¤ç°
    )

    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=64,
        shuffle=True,
        num_workers=2
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=64,
        shuffle=False,  # éªŒè¯é›†ä¸éœ€è¦æ‰“ä¹±
        num_workers=2
    )

    print(f"è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡ ({len(train_loader)} æ‰¹)")
    print(f"éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡ ({len(val_loader)} æ‰¹)")

    return train_loader, val_loader


# ============================================================
# 3. è®­ç»ƒå‡½æ•° - æ–°å¢: è¿”å›å¹³å‡loss
# ============================================================
def train_one_epoch(model, train_loader, criterion, optimizer, epoch, writer):
    """
    è®­ç»ƒä¸€ä¸ªepoch

    å‚æ•°:
        model: æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        epoch: å½“å‰epoch
        writer: TensorBoard writer

    è¿”å›:
        avg_loss: å¹³å‡è®­ç»ƒæŸå¤±
    """
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ (é‡è¦! å¯ç”¨dropoutå’ŒBN)

    running_loss = 0.0
    total_samples = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        # ========================================
        # è®­ç»ƒçš„4ä¸ªå…³é”®æ­¥éª¤
        # ========================================
        # æ­¥éª¤1: å‰å‘ä¼ æ’­
        outputs = model(images)
        loss = criterion(outputs, labels)

        # æ­¥éª¤2: æ¸…ç©ºæ¢¯åº¦
        optimizer.zero_grad()

        # æ­¥éª¤3: åå‘ä¼ æ’­
        loss.backward()

        # æ­¥éª¤4: æ›´æ–°å‚æ•°
        optimizer.step()

        # ========================================

        # ç´¯è®¡loss
        running_loss += loss.item() * images.size(0)
        total_samples += images.size(0)

        # æ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡
        if (batch_idx + 1) % 100 == 0:
            avg_loss = running_loss / total_samples
            print(f"  Batch [{batch_idx+1}/{len(train_loader)}] Loss: {avg_loss:.4f}")

            # å†™å…¥TensorBoard
            global_step = epoch * len(train_loader) + batch_idx
            writer.add_scalar('Train/Batch_Loss', loss.item(), global_step)

    # è®¡ç®—æ•´ä¸ªepochçš„å¹³å‡loss
    avg_loss = running_loss / total_samples
    return avg_loss


# ============================================================
# 4. éªŒè¯å‡½æ•° - æ–°å¢!
# ============================================================
def validate(model, val_loader, criterion):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹

    å‚æ•°:
        model: æ¨¡å‹
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°

    è¿”å›:
        avg_loss: å¹³å‡éªŒè¯æŸå¤±
        accuracy: éªŒè¯å‡†ç¡®ç‡
    """
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ (é‡è¦! å…³é—­dropoutå’ŒBN)

    running_loss = 0.0
    correct = 0
    total = 0

    # éªŒè¯æ—¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦
    with torch.no_grad():
        for images, labels in val_loader:
            # å‰å‘ä¼ æ’­
            outputs = model(images)
            loss = criterion(outputs, labels)

            # ç´¯è®¡loss
            running_loss += loss.item() * images.size(0)

            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs, 1)  # è·å–é¢„æµ‹ç±»åˆ« (dim=1 è¡¨ç¤ºæŒ‰è¡Œå–æœ€å¤§å€¼)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_loss = running_loss / total
    accuracy = 100 * correct / total

    return avg_loss, accuracy


# ============================================================
# 5. ä¸»è®­ç»ƒå¾ªç¯
# ============================================================
def train(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):
    """
    å®Œæ•´çš„è®­ç»ƒ+éªŒè¯å¾ªç¯

    å‚æ•°:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        num_epochs: è®­ç»ƒè½®æ•°
    """
    print("\nå¼€å§‹è®­ç»ƒ...")
    print("=" * 70)

    # åˆ›å»ºTensorBoard writer
    log_dir = '/home/seeback/PycharmProjects/DeepLearning/tensor_board/02_with_validation'
    os.makedirs(log_dir, exist_ok=True)
    writer = SummaryWriter(log_dir)
    print(f"TensorBoard æ—¥å¿—ä¿å­˜åˆ°: {log_dir}")
    print(f"æŸ¥çœ‹å‘½ä»¤: tensorboard --logdir=/home/seeback/PycharmProjects/DeepLearning/tensor_board")
    print("=" * 70)

    best_val_acc = 0.0
    train_losses = []
    val_losses = []
    val_accuracies = []

    # å¤–å±‚å¾ªç¯: éå†æ¯ä¸ªepoch
    for epoch in range(num_epochs):
        print(f"\nEpoch [{epoch+1}/{num_epochs}]")
        print("-" * 70)

        # è®­ç»ƒé˜¶æ®µ
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, epoch, writer)
        train_losses.append(train_loss)

        # éªŒè¯é˜¶æ®µ
        val_loss, val_acc = validate(model, val_loader, criterion)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)

        # æ‰“å°æœ¬epochçš„ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š Epoch [{epoch+1}/{num_epochs}] æ€»ç»“:")
        print(f"  è®­ç»ƒLoss: {train_loss:.4f}")
        print(f"  éªŒè¯Loss: {val_loss:.4f}")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")

        # å†™å…¥TensorBoard
        writer.add_scalar('Loss/Train', train_loss, epoch)
        writer.add_scalar('Loss/Validation', val_loss, epoch)
        writer.add_scalar('Accuracy/Validation', val_acc, epoch)

        # åŒæ—¶ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯losså¯¹æ¯”
        writer.add_scalars('Loss/Comparison', {
            'Train': train_loss,
            'Validation': val_loss
        }, epoch)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            print(f"  â­ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡! ä¿å­˜æ¨¡å‹...")

        # æ£€æµ‹è¿‡æ‹Ÿåˆ
        if epoch > 0 and val_loss > train_loss * 1.2:
            print(f"  âš ï¸ è­¦å‘Š: å¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆ (éªŒè¯lossæ˜æ˜¾é«˜äºè®­ç»ƒloss)")

        print("-" * 70)

    writer.close()
    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")


# ============================================================
# 6. ä¸»å‡½æ•°
# ============================================================
def main():
    """ä¸»å‡½æ•°:ä¸²è”æ‰€æœ‰æ­¥éª¤"""
    print("=" * 70)
    print("ç¤ºä¾‹2: æ·»åŠ éªŒè¯é›†å’Œæ¨¡å‹è¯„ä¼°")
    print("=" * 70)

    # æ­¥éª¤1: å‡†å¤‡æ•°æ® (æ–°å¢: è®­ç»ƒ/éªŒè¯åˆ’åˆ†)
    train_loader, val_loader = prepare_data(val_ratio=0.2)

    # æ­¥éª¤2: åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºæ¨¡å‹...")
    model = SimpleCNN()

    # è®¡ç®—æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}")

    # æ­¥éª¤3: å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(
        model.parameters(),
        lr=0.01,
        momentum=0.9
    )

    print(f"æŸå¤±å‡½æ•°: CrossEntropyLoss")
    print(f"ä¼˜åŒ–å™¨: SGD (lr=0.01, momentum=0.9)")

    # æ­¥éª¤4: è®­ç»ƒæ¨¡å‹
    train(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)

    print("\n" + "=" * 70)
    print("âœ… è®­ç»ƒç¤ºä¾‹å®Œæˆ!")
    print("=" * 70)
    print("\nğŸ’¡ æ–°å¢å†…å®¹:")
    print("  1. âœ… è®­ç»ƒé›†/éªŒè¯é›†åˆ’åˆ† (80%/20%)")
    print("  2. âœ… model.train() å’Œ model.eval() æ¨¡å¼åˆ‡æ¢")
    print("  3. âœ… éªŒè¯é›†å‡†ç¡®ç‡è®¡ç®—")
    print("  4. âœ… TensorBoard å¯è§†åŒ–")
    print("  5. âœ… è¿‡æ‹Ÿåˆæ£€æµ‹")
    print("\nğŸ“Š æŸ¥çœ‹è®­ç»ƒå¯è§†åŒ–:")
    print("  tensorboard --logdir=/home/seeback/PycharmProjects/DeepLearning/tensor_board")
    print("=" * 70)


if __name__ == '__main__':
    main()
