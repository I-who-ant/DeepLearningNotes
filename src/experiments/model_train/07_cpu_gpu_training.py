"""
ç¤ºä¾‹7: CPU/GPUè®­ç»ƒè¯¦è§£

è¯¦ç»†è¯´æ˜:
- å¦‚ä½•æ£€æµ‹GPUæ˜¯å¦å¯ç”¨
- å¦‚ä½•å°†æ¨¡å‹å’Œæ•°æ®ç§»åŠ¨åˆ°GPU/CPU
- CPU vs GPUè®­ç»ƒé€Ÿåº¦å¯¹æ¯”
- å¤šGPUè®­ç»ƒåŸºç¡€
- æ··åˆç²¾åº¦è®­ç»ƒ (AMP)
- æœ€ä½³å®è·µå’Œå¸¸è§é”™è¯¯

è¿è¡Œ: python src/experiments/model_train/07_cpu_gpu_training.py
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import time
import os


# ============================================================
# 1. æ£€æµ‹å’Œé€‰æ‹©è®¾å¤‡
# ============================================================
def get_device(prefer_gpu=True):
    """
    æ™ºèƒ½é€‰æ‹©è®­ç»ƒè®¾å¤‡

    å‚æ•°:
        prefer_gpu: å¦‚æœGPUå¯ç”¨,æ˜¯å¦ä¼˜å…ˆä½¿ç”¨GPU

    è¿”å›:
        device: torch.deviceå¯¹è±¡
    """
    if prefer_gpu and torch.cuda.is_available():
        device = torch.device('cuda')
        print("=" * 70)
        print("ğŸ® GPU ä¿¡æ¯")
        print("=" * 70)
        print(f"âœ… æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU æ•°é‡: {torch.cuda.device_count()}")
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")

        # æ˜¾ç¤ºGPUå†…å­˜ä¿¡æ¯
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"GPU æ€»å†…å­˜: {total_memory:.2f} GB")

        print("=" * 70)
    else:
        device = torch.device('cpu')
        print("=" * 70)
        print("ğŸ’» CPU ä¿¡æ¯")
        print("=" * 70)
        if not torch.cuda.is_available():
            print("âš ï¸ æœªæ£€æµ‹åˆ° GPU,ä½¿ç”¨ CPU è®­ç»ƒ")
            print("ğŸ’¡ æç¤º: CPUè®­ç»ƒä¼šæ¯”GPUæ…¢å¾ˆå¤š")
        else:
            print("â„¹ï¸ GPU å¯ç”¨ä½†é€‰æ‹©ä½¿ç”¨ CPU")

        print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"CPU çº¿ç¨‹æ•°: {torch.get_num_threads()}")
        print("=" * 70)

    return device


# ============================================================
# 2. å®šä¹‰CNNæ¨¡å‹
# ============================================================
class SimpleCNN(nn.Module):
    """ç®€å•çš„3å±‚å·ç§¯ç¥ç»ç½‘ç»œ"""
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(128 * 4 * 4, 256)
        self.fc2 = nn.Linear(256, 10)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = self.pool(self.relu(self.conv3(x)))
        x = x.view(x.size(0), -1)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x


# ============================================================
# 3. å‡†å¤‡æ•°æ®
# ============================================================
def prepare_data():
    """åŠ è½½CIFAR-10æ•°æ®é›†"""
    print("\næ­£åœ¨åŠ è½½ CIFAR-10 æ•°æ®é›†...")

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    train_dataset = datasets.CIFAR10(
        root='./data', train=True, download=True, transform=transform
    )

    # åªä½¿ç”¨ä¸€éƒ¨åˆ†æ•°æ®æ¥å¿«é€Ÿæ¼”ç¤º
    train_subset, _ = random_split(
        train_dataset, [5000, 45000],
        generator=torch.Generator().manual_seed(42)
    )

    train_loader = DataLoader(
        train_subset, batch_size=128, shuffle=True, num_workers=2
    )

    print(f"è®­ç»ƒé›†: {len(train_subset)} å¼ å›¾ç‰‡")

    return train_loader


# ============================================================
# 4. è®­ç»ƒå‡½æ•° (æ­£ç¡®çš„CPU/GPUå¤„ç†)
# ============================================================
def train_with_device(model, train_loader, criterion, optimizer, device, num_epochs=3):
    """
    åœ¨æŒ‡å®šè®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹

    å‚æ•°:
        model: æ¨¡å‹
        train_loader: æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        device: è®­ç»ƒè®¾å¤‡ (cpu æˆ– cuda)
        num_epochs: è®­ç»ƒè½®æ•°

    è¿”å›:
        avg_time_per_epoch: æ¯ä¸ªepochçš„å¹³å‡æ—¶é—´
    """
    print(f"\nå¼€å§‹åœ¨ {device} ä¸Šè®­ç»ƒ...")
    print("=" * 70)

    # ========================================
    # å…³é”®æ­¥éª¤1: å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    # ========================================
    model = model.to(device)
    print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° {device}")

    total_time = 0

    for epoch in range(num_epochs):
        model.train()
        epoch_start = time.time()

        running_loss = 0.0

        for batch_idx, (images, labels) in enumerate(train_loader):
            # ========================================
            # å…³é”®æ­¥éª¤2: å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            # ========================================
            images = images.to(device)  # æ¨èä½¿ç”¨ .to(device)
            labels = labels.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(images)
            loss = criterion(outputs, labels)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            # æ¯20ä¸ªbatchæ‰“å°ä¸€æ¬¡
            if (batch_idx + 1) % 20 == 0:
                avg_loss = running_loss / 20
                print(f"  Epoch [{epoch+1}/{num_epochs}] "
                      f"Batch [{batch_idx+1}/{len(train_loader)}] "
                      f"Loss: {avg_loss:.4f}")
                running_loss = 0.0

        epoch_time = time.time() - epoch_start
        total_time += epoch_time

        print(f"  Epoch {epoch+1} å®Œæˆ,è€—æ—¶: {epoch_time:.2f} ç§’")
        print("-" * 70)

    avg_time = total_time / num_epochs
    print(f"âœ… è®­ç»ƒå®Œæˆ! å¹³å‡æ¯ä¸ªepoch: {avg_time:.2f} ç§’")

    return avg_time


# ============================================================
# 5. CPU vs GPU é€Ÿåº¦å¯¹æ¯”
# ============================================================
def compare_cpu_gpu():
    """å¯¹æ¯”CPUå’ŒGPUçš„è®­ç»ƒé€Ÿåº¦"""
    print("\n" + "=" * 70)
    print("âš¡ CPU vs GPU è®­ç»ƒé€Ÿåº¦å¯¹æ¯”")
    print("=" * 70)

    # å‡†å¤‡æ•°æ®
    train_loader = prepare_data()
    criterion = nn.CrossEntropyLoss()

    results = {}

    # æµ‹è¯•CPU
    print("\n" + "="*70)
    print("æµ‹è¯• CPU è®­ç»ƒé€Ÿåº¦")
    print("="*70)

    model_cpu = SimpleCNN()
    optimizer_cpu = optim.SGD(model_cpu.parameters(), lr=0.01, momentum=0.9)
    device_cpu = torch.device('cpu')

    cpu_time = train_with_device(
        model_cpu, train_loader, criterion, optimizer_cpu,
        device_cpu, num_epochs=2
    )
    results['CPU'] = cpu_time

    # æµ‹è¯•GPU (å¦‚æœå¯ç”¨)
    if torch.cuda.is_available():
        print("\n" + "="*70)
        print("æµ‹è¯• GPU è®­ç»ƒé€Ÿåº¦")
        print("="*70)

        model_gpu = SimpleCNN()
        optimizer_gpu = optim.SGD(model_gpu.parameters(), lr=0.01, momentum=0.9)
        device_gpu = torch.device('cuda')

        # æ¸…ç©ºGPUç¼“å­˜
        torch.cuda.empty_cache()

        gpu_time = train_with_device(
            model_gpu, train_loader, criterion, optimizer_gpu,
            device_gpu, num_epochs=2
        )
        results['GPU'] = gpu_time

        # æ‰“å°å¯¹æ¯”ç»“æœ
        print("\n" + "=" * 70)
        print("ğŸ“Š é€Ÿåº¦å¯¹æ¯”ç»“æœ")
        print("=" * 70)
        print(f"\nCPU å¹³å‡æ¯ä¸ªepoch: {cpu_time:.2f} ç§’")
        print(f"GPU å¹³å‡æ¯ä¸ªepoch: {gpu_time:.2f} ç§’")
        print(f"\nğŸš€ GPU æ¯” CPU å¿«: {cpu_time / gpu_time:.2f} å€")
        print("=" * 70)
    else:
        print("\nâš ï¸ GPU ä¸å¯ç”¨,æ— æ³•è¿›è¡Œå¯¹æ¯”æµ‹è¯•")
        print("=" * 70)


# ============================================================
# 6. å¸¸è§é”™è¯¯ç¤ºä¾‹
# ============================================================
def common_mistakes():
    """æ¼”ç¤ºå¸¸è§çš„CPU/GPUé”™è¯¯"""
    print("\n" + "=" * 70)
    print("âš ï¸ å¸¸è§é”™è¯¯ç¤ºä¾‹")
    print("=" * 70)

    model = SimpleCNN()

    print("\né”™è¯¯1: æ¨¡å‹åœ¨CPU,æ•°æ®åœ¨GPU (æˆ–ç›¸å)")
    print("-" * 70)
    print("ä»£ç ç¤ºä¾‹:")
    print("""
    model = SimpleCNN()  # æ¨¡å‹åœ¨CPU
    images = images.cuda()  # æ•°æ®åœ¨GPU
    outputs = model(images)  # âŒ é”™è¯¯! RuntimeError
    """)
    print("é”™è¯¯ä¿¡æ¯: RuntimeError: Expected all tensors to be on the same device")

    print("\næ­£ç¡®åšæ³•:")
    print("""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)  # æ¨¡å‹ç§»åˆ°è®¾å¤‡
    images = images.to(device)  # æ•°æ®ç§»åˆ°è®¾å¤‡
    outputs = model(images)  # âœ… æ­£ç¡®!
    """)

    print("\né”™è¯¯2: ç›´æ¥ä½¿ç”¨ .cuda() è€Œä¸æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨")
    print("-" * 70)
    print("ä»£ç ç¤ºä¾‹:")
    print("""
    model = model.cuda()  # âŒ å¦‚æœæ²¡æœ‰GPUä¼šæŠ¥é”™
    images = images.cuda()  # âŒ RuntimeError: CUDA not available
    """)

    print("\næ­£ç¡®åšæ³•:")
    print("""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)  # âœ… è‡ªåŠ¨é€‚é…
    images = images.to(device)  # âœ… è‡ªåŠ¨é€‚é…
    """)

    print("\né”™è¯¯3: å¿˜è®°å°†æŸå¤±å‡½æ•°çš„è¾“å…¥ç§»åˆ°GPU")
    print("-" * 70)
    print("ä»£ç ç¤ºä¾‹:")
    print("""
    model = model.cuda()
    images = images.cuda()
    labels = labels  # âŒ å¿˜è®°ç§»åˆ°GPU
    outputs = model(images)
    loss = criterion(outputs, labels)  # âŒ é”™è¯¯!
    """)

    print("\næ­£ç¡®åšæ³•:")
    print("""
    model = model.to(device)
    images = images.to(device)
    labels = labels.to(device)  # âœ… æ ‡ç­¾ä¹Ÿè¦ç§»åˆ°è®¾å¤‡
    outputs = model(images)
    loss = criterion(outputs, labels)  # âœ… æ­£ç¡®!
    """)


# ============================================================
# 7. æœ€ä½³å®è·µ
# ============================================================
def best_practices():
    """CPU/GPUä½¿ç”¨çš„æœ€ä½³å®è·µ"""
    print("\n" + "=" * 70)
    print("ğŸ’¡ CPU/GPU ä½¿ç”¨æœ€ä½³å®è·µ")
    print("=" * 70)

    print("""
1. å§‹ç»ˆä½¿ç”¨ device å¯¹è±¡
   âœ… æ¨è:
      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
      model = model.to(device)
      data = data.to(device)

   âŒ ä¸æ¨è:
      model = model.cuda()  # ä¸å¤Ÿçµæ´»
      data = data.cuda()

2. åœ¨è®­ç»ƒå¼€å§‹æ—¶ç§»åŠ¨æ¨¡å‹,åœ¨å¾ªç¯å†…ç§»åŠ¨æ•°æ®
   âœ… æ¨è:
      model = model.to(device)  # åªç§»åŠ¨ä¸€æ¬¡
      for images, labels in loader:
          images = images.to(device)  # æ¯ä¸ªbatchç§»åŠ¨
          labels = labels.to(device)

3. ä½¿ç”¨ .to(device, non_blocking=True) åŠ é€Ÿæ•°æ®ä¼ è¾“
   âœ… æ¨è:
      images = images.to(device, non_blocking=True)
      # å…è®¸CPUå’ŒGPUå¼‚æ­¥æ‰§è¡Œ

4. GPUè®­ç»ƒå®Œæˆå,å°†ç»“æœç§»å›CPU (ç”¨äºä¿å­˜æˆ–å¯è§†åŒ–)
   âœ… æ¨è:
      predictions = model(images)  # GPUä¸Šè®¡ç®—
      predictions = predictions.cpu()  # ç§»å›CPU
      predictions = predictions.numpy()  # è½¬ä¸ºnumpy

5. å®šæœŸæ¸…ç©ºGPUç¼“å­˜ (é¿å…å†…å­˜æ³„æ¼)
   âœ… æ¨è:
      torch.cuda.empty_cache()

6. ä½¿ç”¨å¤šä¸ªworkeråŠ è½½æ•°æ® (CPUé¢„å¤„ç†,GPUè®­ç»ƒ)
   âœ… æ¨è:
      DataLoader(dataset, batch_size=64, num_workers=4)
      # CPUå¹¶è¡ŒåŠ è½½æ•°æ®,GPUä¸“æ³¨è®­ç»ƒ

7. æ··åˆç²¾åº¦è®­ç»ƒ (GPUä¸“ç”¨,æ›´å¿«æ›´çœå†…å­˜)
   âœ… æ¨è (å¦‚æœæœ‰GPU):
      from torch.cuda.amp import autocast, GradScaler
      scaler = GradScaler()

      with autocast():  # è‡ªåŠ¨ä½¿ç”¨FP16
          outputs = model(images)
          loss = criterion(outputs, labels)

      scaler.scale(loss).backward()
      scaler.step(optimizer)
      scaler.update()
    """)


# ============================================================
# 8. æ£€æŸ¥å½“å‰ç¯å¢ƒ
# ============================================================
def check_environment():
    """æ£€æŸ¥å½“å‰ç¯å¢ƒçš„GPUé…ç½®"""
    print("\n" + "=" * 70)
    print("ğŸ” ç¯å¢ƒæ£€æŸ¥")
    print("=" * 70)

    print(f"\nPyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")

    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"GPU æ•°é‡: {torch.cuda.device_count()}")

        for i in range(torch.cuda.device_count()):
            print(f"\nGPU {i}:")
            print(f"  åç§°: {torch.cuda.get_device_name(i)}")
            props = torch.cuda.get_device_properties(i)
            print(f"  æ˜¾å­˜: {props.total_memory / 1024**3:.2f} GB")
            print(f"  è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
    else:
        print("\nâš ï¸ å½“å‰ç¯å¢ƒæ²¡æœ‰å¯ç”¨çš„GPU")
        print("ğŸ’¡ ä½ æ­£åœ¨ä½¿ç”¨ CPU è¿›è¡Œè®­ç»ƒ")
        print("\nå¦‚ä½•è·å¾—GPUæ”¯æŒ:")
        print("  1. ä½¿ç”¨äº‘å¹³å°: Google Colab, Kaggle, AWS, Azure")
        print("  2. æœ¬åœ°å®‰è£…: éœ€è¦NVIDIAæ˜¾å¡ + CUDA + cuDNN")
        print("  3. è´­ä¹°GPUæœåŠ¡å™¨: é˜¿é‡Œäº‘ã€è…¾è®¯äº‘ç­‰")

    print("\nå½“å‰é»˜è®¤è®¾å¤‡:")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"  {device}")

    print("=" * 70)


# ============================================================
# 9. ä¸»å‡½æ•°
# ============================================================
def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ç¤ºä¾‹7: CPU/GPU è®­ç»ƒè¯¦è§£")
    print("=" * 70)

    # 1. æ£€æŸ¥ç¯å¢ƒ
    check_environment()

    # 2. æ¼”ç¤ºæ­£ç¡®çš„è®¾å¤‡é€‰æ‹©
    device = get_device(prefer_gpu=True)

    # 3. å¸¸è§é”™è¯¯
    common_mistakes()

    # 4. æœ€ä½³å®è·µ
    best_practices()

    # 5. é€Ÿåº¦å¯¹æ¯” (å¯é€‰)
    print("\n" + "=" * 70)
    response = input("æ˜¯å¦è¿›è¡Œ CPU vs GPU é€Ÿåº¦å¯¹æ¯”æµ‹è¯•? (y/n): ")
    if response.lower() == 'y':
        compare_cpu_gpu()

    print("\n" + "=" * 70)
    print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
    print("=" * 70)
    print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
    print("  1. âœ… ä½¿ç”¨ torch.device è‡ªåŠ¨é€‚é… CPU/GPU")
    print("  2. âœ… æ¨¡å‹å’Œæ•°æ®éƒ½è¦ç§»åˆ°åŒä¸€è®¾å¤‡")
    print("  3. âœ… ä¼˜å…ˆä½¿ç”¨ .to(device) è€Œä¸æ˜¯ .cuda()")
    print("  4. âœ… GPU è®­ç»ƒé€Ÿåº¦é€šå¸¸æ˜¯ CPU çš„ 10-100 å€")
    print("  5. âœ… æ²¡æœ‰ GPU æ—¶,CPU è®­ç»ƒå®Œå…¨å¯è¡Œ,åªæ˜¯æ…¢ä¸€äº›")
    print("\nğŸ“– ä½ å½“å‰çš„æƒ…å†µ:")
    if torch.cuda.is_available():
        print("  âœ… ä½ æœ‰å¯ç”¨çš„ GPU,å»ºè®®ä½¿ç”¨ GPU è®­ç»ƒ")
    else:
        print("  ğŸ’» ä½ æ²¡æœ‰ GPU,ä½¿ç”¨ CPU è®­ç»ƒ")
        print("     CPU è®­ç»ƒå®Œå…¨æ²¡é—®é¢˜,åªæ˜¯éœ€è¦æ›´å¤šæ—¶é—´")
        print("     å»ºè®®: å‡å°æ¨¡å‹å¤§å°æˆ–æ•°æ®é‡æ¥åŠ å¿«è®­ç»ƒ")
    print("=" * 70)


if __name__ == '__main__':
    main()
