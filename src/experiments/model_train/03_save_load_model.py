"""
ç¤ºä¾‹3: æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

åœ¨ç¤ºä¾‹2çš„åŸºç¡€ä¸Šæ·»åŠ :
- è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜checkpoint
- ä¿å­˜æœ€ä½³æ¨¡å‹
- ä»checkpointæ¢å¤è®­ç»ƒ
- TensorBoard å¯è§†åŒ–
- æ¨¡å‹åŠ è½½å’Œæ¨ç†

è¿è¡Œ: python src/experiments/model_train/03_save_load_model.py
æŸ¥çœ‹TensorBoard: tensorboard --logdir=/home/seeback/PycharmProjects/DeepLearning/tensor_board
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, transforms
import os
from datetime import datetime


# ============================================================
# 1. å®šä¹‰ç›¸åŒçš„CNNæ¨¡å‹
# ============================================================
class SimpleCNN(nn.Module):
    """ç®€å•çš„3å±‚å·ç§¯ç¥ç»ç½‘ç»œ"""
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(128 * 4 * 4, 256)
        self.fc2 = nn.Linear(256, 10)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = self.pool(self.relu(self.conv3(x)))
        x = x.view(x.size(0), -1)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x


# ============================================================
# 2. å‡†å¤‡æ•°æ®
# ============================================================
def prepare_data(val_ratio=0.2):
    """åŠ è½½CIFAR-10æ•°æ®é›†å¹¶åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†"""
    print("æ­£åœ¨åŠ è½½ CIFAR-10 æ•°æ®é›†...")

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    full_train_dataset = datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )

    total_size = len(full_train_dataset)
    val_size = int(total_size * val_ratio)
    train_size = total_size - val_size

    train_dataset, val_dataset = random_split(
        full_train_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)

    print(f"è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
    print(f"éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡")

    return train_loader, val_loader


# ============================================================
# 3. è®­ç»ƒä¸€ä¸ªepoch, å¹¶åœ¨æ¯ä¸ªbatchæ‰“å°æŸå¤±
# ============================================================
def train_one_epoch(model, train_loader, criterion, optimizer, epoch, writer):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()

    running_loss = 0.0
    total_samples = 0

    for batch_idx, (images, labels) in enumerate(train_loader): # éå†æ¯ä¸ªbatch
        outputs = model(images)
        loss = criterion(outputs, labels) # è®¡ç®—æŸå¤±

        optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        total_samples += images.size(0)

        if (batch_idx + 1) % 100 == 0:
            avg_loss = running_loss / total_samples
            print(f"  Batch [{batch_idx+1}/{len(train_loader)}] Loss: {avg_loss:.4f}")

            global_step = epoch * len(train_loader) + batch_idx
            writer.add_scalar('Train/Batch_Loss', loss.item(), global_step)

    avg_loss = running_loss / total_samples
    return avg_loss


# ============================================================
# 4. éªŒè¯å‡½æ•°
# ============================================================
def validate(model, val_loader, criterion):
    """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    model.eval()

    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in val_loader:
            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * images.size(0)

            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    avg_loss = running_loss / total
    accuracy = 100 * correct / total

    return avg_loss, accuracy


# ============================================================
# 5. ä¿å­˜checkpoint - æ–°å¢!
# ============================================================
def save_checkpoint(model, optimizer, epoch, train_loss, val_loss, val_acc, checkpoint_dir, is_best=False):
    """
    ä¿å­˜è®­ç»ƒcheckpoint , åŒ…å«æ¨¡å‹çŠ¶æ€ã€ä¼˜åŒ–å™¨çŠ¶æ€ã€æŸå¤±å’Œå‡†ç¡®ç‡

    å‚æ•°:
        model: æ¨¡å‹
        optimizer: ä¼˜åŒ–å™¨
        epoch: å½“å‰epoch
        train_loss: è®­ç»ƒæŸå¤±
        val_loss: éªŒè¯æŸå¤±
        val_acc: éªŒè¯å‡†ç¡®ç‡
        checkpoint_dir: ä¿å­˜ç›®å½•
        is_best: æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
    """
    os.makedirs(checkpoint_dir, exist_ok=True)

    # æ„å»ºcheckpointå­—å…¸
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'val_acc': val_acc,
        'date': datetime.now().isoformat(),
        'model_architecture': 'SimpleCNN',
        'num_classes': 10
    }

    # ä¿å­˜æœ€æ–°çš„checkpoint
    latest_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
    torch.save(checkpoint, latest_path)
    print(f"  ğŸ’¾ ä¿å­˜checkpoint: {latest_path}")

    # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹,é¢å¤–ä¿å­˜ä¸€ä»½
    if is_best:
        best_path = os.path.join(checkpoint_dir, 'best_model.pth')
        torch.save(checkpoint, best_path)
        print(f"  â­ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")

    # æ¯5ä¸ªepochä¿å­˜ä¸€ä¸ªå¸¦ç¼–å·çš„checkpoint
    if (epoch + 1) % 5 == 0:
        epoch_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')
        torch.save(checkpoint, epoch_path)
        print(f"  ğŸ“Œ ä¿å­˜epoch checkpoint: {epoch_path}")


# ============================================================
# 6. åŠ è½½checkpoint - æ–°å¢!
# ============================================================
def load_checkpoint(model, optimizer, checkpoint_path):
    """
    ä»checkpointæ¢å¤è®­ç»ƒ

    å‚æ•°:
        model: æ¨¡å‹
        optimizer: ä¼˜åŒ–å™¨
        checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„

    è¿”å›:
        start_epoch: åº”è¯¥ä»å“ªä¸ªepochå¼€å§‹è®­ç»ƒ
        best_val_acc: ä¹‹å‰çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡
    """
    if not os.path.exists(checkpoint_path):
        print(f"âš ï¸ Checkpoint ä¸å­˜åœ¨: {checkpoint_path}")
        return 0, 0.0

    print(f"\nğŸ“‚ åŠ è½½checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path)

    # æ¢å¤æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    # æ¢å¤è®­ç»ƒçŠ¶æ€
    start_epoch = checkpoint['epoch'] + 1
    best_val_acc = checkpoint['val_acc']

    print(f"  âœ… æ¢å¤æˆåŠŸ!")
    print(f"     ä¸Šæ¬¡è®­ç»ƒåˆ° Epoch {checkpoint['epoch']}")
    print(f"     éªŒè¯å‡†ç¡®ç‡: {checkpoint['val_acc']:.2f}%")
    print(f"     éªŒè¯Loss: {checkpoint['val_loss']:.4f}")
    print(f"     ä¿å­˜æ—¶é—´: {checkpoint['date']}")

    return start_epoch, best_val_acc


# ============================================================
# 7. ä¸»è®­ç»ƒå¾ªç¯ - æ–°å¢: checkpointä¿å­˜
# ============================================================
def train(model, train_loader, val_loader, criterion, optimizer, num_epochs=15, resume_from=None):
    """
    å®Œæ•´çš„è®­ç»ƒ+éªŒè¯å¾ªç¯,æ”¯æŒcheckpointä¿å­˜å’Œæ¢å¤

    å‚æ•°:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        num_epochs: è®­ç»ƒè½®æ•°
        resume_from: ä»å“ªä¸ªcheckpointæ¢å¤ (Noneè¡¨ç¤ºä»å¤´å¼€å§‹)
    """
    print("\nå¼€å§‹è®­ç»ƒ...")
    print("=" * 70)

    # åˆ›å»ºcheckpointä¿å­˜ç›®å½•
    checkpoint_dir = 'artifacts/checkpoints/03_save_load_model'
    os.makedirs(checkpoint_dir, exist_ok=True)
    print(f"Checkpoint ä¿å­˜ç›®å½•: {checkpoint_dir}")

    # åˆ›å»ºTensorBoard writer
    log_dir = '/home/seeback/PycharmProjects/DeepLearning/tensor_board/03_save_load_model'
    os.makedirs(log_dir, exist_ok=True)
    writer = SummaryWriter(log_dir)
    print(f"TensorBoard æ—¥å¿—: {log_dir}")
    print("=" * 70)

    # å°è¯•ä»checkpointæ¢å¤
    start_epoch = 0
    best_val_acc = 0.0

    if resume_from is not None:
        start_epoch, best_val_acc = load_checkpoint(model, optimizer, resume_from)
        print(f"\nâ–¶ï¸ ä» Epoch {start_epoch} ç»§ç»­è®­ç»ƒ...")
    else:
        print(f"\nâ–¶ï¸ ä»å¤´å¼€å§‹è®­ç»ƒ...")

    # è®­ç»ƒå¾ªç¯
    for epoch in range(start_epoch, num_epochs):
        print(f"\nEpoch [{epoch+1}/{num_epochs}]")
        print("-" * 70)

        # è®­ç»ƒé˜¶æ®µ
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, epoch, writer)

        # éªŒè¯é˜¶æ®µ
        val_loss, val_acc = validate(model, val_loader, criterion)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š Epoch [{epoch+1}/{num_epochs}] æ€»ç»“:")
        print(f"  è®­ç»ƒLoss: {train_loss:.4f}")
        print(f"  éªŒè¯Loss: {val_loss:.4f}")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")

        # å†™å…¥TensorBoard
        writer.add_scalar('Loss/Train', train_loss, epoch)
        writer.add_scalar('Loss/Validation', val_loss, epoch)
        writer.add_scalar('Accuracy/Validation', val_acc, epoch)

        # åˆ¤æ–­æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        is_best = val_acc > best_val_acc
        if is_best:
            best_val_acc = val_acc
            print(f"  â­ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡!")

        # ä¿å­˜checkpoint
        save_checkpoint(
            model, optimizer, epoch,
            train_loss, val_loss, val_acc,
            checkpoint_dir, is_best=is_best
        )

        print("-" * 70)

    writer.close()
    print("\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")

    return best_val_acc


# ============================================================
# 8. æ¼”ç¤ºåŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç† - æ–°å¢!
# ============================================================
def demo_inference(checkpoint_path):
    """
    æ¼”ç¤ºå¦‚ä½•åŠ è½½ä¿å­˜çš„æ¨¡å‹è¿›è¡Œæ¨ç†

    å‚æ•°:
        checkpoint_path: æ¨¡å‹checkpointè·¯å¾„
    """
    print("\n" + "=" * 70)
    print("æ¼”ç¤º: åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†")
    print("=" * 70)

    # 1. åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = SimpleCNN() # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model.eval()

    # 2. åŠ è½½æ¨¡å‹æƒé‡
    print(f"\n1ï¸âƒ£ åŠ è½½æ¨¡å‹: {checkpoint_path}") # åŠ è½½æ¨¡å‹checkpoint
    checkpoint = torch.load(checkpoint_path) # åŠ è½½checkpointæ–‡ä»¶
    model.load_state_dict(checkpoint['model_state_dict']) # åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸
    print(f"   âœ… åŠ è½½æˆåŠŸ! (éªŒè¯å‡†ç¡®ç‡: {checkpoint['val_acc']:.2f}%)") # æ‰“å°åŠ è½½æˆåŠŸä¿¡æ¯

    # 3. å‡†å¤‡æµ‹è¯•æ•°æ®
    print(f"\n2ï¸âƒ£ åŠ è½½æµ‹è¯•æ•°æ®...")
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # å½’ä¸€åŒ–, ä½¿åƒç´ å€¼åœ¨[-1, 1]ä¹‹é—´
    ])

    test_dataset = datasets.CIFAR10(
        root='./data',
        train=False,
        download=True,
        transform=transform
    )

    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

    # 4. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    print(f"\n3ï¸âƒ£ åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
    criterion = nn.CrossEntropyLoss() # å®šä¹‰æŸå¤±å‡½æ•°
    test_loss, test_acc = validate(model, test_loader, criterion) # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹

    print(f"\nğŸ“Š æµ‹è¯•é›†ç»“æœ:")
    print(f"  æµ‹è¯•Loss: {test_loss:.4f}")
    print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")

    # 5. å•å¼ å›¾ç‰‡æ¨ç†ç¤ºä¾‹
    print(f"\n4ï¸âƒ£ å•å¼ å›¾ç‰‡æ¨ç†ç¤ºä¾‹...")
    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
                   'dog', 'frog', 'horse', 'ship', 'truck']

    # è·å–ä¸€å¼ å›¾ç‰‡
    image, true_label = test_dataset[0]
    image_batch = image.unsqueeze(0)  # æ·»åŠ batchç»´åº¦

    # æ¨ç†
    with torch.no_grad():
        output = model(image_batch)
        _, predicted = torch.max(output, 1)

    print(f"  çœŸå®æ ‡ç­¾: {class_names[true_label]}")
    print(f"  é¢„æµ‹æ ‡ç­¾: {class_names[predicted.item()]}")
    print(f"  é¢„æµ‹æ­£ç¡®: {'âœ…' if predicted.item() == true_label else 'âŒ'}")

    print("=" * 70)


# ============================================================
# 9. ä¸»å‡½æ•°
# ============================================================
def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ç¤ºä¾‹3: æ¨¡å‹ä¿å­˜å’ŒåŠ è½½")
    print("=" * 70)

    # æ­¥éª¤1: å‡†å¤‡æ•°æ®
    train_loader, val_loader = prepare_data(val_ratio=0.2)

    # æ­¥éª¤2: åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºæ¨¡å‹...")
    model = SimpleCNN()
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}")

    # æ­¥éª¤3: å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

    # æ­¥éª¤4: è®­ç»ƒæ¨¡å‹ (å¯ä»¥æŒ‡å®š resume_from æ¥æ¢å¤è®­ç»ƒ)
    # resume_from = 'artifacts/checkpoints/03_save_load_model/latest_checkpoint.pth'  # ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­
    resume_from = None  # ä»å¤´å¼€å§‹

    best_acc = train(
        model, train_loader, val_loader,
        criterion, optimizer,
        num_epochs=15,
        resume_from=resume_from
    )

    # æ­¥éª¤5: æ¼”ç¤ºåŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†
    best_model_path = 'artifacts/checkpoints/03_save_load_model/best_model.pth'
    if os.path.exists(best_model_path):
        demo_inference(best_model_path)

    print("\n" + "=" * 70)
    print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
    print("=" * 70)
    print("\nğŸ’¡ æ–°å¢å†…å®¹:")
    print("  1. âœ… ä¿å­˜checkpoint (æ¨¡å‹+ä¼˜åŒ–å™¨+è®­ç»ƒçŠ¶æ€)")
    print("  2. âœ… ä¿å­˜æœ€ä½³æ¨¡å‹")
    print("  3. âœ… æ¯5ä¸ªepochä¿å­˜ä¸€ä¸ªcheckpoint")
    print("  4. âœ… ä»checkpointæ¢å¤è®­ç»ƒ")
    print("  5. âœ… åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†")
    print("\nğŸ’¾ ä¿å­˜çš„checkpoint:")
    print("  - latest_checkpoint.pth  (æœ€æ–°)")
    print("  - best_model.pth         (æœ€ä½³)")
    print("  - checkpoint_epoch_X.pth (æ¯5ä¸ªepoch)")
    print("\nğŸ“Š æŸ¥çœ‹è®­ç»ƒå¯è§†åŒ–:")
    print("  tensorboard --logdir=/home/seeback/PycharmProjects/DeepLearning/tensor_board")
    print("=" * 70)


if __name__ == '__main__':
    main()
