"""
ç¤ºä¾‹4: å­¦ä¹ ç‡è°ƒåº¦ (Learning Rate Scheduling)

åœ¨ç¤ºä¾‹3çš„åŸºç¡€ä¸Šæ·»åŠ :
- å­¦ä¹ ç‡è°ƒåº¦å™¨ (StepLR, CosineAnnealingLR, ReduceLROnPlateau)
- å­¦ä¹ ç‡å¯è§†åŒ–
- å¯¹æ¯”ä¸åŒå­¦ä¹ ç‡ç­–ç•¥çš„æ•ˆæœ
- TensorBoard å¯è§†åŒ–å­¦ä¹ ç‡å˜åŒ–

è¿è¡Œ: python src/experiments/model_train/04_lr_scheduler.py
æŸ¥çœ‹TensorBoard: tensorboard --logdir=/home/seeback/PycharmProjects/DeepLearning/tensor_board
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, transforms
import os


# ============================================================
# 1. å®šä¹‰ç›¸åŒçš„CNNæ¨¡å‹
# ============================================================
class SimpleCNN(nn.Module):
    """ç®€å•çš„3å±‚å·ç§¯ç¥ç»ç½‘ç»œ"""
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(128 * 4 * 4, 256)
        self.fc2 = nn.Linear(256, 10)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = self.pool(self.relu(self.conv3(x)))
        x = x.view(x.size(0), -1)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x


# ============================================================
# 2. å‡†å¤‡æ•°æ®
# ============================================================
def prepare_data(val_ratio=0.2):
    """åŠ è½½CIFAR-10æ•°æ®é›†å¹¶åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†"""
    print("æ­£åœ¨åŠ è½½ CIFAR-10 æ•°æ®é›†...")

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])

    full_train_dataset = datasets.CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )

    total_size = len(full_train_dataset)
    val_size = int(total_size * val_ratio)
    train_size = total_size - val_size

    train_dataset, val_dataset = random_split(
        full_train_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)

    print(f"è®­ç»ƒé›†: {len(train_dataset)} å¼ å›¾ç‰‡")
    print(f"éªŒè¯é›†: {len(val_dataset)} å¼ å›¾ç‰‡")

    return train_loader, val_loader


# ============================================================
# 3. è®­ç»ƒä¸€ä¸ªepoch
# ============================================================
def train_one_epoch(model, train_loader, criterion, optimizer, epoch, writer):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()

    running_loss = 0.0
    total_samples = 0

    for batch_idx, (images, labels) in enumerate(train_loader):
        outputs = model(images)
        loss = criterion(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        total_samples += images.size(0)

        if (batch_idx + 1) % 100 == 0:
            avg_loss = running_loss / total_samples
            print(f"  Batch [{batch_idx+1}/{len(train_loader)}] Loss: {avg_loss:.4f}")

    avg_loss = running_loss / total_samples
    return avg_loss


# ============================================================
# 4. éªŒè¯å‡½æ•°
# ============================================================
def validate(model, val_loader, criterion):
    """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    model.eval()

    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in val_loader:
            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * images.size(0)

            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    avg_loss = running_loss / total
    accuracy = 100 * correct / total

    return avg_loss, accuracy


# ============================================================
# 5. ä¸»è®­ç»ƒå¾ªç¯ - æ–°å¢: å­¦ä¹ ç‡è°ƒåº¦å™¨
# ============================================================
def train(model, train_loader, val_loader, criterion, optimizer, scheduler, scheduler_name, num_epochs=20):
    """
    å®Œæ•´çš„è®­ç»ƒ+éªŒè¯å¾ªç¯,åŒ…å«å­¦ä¹ ç‡è°ƒåº¦

    å‚æ•°:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ (æ–°å¢!)
        scheduler_name: è°ƒåº¦å™¨åç§° (ç”¨äºæ—¥å¿—)
        num_epochs: è®­ç»ƒè½®æ•°
    """
    print(f"\nå¼€å§‹è®­ç»ƒ (å­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_name})...")
    print("=" * 70)

    # åˆ›å»ºTensorBoard writer
    log_dir = f'/home/seeback/PycharmProjects/DeepLearning/tensor_board/04_lr_scheduler/{scheduler_name}'
    os.makedirs(log_dir, exist_ok=True)
    writer = SummaryWriter(log_dir)
    print(f"TensorBoard æ—¥å¿—: {log_dir}")
    print("=" * 70)

    best_val_acc = 0.0

    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        print(f"\nEpoch [{epoch+1}/{num_epochs}]")
        print("-" * 70)

        # è·å–å½“å‰å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']
        print(f"  å½“å‰å­¦ä¹ ç‡: {current_lr:.6f}")

        # è®­ç»ƒé˜¶æ®µ
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, epoch, writer)

        # éªŒè¯é˜¶æ®µ
        val_loss, val_acc = validate(model, val_loader, criterion)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š Epoch [{epoch+1}/{num_epochs}] æ€»ç»“:")
        print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
        print(f"  è®­ç»ƒLoss: {train_loss:.4f}")
        print(f"  éªŒè¯Loss: {val_loss:.4f}")
        print(f"  éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")

        # å†™å…¥TensorBoard
        writer.add_scalar('Loss/Train', train_loss, epoch)
        writer.add_scalar('Loss/Validation', val_loss, epoch)
        writer.add_scalar('Accuracy/Validation', val_acc, epoch)
        writer.add_scalar('Learning_Rate', current_lr, epoch)  # æ–°å¢: è®°å½•å­¦ä¹ ç‡

        # æ›´æ–°æœ€ä½³å‡†ç¡®ç‡
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            print(f"  â­ æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡!")

        # ========================================
        # å…³é”®: æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        # ========================================
        if scheduler_name == 'ReduceLROnPlateau':
            # ReduceLROnPlateau éœ€è¦ metric å‚æ•°
            scheduler.step(val_loss)
        else:
            # StepLR, CosineAnnealingLR ç­‰ä¸éœ€è¦å‚æ•°
            scheduler.step()

        print("-" * 70)

    writer.close()
    print(f"\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")

    return best_val_acc


# ============================================================
# 6. åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ - æ–°å¢!
# ============================================================
def create_scheduler(optimizer, scheduler_type):
    """
    åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨

    å‚æ•°:
        optimizer: ä¼˜åŒ–å™¨
        scheduler_type: è°ƒåº¦å™¨ç±»å‹

    è¿”å›:
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    if scheduler_type == 'StepLR':
        # æ¯ step_size ä¸ªepoché™ä½å­¦ä¹ ç‡
        scheduler = optim.lr_scheduler.StepLR(
            optimizer,
            step_size=5,    # æ¯5ä¸ªepoch
            gamma=0.5       # å­¦ä¹ ç‡ä¹˜ä»¥0.5
        )
        print("  StepLR: æ¯5ä¸ªepoch, å­¦ä¹ ç‡ Ã— 0.5")

    elif scheduler_type == 'MultiStepLR':
        # åœ¨æŒ‡å®šçš„epoché™ä½å­¦ä¹ ç‡
        scheduler = optim.lr_scheduler.MultiStepLR(
            optimizer,
            milestones=[6, 12, 18],  # åœ¨ç¬¬6, 12, 18ä¸ªepoch
            gamma=0.5                # å­¦ä¹ ç‡ä¹˜ä»¥0.5
        )
        print("  MultiStepLR: åœ¨epoch [6, 12, 18], å­¦ä¹ ç‡ Ã— 0.5")

    elif scheduler_type == 'CosineAnnealingLR':
        # ä½™å¼¦é€€ç«: å­¦ä¹ ç‡æŒ‰ä½™å¼¦æ›²çº¿å˜åŒ–
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=20,      # å‘¨æœŸé•¿åº¦
            eta_min=1e-6   # æœ€å°å­¦ä¹ ç‡
        )
        print("  CosineAnnealingLR: T_max=20, eta_min=1e-6")

    elif scheduler_type == 'ReduceLROnPlateau':
        # å½“æŒ‡æ ‡åœæ­¢æ”¹å–„æ—¶é™ä½å­¦ä¹ ç‡
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',      # ç›‘æ§æŒ‡æ ‡è¶Šå°è¶Šå¥½ (loss)
            factor=0.5,      # å­¦ä¹ ç‡ä¹˜ä»¥0.5
            patience=3,      # å®¹å¿3ä¸ªepochä¸æ”¹å–„
            verbose=True     # æ‰“å°ä¿¡æ¯
        )
        print("  ReduceLROnPlateau: patience=3, factor=0.5")

    elif scheduler_type == 'ExponentialLR':
        # æŒ‡æ•°è¡°å‡
        scheduler = optim.lr_scheduler.ExponentialLR(
            optimizer,
            gamma=0.95  # æ¯ä¸ªepochå­¦ä¹ ç‡ä¹˜ä»¥0.95
        )
        print("  ExponentialLR: gamma=0.95")

    else:
        raise ValueError(f"æœªçŸ¥çš„è°ƒåº¦å™¨ç±»å‹: {scheduler_type}")

    return scheduler


# ============================================================
# 7. å¯¹æ¯”ä¸åŒè°ƒåº¦å™¨ - æ–°å¢!
# ============================================================
def compare_schedulers():
    """å¯¹æ¯”ä¸åŒå­¦ä¹ ç‡è°ƒåº¦å™¨çš„æ•ˆæœ"""
    print("\n" + "=" * 70)
    print("å¯¹æ¯”ä¸åŒå­¦ä¹ ç‡è°ƒåº¦å™¨")
    print("=" * 70)

    # å‡†å¤‡æ•°æ® (æ‰€æœ‰å®éªŒä½¿ç”¨ç›¸åŒæ•°æ®)
    train_loader, val_loader = prepare_data(val_ratio=0.2)

    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()

    # è¦å¯¹æ¯”çš„è°ƒåº¦å™¨
    scheduler_types = [
        'StepLR',
        'CosineAnnealingLR',
        'ReduceLROnPlateau'
    ]

    results = {}

    for scheduler_type in scheduler_types:
        print(f"\n{'='*70}")
        print(f"æµ‹è¯•è°ƒåº¦å™¨: {scheduler_type}")
        print(f"{'='*70}")

        # åˆ›å»ºæ–°æ¨¡å‹ (ç¡®ä¿æ¯ä¸ªå®éªŒä»ç›¸åŒåˆå§‹çŠ¶æ€å¼€å§‹)
        model = SimpleCNN()
        torch.manual_seed(42)  # å›ºå®šéšæœºç§å­

        # åˆ›å»ºä¼˜åŒ–å™¨ (åˆå§‹å­¦ä¹ ç‡éƒ½æ˜¯0.01)
        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = create_scheduler(optimizer, scheduler_type)

        # è®­ç»ƒ
        best_acc = train(
            model, train_loader, val_loader,
            criterion, optimizer, scheduler,
            scheduler_name=scheduler_type,
            num_epochs=20
        )

        results[scheduler_type] = best_acc

    # æ‰“å°å¯¹æ¯”ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š è°ƒåº¦å™¨å¯¹æ¯”ç»“æœ")
    print("=" * 70)
    print(f"\n{'è°ƒåº¦å™¨':<25} {'æœ€ä½³éªŒè¯å‡†ç¡®ç‡':>15}")
    print("-" * 42)
    for scheduler_type, acc in results.items():
        print(f"{scheduler_type:<25} {acc:>14.2f}%")
    print("=" * 70)


# ============================================================
# 8. å¯è§†åŒ–å­¦ä¹ ç‡å˜åŒ– - æ–°å¢!
# ============================================================
def visualize_lr_schedules():
    """å¯è§†åŒ–ä¸åŒå­¦ä¹ ç‡è°ƒåº¦å™¨çš„å˜åŒ–æ›²çº¿"""
    print("\n" + "=" * 70)
    print("å¯è§†åŒ–å­¦ä¹ ç‡è°ƒåº¦")
    print("=" * 70)

    # åˆ›å»ºä¸€ä¸ªç®€å•æ¨¡å‹ (åªæ˜¯ä¸ºäº†åˆ›å»ºoptimizer)
    model = SimpleCNN()

    scheduler_types = ['StepLR', 'CosineAnnealingLR', 'ExponentialLR']

    for scheduler_type in scheduler_types:
        # åˆ›å»ºoptimizerå’Œscheduler
        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
        scheduler = create_scheduler(optimizer, scheduler_type)

        # åˆ›å»ºTensorBoard writer
        log_dir = f'/home/seeback/PycharmProjects/DeepLearning/tensor_board/04_lr_scheduler/viz_{scheduler_type}'
        writer = SummaryWriter(log_dir)

        # æ¨¡æ‹Ÿ20ä¸ªepoch,è®°å½•å­¦ä¹ ç‡å˜åŒ–
        print(f"\n{scheduler_type} å­¦ä¹ ç‡å˜åŒ–:")
        for epoch in range(20):
            lr = optimizer.param_groups[0]['lr']
            writer.add_scalar('Learning_Rate', lr, epoch)

            if epoch % 5 == 0:
                print(f"  Epoch {epoch:2d}: lr = {lr:.6f}")

            # æ›´æ–°å­¦ä¹ ç‡
            if scheduler_type != 'ReduceLROnPlateau':
                scheduler.step()
            else:
                # ReduceLROnPlateauéœ€è¦metric,è¿™é‡Œæ¨¡æ‹Ÿä¸€ä¸ªé€’å‡çš„loss
                fake_loss = 2.0 - epoch * 0.05
                scheduler.step(fake_loss)

        writer.close()

    print("\nâœ… å­¦ä¹ ç‡å¯è§†åŒ–å®Œæˆ!")
    print("=" * 70)


# ============================================================
# 9. ä¸»å‡½æ•°
# ============================================================
def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ç¤ºä¾‹4: å­¦ä¹ ç‡è°ƒåº¦")
    print("=" * 70)

    print("\nğŸ“– å­¦ä¹ ç‡è°ƒåº¦å™¨è¯´æ˜:")
    print("-" * 70)
    print("""
    1. StepLR
       - æ¯éš”å›ºå®šepoché™ä½å­¦ä¹ ç‡
       - ç®€å•ç¨³å®š,å¸¸ç”¨äºåŸºç¡€è®­ç»ƒ
       - ç¤ºä¾‹: æ¯5ä¸ªepoch, lr Ã— 0.5

    2. CosineAnnealingLR
       - å­¦ä¹ ç‡æŒ‰ä½™å¼¦æ›²çº¿å˜åŒ–
       - å¹³æ»‘è¡°å‡,å¸¸ç”¨äºfine-tuning
       - å…ˆå¿«åæ…¢,æœ‰åˆ©äºæ”¶æ•›

    3. ReduceLROnPlateau
       - å½“éªŒè¯lossä¸å†ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡
       - è‡ªé€‚åº”,æ— éœ€æ‰‹åŠ¨è®¾ç½®epoch
       - é€‚åˆä¸ç¡®å®šè®­ç»ƒè½®æ•°çš„æƒ…å†µ

    4. ExponentialLR
       - æŒ‡æ•°è¡°å‡
       - æ¯ä¸ªepochå­¦ä¹ ç‡ä¹˜ä»¥å›ºå®šç³»æ•°
       - è¡°å‡é€Ÿåº¦å¯æ§

    5. MultiStepLR
       - åœ¨æŒ‡å®šçš„epoché™ä½å­¦ä¹ ç‡
       - çµæ´»,å¯ä»¥è‡ªå®šä¹‰é™ä½æ—¶æœº
    """)

    # é€‰é¡¹1: å¯è§†åŒ–å­¦ä¹ ç‡å˜åŒ–æ›²çº¿
    print("\né€‰é¡¹1: å¯è§†åŒ–å­¦ä¹ ç‡å˜åŒ–æ›²çº¿")
    visualize_lr_schedules()

    # é€‰é¡¹2: å¯¹æ¯”ä¸åŒè°ƒåº¦å™¨çš„è®­ç»ƒæ•ˆæœ
    print("\n\né€‰é¡¹2: å¯¹æ¯”ä¸åŒè°ƒåº¦å™¨çš„è®­ç»ƒæ•ˆæœ")
    print("(è¿™å°†è®­ç»ƒ3ä¸ªæ¨¡å‹,éœ€è¦ä¸€äº›æ—¶é—´...)")
    response = input("æ˜¯å¦æ‰§è¡Œå¯¹æ¯”å®éªŒ? (y/n): ")
    if response.lower() == 'y':
        compare_schedulers()

    print("\n" + "=" * 70)
    print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
    print("=" * 70)
    print("\nğŸ’¡ æ–°å¢å†…å®¹:")
    print("  1. âœ… StepLR - å›ºå®šæ­¥é•¿é™ä½å­¦ä¹ ç‡")
    print("  2. âœ… CosineAnnealingLR - ä½™å¼¦é€€ç«")
    print("  3. âœ… ReduceLROnPlateau - è‡ªé€‚åº”é™ä½å­¦ä¹ ç‡")
    print("  4. âœ… ExponentialLR - æŒ‡æ•°è¡°å‡")
    print("  5. âœ… MultiStepLR - å¤šé˜¶æ®µé™ä½å­¦ä¹ ç‡")
    print("  6. âœ… å­¦ä¹ ç‡å¯è§†åŒ–")
    print("  7. âœ… å¯¹æ¯”ä¸åŒè°ƒåº¦å™¨æ•ˆæœ")
    print("\nğŸ“Š æŸ¥çœ‹è®­ç»ƒå¯è§†åŒ–:")
    print("  tensorboard --logdir=/home/seeback/PycharmProjects/DeepLearning/tensor_board/04_lr_scheduler")
    print("=" * 70)


if __name__ == '__main__':
    main()
