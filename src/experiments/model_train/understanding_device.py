"""
æ·±å…¥ç†è§£ torch.device() å’Œ GPU é€‰æ‹©æœºåˆ¶

è¯¦ç»†è§£é‡Š:
1. torch.device() æ˜¯ä»€ä¹ˆ
2. ä¸ºä»€ä¹ˆå®šä¹‰deviceå°±èƒ½è‡ªåŠ¨ä½¿ç”¨GPU
3. å¦‚ä½•é€‰æ‹©ç‰¹å®šçš„GPU
4. å¤šGPUç¯å¢ƒä¸‹çš„è®¾å¤‡ç®¡ç†
5. å®é™…æ¼”ç¤ºå’Œæœ€ä½³å®è·µ
"""

import torch
import os


# ============================================================
# 1. torch.device() åŸºç¡€æ¦‚å¿µ
# ============================================================
def explain_device_concept():
    """è§£é‡Šdeviceçš„åŸºæœ¬æ¦‚å¿µ"""
    print("=" * 70)
    print("ğŸ“š torch.device() åŸºç¡€æ¦‚å¿µ")
    print("=" * 70)

    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 torch.device() æ˜¯ä»€ä¹ˆ?                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

torch.device æ˜¯ PyTorch ä¸­è¡¨ç¤º"è®¡ç®—è®¾å¤‡"çš„æŠ½è±¡å¯¹è±¡ã€‚

å¯ä»¥æŠŠå®ƒç†è§£ä¸ºä¸€ä¸ª"åœ°å€æ ‡ç­¾",å‘Šè¯‰ PyTorch:
  "è¯·åœ¨è¿™ä¸ªè®¾å¤‡ä¸Šè¿›è¡Œè®¡ç®—"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 åˆ›å»º device å¯¹è±¡çš„æ–¹å¼                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ–¹æ³•1: ä½¿ç”¨å­—ç¬¦ä¸²
  device = torch.device('cpu')           # CPU
  device = torch.device('cuda')          # é»˜è®¤GPU (cuda:0)
  device = torch.device('cuda:0')        # ç¬¬0ä¸ªGPU
  device = torch.device('cuda:1')        # ç¬¬1ä¸ªGPU

æ–¹æ³•2: åŠ¨æ€é€‰æ‹© (æ¨è!)
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  # å¦‚æœæœ‰GPUå°±ç”¨GPU,æ²¡æœ‰å°±ç”¨CPU

æ–¹æ³•3: ä½¿ç”¨ç¯å¢ƒå˜é‡ (é«˜çº§)
  os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # åªä½¿ç”¨GPU 0å’Œ1
  device = torch.device('cuda')

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 device å¯¹è±¡åŒ…å«ä»€ä¹ˆä¿¡æ¯?                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

    # æ¼”ç¤ºdeviceå¯¹è±¡çš„å±æ€§
    print("å®é™…æ¼”ç¤º:")
    print("-" * 70)

    # CPUè®¾å¤‡
    cpu_device = torch.device('cpu')
    print(f"\nCPUè®¾å¤‡:")
    print(f"  deviceå¯¹è±¡: {cpu_device}")
    print(f"  è®¾å¤‡ç±»å‹: {cpu_device.type}")
    print(f"  è®¾å¤‡ç´¢å¼•: {cpu_device.index}")

    # GPUè®¾å¤‡ (å¦‚æœå¯ç”¨)
    if torch.cuda.is_available():
        gpu_device = torch.device('cuda:0')
        print(f"\nGPUè®¾å¤‡:")
        print(f"  deviceå¯¹è±¡: {gpu_device}")
        print(f"  è®¾å¤‡ç±»å‹: {gpu_device.type}")
        print(f"  è®¾å¤‡ç´¢å¼•: {gpu_device.index}")
        print(f"  GPUåç§°: {torch.cuda.get_device_name(0)}")
    else:
        print(f"\nâš ï¸ å½“å‰ç¯å¢ƒæ²¡æœ‰GPU")

    print("\nğŸ’¡ å…³é”®ç†è§£:")
    print("  device åªæ˜¯ä¸€ä¸ª'æ ‡ç­¾',å‘Šè¯‰PyTorchåœ¨å“ªé‡Œè®¡ç®—")
    print("  å°±åƒå¿«é€’åœ°å€ä¸€æ ·!")


# ============================================================
# 2. ä¸ºä»€ä¹ˆå®šä¹‰deviceå°±èƒ½è‡ªåŠ¨ä½¿ç”¨GPU?
# ============================================================
def explain_how_device_works():
    """è§£é‡Šdeviceæ˜¯å¦‚ä½•å·¥ä½œçš„"""
    print("\n" + "=" * 70)
    print("ğŸ” ä¸ºä»€ä¹ˆå®šä¹‰deviceå°±èƒ½è‡ªåŠ¨ä½¿ç”¨GPU?")
    print("=" * 70)

    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 å·¥ä½œåŸç†è¯¦è§£                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®: device ä¸æ˜¯"æŒ‘é€‰æ˜¾å¡",è€Œæ˜¯"å‘Šè¯‰æ•°æ®å’Œæ¨¡å‹å»å“ªé‡Œ"

ç±»æ¯”: å¿«é€’ç³»ç»Ÿ
  1ï¸âƒ£ device = æ”¶ä»¶åœ°å€
  2ï¸âƒ£ model.to(device) = æŠŠæ¨¡å‹(åŒ…è£¹)é€åˆ°è¿™ä¸ªåœ°å€
  3ï¸âƒ£ data.to(device) = æŠŠæ•°æ®(åŒ…è£¹)é€åˆ°è¿™ä¸ªåœ°å€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 å®Œæ•´æµç¨‹                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ­¥éª¤1: åˆ›å»ºdeviceå¯¹è±¡
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  PyTorch åšäº†ä»€ä¹ˆ:
    âœ… æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦æœ‰GPU
    âœ… å¦‚æœæœ‰: device = cuda:0 (é»˜è®¤ç¬¬0ä¸ªGPU)
    âœ… å¦‚æœæ²¡æœ‰: device = cpu

æ­¥éª¤2: å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
  model = model.to(device)

  PyTorch åšäº†ä»€ä¹ˆ:
    âœ… å°†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°(æƒé‡ã€åç½®)å¤åˆ¶åˆ°æŒ‡å®šè®¾å¤‡çš„å†…å­˜
    âœ… CPU â†’ CPUå†…å­˜
    âœ… CUDA â†’ GPUæ˜¾å­˜
    âœ… è¿”å›ä¸€ä¸ªåœ¨æ–°è®¾å¤‡ä¸Šçš„æ¨¡å‹å¼•ç”¨

æ­¥éª¤3: å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
  images = images.to(device)

  PyTorch åšäº†ä»€ä¹ˆ:
    âœ… å°†tensorä»å½“å‰ä½ç½®å¤åˆ¶åˆ°æŒ‡å®šè®¾å¤‡
    âœ… ä¾‹å¦‚: CPUå†…å­˜ â†’ GPUæ˜¾å­˜

æ­¥éª¤4: è‡ªåŠ¨åœ¨æ­£ç¡®è®¾å¤‡ä¸Šè®¡ç®—
  outputs = model(images)

  PyTorch åšäº†ä»€ä¹ˆ:
    âœ… æ£€æµ‹åˆ° model åœ¨ GPU, images ä¹Ÿåœ¨ GPU
    âœ… è‡ªåŠ¨è°ƒç”¨ GPU ä¸Šçš„è®¡ç®—æ ¸å¿ƒ
    âœ… ç»“æœ outputs ä¹Ÿåœ¨ GPU ä¸Š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 å†…å­˜åˆ†å¸ƒç¤ºæ„å›¾                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CPUè®¾å¤‡:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚       CPU å†…å­˜ (RAM)         â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
  â”‚  â”‚ æ¨¡å‹   â”‚  â”‚ æ•°æ®   â”‚     â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GPUè®¾å¤‡ (cuda:0):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚      GPU æ˜¾å­˜ (VRAM)         â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
  â”‚  â”‚ æ¨¡å‹   â”‚  â”‚ æ•°æ®   â”‚     â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®:
  - model.to(device) æŠŠæ¨¡å‹å¤åˆ¶åˆ°æŒ‡å®šè®¾å¤‡çš„å†…å­˜
  - data.to(device) æŠŠæ•°æ®å¤åˆ¶åˆ°æŒ‡å®šè®¾å¤‡çš„å†…å­˜
  - è®¡ç®—è‡ªåŠ¨åœ¨æ•°æ®æ‰€åœ¨çš„è®¾å¤‡ä¸Šè¿›è¡Œ
""")


# ============================================================
# 3. GPUé€‰æ‹©æœºåˆ¶
# ============================================================
def explain_gpu_selection():
    """è§£é‡Šå¦‚ä½•é€‰æ‹©ç‰¹å®šçš„GPU"""
    print("\n" + "=" * 70)
    print("ğŸ¯ GPUé€‰æ‹©æœºåˆ¶è¯¦è§£")
    print("=" * 70)

    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 å•GPUç¯å¢ƒ (æœ€å¸¸è§)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å¦‚æœåªæœ‰ä¸€ä¸ªGPU:
  device = torch.device('cuda')          # è‡ªåŠ¨ä½¿ç”¨GPU 0
  device = torch.device('cuda:0')        # æ˜ç¡®æŒ‡å®šGPU 0
  # æ•ˆæœç›¸åŒ!

PyTorch ä¼šè‡ªåŠ¨ä½¿ç”¨ç³»ç»Ÿä¸­å”¯ä¸€å¯ç”¨çš„GPUã€‚

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 å¤šGPUç¯å¢ƒ                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å‡è®¾ä½ æœ‰4ä¸ªGPU: [GPU 0, GPU 1, GPU 2, GPU 3]

æ–¹æ³•1: ç›´æ¥æŒ‡å®šGPUç¼–å·
  device = torch.device('cuda:0')        # ä½¿ç”¨GPU 0
  device = torch.device('cuda:1')        # ä½¿ç”¨GPU 1
  device = torch.device('cuda:2')        # ä½¿ç”¨GPU 2
  device = torch.device('cuda:3')        # ä½¿ç”¨GPU 3

æ–¹æ³•2: ä½¿ç”¨ç¯å¢ƒå˜é‡ (æ¨è!)
  import os
  os.environ['CUDA_VISIBLE_DEVICES'] = '0'      # åªä½¿ç”¨GPU 0
  os.environ['CUDA_VISIBLE_DEVICES'] = '1,3'    # åªä½¿ç”¨GPU 1å’Œ3
  os.environ['CUDA_VISIBLE_DEVICES'] = '2'      # åªä½¿ç”¨GPU 2

  device = torch.device('cuda')  # ä¼šä½¿ç”¨CUDA_VISIBLE_DEVICESæŒ‡å®šçš„ç¬¬ä¸€ä¸ª

  âš ï¸ æ³¨æ„: è¦åœ¨ import torch ä¹‹å‰è®¾ç½®!

æ–¹æ³•3: åŠ¨æ€é€‰æ‹©ç©ºé—²GPU
  def get_free_gpu():
      '''é€‰æ‹©æ˜¾å­˜ä½¿ç”¨æœ€å°‘çš„GPU'''
      import subprocess
      result = subprocess.check_output(
          ['nvidia-smi', '--query-gpu=memory.free',
           '--format=csv,nounits,noheader'],
          encoding='utf-8'
      )
      gpu_memory = [int(x) for x in result.strip().split('\\n')]
      return gpu_memory.index(max(gpu_memory))

  free_gpu = get_free_gpu()
  device = torch.device(f'cuda:{free_gpu}')

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 é»˜è®¤GPU vs æ˜¾å¼æŒ‡å®š                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æƒ…å†µA: ä½¿ç”¨ 'cuda' (é»˜è®¤)
  device = torch.device('cuda')
  â†’ ä½¿ç”¨ GPU 0 (ç¬¬ä¸€ä¸ªGPU)

æƒ…å†µB: æ˜¾å¼æŒ‡å®š
  device = torch.device('cuda:1')
  â†’ ä½¿ç”¨ GPU 1 (ç¬¬äºŒä¸ªGPU)

ç¤ºä¾‹ä»£ç :
  # å‡è®¾æœ‰2ä¸ªGPU
  device0 = torch.device('cuda:0')  # GPU 0
  device1 = torch.device('cuda:1')  # GPU 1

  # å¯ä»¥æŠŠä¸åŒæ¨¡å‹æ”¾åœ¨ä¸åŒGPUä¸Š
  model1 = Model1().to(device0)  # æ¨¡å‹1åœ¨GPU 0
  model2 = Model2().to(device1)  # æ¨¡å‹2åœ¨GPU 1
""")


# ============================================================
# 4. å®é™…æ¼”ç¤º
# ============================================================
def demo_device_usage():
    """å®é™…æ¼”ç¤ºdeviceçš„ä½¿ç”¨"""
    print("\n" + "=" * 70)
    print("ğŸ”¬ å®é™…æ¼”ç¤º")
    print("=" * 70)

    print("\nç¤ºä¾‹1: åŸºç¡€ä½¿ç”¨")
    print("-" * 70)

    # æ£€æŸ¥GPU
    print(f"GPUå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")

    # åˆ›å»ºdevice
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nå½“å‰ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºtensorå¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    print("\nç¤ºä¾‹2: ç§»åŠ¨tensoråˆ°è®¾å¤‡")
    print("-" * 70)

    # åœ¨CPUä¸Šåˆ›å»º
    x = torch.randn(3, 3)
    print(f"åˆ›å»ºtensor x:")
    print(f"  æ•°æ®: {x}")
    print(f"  è®¾å¤‡: {x.device}")
    print(f"  å†…å­˜ä½ç½®: {'CPUå†…å­˜' if x.device.type == 'cpu' else 'GPUæ˜¾å­˜'}")

    # ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
    x = x.to(device)
    print(f"\nç§»åŠ¨åˆ° {device} å:")
    print(f"  è®¾å¤‡: {x.device}")
    print(f"  å†…å­˜ä½ç½®: {'CPUå†…å­˜' if x.device.type == 'cpu' else 'GPUæ˜¾å­˜'}")

    # åˆ›å»ºç®€å•æ¨¡å‹
    print("\nç¤ºä¾‹3: ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡")
    print("-" * 70)

    import torch.nn as nn

    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc = nn.Linear(10, 5)

        def forward(self, x):
            return self.fc(x)

    model = SimpleModel()
    print(f"åˆ›å»ºæ¨¡å‹:")
    print(f"  æ¨¡å‹ç±»å‹: {type(model).__name__}")

    # æ£€æŸ¥å‚æ•°ä½ç½®
    first_param = next(model.parameters())
    print(f"  å‚æ•°è®¾å¤‡ (ç§»åŠ¨å‰): {first_param.device}")

    # ç§»åŠ¨æ¨¡å‹
    model = model.to(device)  # ç§»åŠ¨æ¨¡å‹åˆ°ç›®æ ‡è®¾å¤‡
    first_param = next(model.parameters())
    print(f"  å‚æ•°è®¾å¤‡ (ç§»åŠ¨å): {first_param.device}")

    print("\nç¤ºä¾‹4: è®¡ç®—ä¼šè‡ªåŠ¨åœ¨æ­£ç¡®è®¾å¤‡ä¸Šè¿›è¡Œ")
    print("-" * 70)

    # åˆ›å»ºè¾“å…¥æ•°æ®
    input_data = torch.randn(2, 10).to(device)
    print(f"è¾“å…¥æ•°æ®è®¾å¤‡: {input_data.device}") # è¾“å…¥æ•°æ®åœ¨GPUä¸Š
    print(f"æ¨¡å‹å‚æ•°è®¾å¤‡: {next(model.parameters()).device}") # æ¨¡å‹å‚æ•°ä¹Ÿåœ¨GPUä¸Š

    # å‰å‘ä¼ æ’­
    output = model(input_data) # æ¨¡å‹åœ¨GPUä¸Š,è¾“å…¥æ•°æ®ä¹Ÿåœ¨GPUä¸Š,è¾“å‡ºæ•°æ®ä¹Ÿåœ¨GPUä¸Š
    print(f"è¾“å‡ºæ•°æ®è®¾å¤‡: {output.device}")
    print(f"\nâœ… è®¡ç®—è‡ªåŠ¨åœ¨ {output.device} ä¸Šå®Œæˆ!")


# ============================================================
# 5. å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ³•
# ============================================================
def common_mistakes():
    """å¸¸è§é”™è¯¯"""
    print("\n" + "=" * 70)
    print("âš ï¸ å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ³•")
    print("=" * 70)

    print("""
é”™è¯¯1: æ¨¡å‹å’Œæ•°æ®ä¸åœ¨åŒä¸€è®¾å¤‡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âŒ é”™è¯¯ä»£ç :
  model = model.to('cuda')
  data = torch.randn(10, 10).to('cuda')  # åœ¨CPUä¸Šåˆ›å»º, ç„¶åç§»åˆ°GPU
  output = model(data)  # RuntimeError! # æ¨¡å‹åœ¨GPUä¸Š,æ•°æ®åœ¨CPUä¸Š,ä¼šæŠ¥é”™

é”™è¯¯ä¿¡æ¯:
  RuntimeError: Expected all tensors to be on the same device,
  but found at least two devices, cuda:0 and cpu!

âœ… æ­£ç¡®ä»£ç :
  device = torch.device('cuda')
  model = model.to(device)
  data = torch.randn(10, 10).to(device)  # ä¹Ÿç§»åˆ°GPU
  output = model(data)  # âœ… æ­£ç¡®


é”™è¯¯2: ç›´æ¥ä½¿ç”¨cuda()ä¸æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âŒ é”™è¯¯ä»£ç :
  model = model.cuda()  # å¦‚æœæ²¡æœ‰GPUä¼šæŠ¥é”™

é”™è¯¯ä¿¡æ¯:
  RuntimeError: CUDA error: no kernel image is available

âœ… æ­£ç¡®ä»£ç :
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model = model.to(device)  # è‡ªåŠ¨é€‚é…


é”™è¯¯3: åœ¨å¤šGPUç¯å¢ƒæœªæŒ‡å®šä½¿ç”¨å“ªä¸ªGPU
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âŒ é—®é¢˜:
  # å‡è®¾æœ‰4ä¸ªGPU,ä½†éƒ½è¢«å ç”¨
  device = torch.device('cuda')  # é»˜è®¤ä½¿ç”¨GPU 0,å¯èƒ½æ˜¾å­˜ä¸è¶³

âœ… è§£å†³æ–¹æ¡ˆ1: æ˜¾å¼æŒ‡å®šGPU
  device = torch.device('cuda:2')  # ä½¿ç”¨GPU 2

âœ… è§£å†³æ–¹æ¡ˆ2: ä½¿ç”¨ç¯å¢ƒå˜é‡
  import os
  os.environ['CUDA_VISIBLE_DEVICES'] = '3'  # åªä½¿ç”¨GPU 3
  device = torch.device('cuda')


é”™è¯¯4: å¿˜è®°å°†æŸå¤±å€¼ç§»å›CPU (æ— æ³•è½¬numpy)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âŒ é”™è¯¯ä»£ç :
  loss = criterion(output, labels)  # lossåœ¨GPUä¸Š
  loss_value = loss.numpy()  # TypeError!

é”™è¯¯ä¿¡æ¯:
  TypeError: can't convert cuda:0 device type tensor to numpy

âœ… æ­£ç¡®ä»£ç :
  loss = criterion(output, labels)
  loss_value = loss.cpu().numpy()  # å…ˆç§»åˆ°CPU


é”™è¯¯5: å¤šæ¬¡ç§»åŠ¨tensor (æµªè´¹æ—¶é—´)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âŒ ä½æ•ˆä»£ç :
  for epoch in range(100):
      model = model.to(device)  # æ¯ä¸ªepochéƒ½ç§»åŠ¨,æµªè´¹!
      for data, labels in dataloader:
          ...

âœ… é«˜æ•ˆä»£ç :
  model = model.to(device)  # åªç§»åŠ¨ä¸€æ¬¡
  for epoch in range(100):
      for data, labels in dataloader:
          data = data.to(device)  # æ¯ä¸ªbatchç§»åŠ¨
          ...
""")


# ============================================================
# 6. æœ€ä½³å®è·µ
# ============================================================
def best_practices():
    """æœ€ä½³å®è·µ"""
    print("\n" + "=" * 70)
    print("ğŸ’¡ æœ€ä½³å®è·µ")
    print("=" * 70)

    print("""
å®è·µ1: å§‹ç»ˆä½¿ç”¨deviceå¯¹è±¡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… æ¨è:
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model = model.to(device)
  data = data.to(device)

âŒ ä¸æ¨è:
  if torch.cuda.is_available():
      model = model.cuda()
      data = data.cuda()
  # ä»£ç é‡å¤,ä¸ä¼˜é›…


å®è·µ2: æ¨¡å‹ç§»åŠ¨ä¸€æ¬¡,æ•°æ®æ¯ä¸ªbatchç§»åŠ¨
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… æ ‡å‡†æ¨¡å¼:
  # è®­ç»ƒå¼€å§‹å‰ç§»åŠ¨æ¨¡å‹
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model = model.to(device)

  # è®­ç»ƒå¾ªç¯
  for epoch in range(epochs):
      for images, labels in train_loader:
          # æ¯ä¸ªbatchç§»åŠ¨æ•°æ®
          images = images.to(device)
          labels = labels.to(device)

          # è®­ç»ƒä»£ç 
          outputs = model(images)
          loss = criterion(outputs, labels)
          ...


å®è·µ3: ä½¿ç”¨éé˜»å¡ä¼ è¾“åŠ é€Ÿ (é«˜çº§)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… æ›´å¿«çš„æ•°æ®ä¼ è¾“:
  images = images.to(device, non_blocking=True)
  labels = labels.to(device, non_blocking=True)

  # non_blocking=True å…è®¸CPUå’ŒGPUå¼‚æ­¥æ‰§è¡Œ
  # æ•°æ®ä¼ è¾“å’ŒGPUè®¡ç®—å¯ä»¥é‡å 


å®è·µ4: å¤šGPUç¯å¢ƒä½¿ç”¨ç¯å¢ƒå˜é‡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… åœ¨è„šæœ¬å¼€å¤´è®¾ç½®:
  import os
  os.environ['CUDA_VISIBLE_DEVICES'] = '0,2'  # åªä½¿ç”¨GPU 0å’Œ2
  import torch

  device = torch.device('cuda')  # ä¼šä½¿ç”¨GPU 0


å®è·µ5: æ‰“å°è®¾å¤‡ä¿¡æ¯ (è°ƒè¯•)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… è®­ç»ƒå¼€å§‹æ—¶æ‰“å°:
  def print_device_info():
      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
      print(f"ä½¿ç”¨è®¾å¤‡: {device}")

      if torch.cuda.is_available():
          print(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
          print(f"GPUæ•°é‡: {torch.cuda.device_count()}")

  print_device_info()  # åœ¨è®­ç»ƒå‰è°ƒç”¨


å®è·µ6: ä¿å­˜/åŠ è½½æ¨¡å‹æ—¶æ³¨æ„è®¾å¤‡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… ä¿å­˜:
  torch.save(model.state_dict(), 'model.pth')  # è‡ªåŠ¨ä¿å­˜åˆ°CPU

âœ… åŠ è½½:
  # æ–¹æ³•1: ç›´æ¥åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model.load_state_dict(torch.load('model.pth', map_location=device))

  # æ–¹æ³•2: å…ˆåŠ è½½å†ç§»åŠ¨
  model.load_state_dict(torch.load('model.pth'))
  model = model.to(device)
""")


# ============================================================
# 7. ä¸»å‡½æ•°
# ============================================================
def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ğŸ¯ æ·±å…¥ç†è§£ torch.device() å’Œ GPU é€‰æ‹©æœºåˆ¶")
    print("=" * 70)

    # 1. åŸºç¡€æ¦‚å¿µ
    explain_device_concept()

    # 2. å·¥ä½œåŸç†
    explain_how_device_works()

    # 3. GPUé€‰æ‹©
    explain_gpu_selection()

    # 4. å®é™…æ¼”ç¤º
    demo_device_usage()

    # 5. å¸¸è§é”™è¯¯
    common_mistakes()

    # 6. æœ€ä½³å®è·µ
    best_practices()

    print("\n" + "=" * 70)
    print("âœ… æ‰€æœ‰è§£é‡Šå®Œæˆ!")
    print("=" * 70)

    print("""
ğŸ¯ æ ¸å¿ƒæ€»ç»“:

1. torch.device() æ˜¯ä»€ä¹ˆ?
   â†’ ä¸€ä¸ª"åœ°å€æ ‡ç­¾",å‘Šè¯‰PyTorchåœ¨å“ªä¸ªè®¾å¤‡ä¸Šè®¡ç®—

2. ä¸ºä»€ä¹ˆå®šä¹‰deviceå°±èƒ½ç”¨GPU?
   â†’ device æœ¬èº«ä¸"æŒ‘é€‰"GPU
   â†’ model.to(device) æŠŠæ¨¡å‹ç§»åˆ°æŒ‡å®šè®¾å¤‡
   â†’ data.to(device) æŠŠæ•°æ®ç§»åˆ°æŒ‡å®šè®¾å¤‡
   â†’ PyTorchè‡ªåŠ¨åœ¨æ•°æ®æ‰€åœ¨è®¾å¤‡ä¸Šè®¡ç®—

3. å•GPU vs å¤šGPU
   â†’ å•GPU: device = torch.device('cuda') è‡ªåŠ¨ä½¿ç”¨GPU 0
   â†’ å¤šGPU: device = torch.device('cuda:1') æŒ‡å®šä½¿ç”¨GPU 1

4. æœ€ä½³å®è·µ
   â†’ å§‹ç»ˆç”¨ device å¯¹è±¡ (ä¸è¦ç”¨ .cuda())
   â†’ æ¨¡å‹ç§»åŠ¨ä¸€æ¬¡,æ•°æ®æ¯batchç§»åŠ¨
   â†’ ä½¿ç”¨ map_location æ­£ç¡®åŠ è½½æ¨¡å‹

ğŸ’¡ è®°ä½:
   device = åœ°å€æ ‡ç­¾
   model.to(device) = æŠŠæ¨¡å‹é€åˆ°è¿™ä¸ªåœ°å€
   data.to(device) = æŠŠæ•°æ®é€åˆ°è¿™ä¸ªåœ°å€
   â†’ PyTorch è‡ªåŠ¨åœ¨æ­£ç¡®åœ°å€è®¡ç®—!
""")


if __name__ == '__main__':
    main()
