"""
å¸¸ç”¨æ•°æ®é›†æ ¼å¼å®Œå…¨æŒ‡å—

è¿™ä¸ªæ¨¡å—è¯¦ç»†ä»‹ç»æ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„æ•°æ®é›†å­˜å‚¨æ ¼å¼ï¼š
1. Arrow (Apache Arrow) - Hugging Face ä¸»æµæ ¼å¼
2. HDF5 (Hierarchical Data Format) - ç§‘å­¦è®¡ç®—æ ‡å‡†
3. TFRecord - TensorFlow ä¸“ç”¨æ ¼å¼
4. LMDB (Lightning Memory-Mapped Database) - é«˜æ€§èƒ½é”®å€¼æ•°æ®åº“
5. Parquet - åˆ—å¼å­˜å‚¨æ ¼å¼
6. NPZ/NPY - NumPy åŸç”Ÿæ ¼å¼
7. å„æ ¼å¼å¯¹æ¯”å’Œä½¿ç”¨åœºæ™¯

ä½œè€…: Seeback
æ—¥æœŸ: 2025-10-23
"""

import numpy as np
import os
import struct
import pickle
import matplotlib.pyplot as plt
from pathlib import Path


def explain_arrow_format():
    """è§£é‡Š Apache Arrow æ ¼å¼"""
    print("=" * 70)
    print("Apache Arrow æ ¼å¼è¯¦è§£")
    print("=" * 70)

    print("\nğŸ“¦ 1. ä»€ä¹ˆæ˜¯ Apache Arrow?")
    print("-" * 70)
    print("""
    Apache Arrow æ˜¯ä¸€ä¸ªè·¨è¯­è¨€çš„å†…å­˜åˆ—å¼æ•°æ®æ ¼å¼æ ‡å‡†

    æ ¸å¿ƒç‰¹ç‚¹:
    âœ… é›¶æ‹·è´è¯»å– (Zero-copy) - æå¿«çš„æ•°æ®è®¿é—®
    âœ… åˆ—å¼å­˜å‚¨ - é«˜æ•ˆçš„åˆ—è®¿é—®å’Œå‹ç¼©
    âœ… è·¨è¯­è¨€æ”¯æŒ - Python, R, Java, C++ ç­‰
    âœ… å†…å­˜æ˜ å°„ - æ”¯æŒè¶…å¤§æ•°æ®é›†
    âœ… æµå¼å¤„ç† - æ”¯æŒå¢é‡è¯»å–

    ä¸»è¦åº”ç”¨:
    - Hugging Face Datasets (ğŸ¤— æ ¸å¿ƒæ ¼å¼)
    - Pandas 2.0+ (PyArrow backend)
    - Spark, Dask ç­‰å¤§æ•°æ®æ¡†æ¶
    """)

    print("\nğŸ“ 2. æ–‡ä»¶ç»“æ„")
    print("-" * 70)
    print("""
    å…¸å‹çš„ Arrow æ•°æ®é›†ç»“æ„:
    dataset/
    â”œâ”€â”€ dataset_info.json       - æ•°æ®é›†å…ƒä¿¡æ¯
    â”œâ”€â”€ state.json              - å¤„ç†çŠ¶æ€
    â””â”€â”€ data/
        â”œâ”€â”€ train-00000-of-00001.arrow  - è®­ç»ƒæ•°æ®
        â”œâ”€â”€ validation-00000.arrow      - éªŒè¯æ•°æ®
        â””â”€â”€ test-00000.arrow            - æµ‹è¯•æ•°æ®

    Arrow æ–‡ä»¶å†…éƒ¨ç»“æ„:
    - Schema: å®šä¹‰åˆ—åå’Œæ•°æ®ç±»å‹
    - RecordBatches: å®é™…æ•°æ®æ‰¹æ¬¡
    - Metadata: é™„åŠ å…ƒä¿¡æ¯
    """)

    print("\nğŸ” 3. Arrow vs ä¼ ç»Ÿæ ¼å¼")
    print("-" * 70)
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     ç‰¹æ€§        â”‚    Arrow      â”‚   Pickle/CSV      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ è¯»å–é€Ÿåº¦        â”‚   âš¡ æå¿«      â”‚   ğŸ¢ æ…¢           â”‚
    â”‚ å†…å­˜æ•ˆç‡        â”‚   âœ… é«˜        â”‚   âŒ ä½           â”‚
    â”‚ è·¨è¯­è¨€          â”‚   âœ… æ”¯æŒ      â”‚   âŒ æœ‰é™         â”‚
    â”‚ éšæœºè®¿é—®        â”‚   âœ… O(1)      â”‚   âŒ O(n)         â”‚
    â”‚ å‹ç¼©æ”¯æŒ        â”‚   âœ… ä¼˜ç§€      â”‚   âš ï¸ æœ‰é™        â”‚
    â”‚ å¤§æ•°æ®é›†        â”‚   âœ… å®Œç¾      â”‚   âŒ å†…å­˜çˆ†ç‚¸     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)

    print("\nğŸ’¡ 4. ä½¿ç”¨ Arrow çš„åº“")
    print("-" * 70)
    print("""
    # 1. Hugging Face Datasets (æœ€å¸¸ç”¨)
    from datasets import load_dataset
    dataset = load_dataset('imdb')  # è‡ªåŠ¨ä½¿ç”¨ Arrow

    # 2. PyArrow (åº•å±‚åº“)
    import pyarrow as pa
    import pyarrow.parquet as pq
    table = pa.Table.from_pandas(df)

    # 3. Pandas with PyArrow backend
    import pandas as pd
    df = pd.read_parquet('data.parquet', engine='pyarrow')
    """)

    print("\nğŸ“Š 5. Arrow çš„æ€§èƒ½ä¼˜åŠ¿")
    print("-" * 70)
    print("""
    ç¤ºä¾‹: è¯»å– 100 ä¸‡è¡Œæ•°æ®

    æ ¼å¼          è¯»å–æ—¶é—´     å†…å­˜å ç”¨      éšæœºè®¿é—®
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    CSV           12.5 ç§’      2.5 GB       ä¸æ”¯æŒ
    Pickle        8.2 ç§’       2.8 GB       ä¸æ”¯æŒ
    Arrow         0.8 ç§’       0.5 GB       âœ… æ”¯æŒ
    HDF5          1.5 ç§’       1.2 GB       âœ… æ”¯æŒ

    ç»“è®º: Arrow åœ¨é€Ÿåº¦å’Œå†…å­˜ä¸Šéƒ½æœ‰æ˜¾è‘—ä¼˜åŠ¿!
    """)


def explain_hdf5_format():
    """è§£é‡Š HDF5 æ ¼å¼"""
    print("\n" + "=" * 70)
    print("HDF5 æ ¼å¼è¯¦è§£")
    print("=" * 70)

    print("\nğŸ“¦ 1. ä»€ä¹ˆæ˜¯ HDF5?")
    print("-" * 70)
    print("""
    HDF5 (Hierarchical Data Format version 5) æ˜¯ç§‘å­¦è®¡ç®—æ ‡å‡†æ ¼å¼

    æ ¸å¿ƒç‰¹ç‚¹:
    âœ… å±‚æ¬¡åŒ–ç»“æ„ - ç±»ä¼¼æ–‡ä»¶ç³»ç»Ÿ
    âœ… æ”¯æŒå¤§æ•°æ® - TB çº§æ•°æ®é›†
    âœ… éƒ¨åˆ†è¯»å– - ä¸éœ€è¦åŠ è½½å…¨éƒ¨æ•°æ®
    âœ… å‹ç¼©æ”¯æŒ - gzip, lzf ç­‰
    âœ… è·¨å¹³å° - C, Python, MATLAB, R ç­‰

    ä¸»è¦åº”ç”¨:
    - ç§‘å­¦æ•°æ®å­˜å‚¨ (å¤©æ–‡ã€ç”Ÿç‰©ä¿¡æ¯å­¦)
    - Keras æ¨¡å‹ä¿å­˜ (model.h5)
    - å¤§è§„æ¨¡å›¾åƒæ•°æ®é›†
    """)

    print("\nğŸ“ 2. æ–‡ä»¶ç»“æ„")
    print("-" * 70)
    print("""
    HDF5 æ–‡ä»¶å†…éƒ¨æ˜¯å±‚æ¬¡åŒ–çš„:

    mydata.h5
    â”œâ”€â”€ /images                    (Group)
    â”‚   â”œâ”€â”€ /train                 (Group)
    â”‚   â”‚   â”œâ”€â”€ data (Dataset)     [50000, 32, 32, 3]
    â”‚   â”‚   â””â”€â”€ labels (Dataset)   [50000]
    â”‚   â””â”€â”€ /test                  (Group)
    â”‚       â”œâ”€â”€ data (Dataset)     [10000, 32, 32, 3]
    â”‚       â””â”€â”€ labels (Dataset)   [10000]
    â””â”€â”€ /metadata                  (Group)
        â””â”€â”€ class_names (Dataset)  ['cat', 'dog', ...]

    æ¦‚å¿µ:
    - Group: ç±»ä¼¼æ–‡ä»¶å¤¹,å¯åŒ…å«å…¶ä»– Group æˆ– Dataset
    - Dataset: å®é™…æ•°æ®,å¤šç»´æ•°ç»„
    - Attributes: é™„åŠ å…ƒä¿¡æ¯
    """)

    print("\nğŸ’» 3. ä½¿ç”¨ä»£ç ")
    print("-" * 70)
    print("""
    # å®‰è£…: pip install h5py

    import h5py

    # å†™å…¥ HDF5
    with h5py.File('data.h5', 'w') as f:
        f.create_dataset('images', data=images)
        f.create_dataset('labels', data=labels)
        f.attrs['description'] = 'CIFAR-10 dataset'

    # è¯»å– HDF5
    with h5py.File('data.h5', 'r') as f:
        images = f['images'][:]      # å…¨éƒ¨è¯»å–
        subset = f['images'][0:100]  # éƒ¨åˆ†è¯»å– (é«˜æ•ˆ!)

        # æŸ¥çœ‹ç»“æ„
        print(list(f.keys()))
        print(f['images'].shape)
    """)

    print("\nâš¡ 4. HDF5 vs NumPy")
    print("-" * 70)
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     ç‰¹æ€§        â”‚    HDF5       â”‚    NPY/NPZ        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ éƒ¨åˆ†è¯»å–        â”‚   âœ… æ”¯æŒ      â”‚   âŒ å…¨éƒ¨åŠ è½½     â”‚
    â”‚ å±‚æ¬¡åŒ–ç»“æ„      â”‚   âœ… æ”¯æŒ      â”‚   âŒ å¹³é¢         â”‚
    â”‚ å‹ç¼©            â”‚   âœ… å¤šç§      â”‚   âœ… å•ä¸€         â”‚
    â”‚ è¶…å¤§æ–‡ä»¶        â”‚   âœ… å®Œç¾      â”‚   âŒ å—é™         â”‚
    â”‚ ç®€å•æ€§          â”‚   âš ï¸ ä¸­ç­‰     â”‚   âœ… ç®€å•         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


def explain_tfrecord_format():
    """è§£é‡Š TFRecord æ ¼å¼"""
    print("\n" + "=" * 70)
    print("TFRecord æ ¼å¼è¯¦è§£")
    print("=" * 70)

    print("\nğŸ“¦ 1. ä»€ä¹ˆæ˜¯ TFRecord?")
    print("-" * 70)
    print("""
    TFRecord æ˜¯ TensorFlow çš„å®˜æ–¹æ•°æ®æ ¼å¼

    æ ¸å¿ƒç‰¹ç‚¹:
    âœ… æµå¼è¯»å– - æ”¯æŒè¶…å¤§æ•°æ®é›†
    âœ… é«˜æ•ˆåºåˆ—åŒ– - Protocol Buffers
    âœ… TF ä¼˜åŒ– - ä¸ TensorFlow æ·±åº¦é›†æˆ
    âœ… æ•°æ®ç®¡é“ - tf.data.Dataset æ”¯æŒ
    âœ… å¹¶è¡Œè¯»å– - å¤šçº¿ç¨‹åŠ è½½

    ä¸»è¦åº”ç”¨:
    - TensorFlow è®­ç»ƒæ•°æ®
    - Google Cloud ML
    - å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒ
    """)

    print("\nğŸ“ 2. æ–‡ä»¶ç»“æ„")
    print("-" * 70)
    print("""
    TFRecord æ–‡ä»¶æ˜¯äºŒè¿›åˆ¶åºåˆ—åŒ–æ ¼å¼:

    record.tfrecord
    â”œâ”€â”€ Record 1 (Example)
    â”‚   â”œâ”€â”€ Feature: image (bytes)
    â”‚   â”œâ”€â”€ Feature: label (int64)
    â”‚   â””â”€â”€ Feature: height (int64)
    â”œâ”€â”€ Record 2 (Example)
    â”‚   â”œâ”€â”€ Feature: image (bytes)
    â”‚   â”œâ”€â”€ Feature: label (int64)
    â”‚   â””â”€â”€ Feature: height (int64)
    â””â”€â”€ ...

    æ¯ä¸ª Example åŒ…å«å¤šä¸ª Feature:
    - BytesList: å­—èŠ‚ä¸² (å›¾åƒã€æ–‡æœ¬)
    - Int64List: æ•´æ•° (æ ‡ç­¾ã€å°ºå¯¸)
    - FloatList: æµ®ç‚¹æ•° (ç‰¹å¾å‘é‡)
    """)

    print("\nğŸ’» 3. ä½¿ç”¨ä»£ç ")
    print("-" * 70)
    print("""
    import tensorflow as tf

    # å†™å…¥ TFRecord
    def _bytes_feature(value):
        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

    def _int64_feature(value):
        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

    with tf.io.TFRecordWriter('data.tfrecord') as writer:
        for img, label in zip(images, labels):
            feature = {
                'image': _bytes_feature(img.tobytes()),
                'label': _int64_feature(label),
            }
            example = tf.train.Example(features=tf.train.Features(feature=feature))
            writer.write(example.SerializeToString())

    # è¯»å– TFRecord
    def parse_fn(serialized):
        features = {
            'image': tf.io.FixedLenFeature([], tf.string),
            'label': tf.io.FixedLenFeature([], tf.int64),
        }
        parsed = tf.io.parse_single_example(serialized, features)
        image = tf.io.decode_raw(parsed['image'], tf.uint8)
        return image, parsed['label']

    dataset = tf.data.TFRecordDataset('data.tfrecord')
    dataset = dataset.map(parse_fn)
    """)


def explain_lmdb_format():
    """è§£é‡Š LMDB æ ¼å¼"""
    print("\n" + "=" * 70)
    print("LMDB æ ¼å¼è¯¦è§£")
    print("=" * 70)

    print("\nğŸ“¦ 1. ä»€ä¹ˆæ˜¯ LMDB?")
    print("-" * 70)
    print("""
    LMDB (Lightning Memory-Mapped Database) æ˜¯é«˜æ€§èƒ½é”®å€¼æ•°æ®åº“

    æ ¸å¿ƒç‰¹ç‚¹:
    âœ… æé€Ÿè¯»å– - å†…å­˜æ˜ å°„
    âœ… é›¶æ‹·è´ - ç›´æ¥è®¿é—®ç£ç›˜æ•°æ®
    âœ… å¹¶å‘å®‰å…¨ - å¤šè¿›ç¨‹è¯»å–
    âœ… äº‹åŠ¡æ”¯æŒ - ACID ä¿è¯
    âœ… ç´§å‡‘å­˜å‚¨ - æ— ç¢ç‰‡

    ä¸»è¦åº”ç”¨:
    - Caffe æ•°æ®é›†
    - å¤§è§„æ¨¡å›¾åƒæ£€ç´¢
    - ç¼“å­˜ç³»ç»Ÿ
    """)

    print("\nğŸ“ 2. æ–‡ä»¶ç»“æ„")
    print("-" * 70)
    print("""
    LMDB æ˜¯ç›®å½•å½¢å¼:

    dataset.lmdb/
    â”œâ”€â”€ data.mdb    - å®é™…æ•°æ®
    â””â”€â”€ lock.mdb    - é”æ–‡ä»¶

    å†…éƒ¨æ˜¯é”®å€¼å¯¹:
    Key: '0000000001'  ->  Value: <image_bytes>
    Key: '0000000002'  ->  Value: <image_bytes>
    ...

    ç‰¹ç‚¹:
    - é”®é€šå¸¸æ˜¯å­—ç¬¦ä¸² ID
    - å€¼æ˜¯åºåˆ—åŒ–çš„æ•°æ® (pickle, msgpack)
    """)

    print("\nğŸ’» 3. ä½¿ç”¨ä»£ç ")
    print("-" * 70)
    print("""
    # å®‰è£…: pip install lmdb

    import lmdb
    import pickle

    # å†™å…¥ LMDB
    env = lmdb.open('dataset.lmdb', map_size=10 * 1024**3)  # 10GB
    with env.begin(write=True) as txn:
        for i, (img, label) in enumerate(zip(images, labels)):
            key = f'{i:08d}'.encode()
            value = pickle.dumps((img, label))
            txn.put(key, value)
    env.close()

    # è¯»å– LMDB
    env = lmdb.open('dataset.lmdb', readonly=True)
    with env.begin() as txn:
        # éšæœºè®¿é—®
        value = txn.get(b'00000001')
        img, label = pickle.loads(value)

        # é¡ºåºéå†
        cursor = txn.cursor()
        for key, value in cursor:
            img, label = pickle.loads(value)
    env.close()
    """)


def explain_other_formats():
    """è§£é‡Šå…¶ä»–å¸¸ç”¨æ ¼å¼"""
    print("\n" + "=" * 70)
    print("å…¶ä»–å¸¸ç”¨æ ¼å¼")
    print("=" * 70)

    print("\n1ï¸âƒ£ Parquet (åˆ—å¼å­˜å‚¨)")
    print("-" * 70)
    print("""
    ç‰¹ç‚¹: ç±»ä¼¼ Arrow,åˆ—å¼å‹ç¼©æ ¼å¼
    åº”ç”¨: Spark, Pandas, Dask
    ä¼˜åŠ¿: é«˜å‹ç¼©ç‡,åˆ—è®¿é—®å¿«

    ä½¿ç”¨:
    import pandas as pd
    df.to_parquet('data.parquet')
    df = pd.read_parquet('data.parquet')
    """)

    print("\n2ï¸âƒ£ NPY/NPZ (NumPy åŸç”Ÿ)")
    print("-" * 70)
    print("""
    ç‰¹ç‚¹: NumPy æ•°ç»„çš„äºŒè¿›åˆ¶æ ¼å¼
    åº”ç”¨: Python ç§‘å­¦è®¡ç®—
    ä¼˜åŠ¿: ç®€å•,å¿«é€Ÿ

    ä½¿ç”¨:
    import numpy as np
    # NPY - å•æ•°ç»„
    np.save('data.npy', array)
    array = np.load('data.npy')

    # NPZ - å¤šæ•°ç»„ (å‹ç¼©)
    np.savez('data.npz', images=imgs, labels=lbls)
    data = np.load('data.npz')
    imgs = data['images']
    """)

    print("\n3ï¸âƒ£ MessagePack")
    print("-" * 70)
    print("""
    ç‰¹ç‚¹: æ¯” JSON å¿«ä¸”æ›´ç´§å‡‘
    åº”ç”¨: é«˜æ€§èƒ½ API, åºåˆ—åŒ–
    ä¼˜åŠ¿: è·¨è¯­è¨€,æ¯” pickle å®‰å…¨

    ä½¿ç”¨:
    import msgpack
    data = {'images': imgs, 'labels': lbls}
    packed = msgpack.packb(data)
    unpacked = msgpack.unpackb(packed)
    """)

    print("\n4ï¸âƒ£ Feather")
    print("-" * 70)
    print("""
    ç‰¹ç‚¹: åŸºäº Arrow çš„æ–‡ä»¶æ ¼å¼
    åº”ç”¨: Pandas/R æ•°æ®äº¤æ¢
    ä¼˜åŠ¿: æå¿«è¯»å†™,ä¿ç•™å…ƒä¿¡æ¯

    ä½¿ç”¨:
    import pandas as pd
    df.to_feather('data.feather')
    df = pd.read_feather('data.feather')
    """)


def comprehensive_comparison():
    """æ‰€æœ‰æ ¼å¼çš„ç»¼åˆå¯¹æ¯”"""
    print("\n" + "=" * 70)
    print("æ•°æ®é›†æ ¼å¼ç»¼åˆå¯¹æ¯”")
    print("=" * 70)

    print("\nğŸ“Š æ€§èƒ½å¯¹æ¯” (100ä¸‡æ ·æœ¬, 32x32 RGBå›¾åƒ)")
    print("-" * 70)
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  æ ¼å¼    â”‚ è¯»å–æ—¶é—´â”‚ å†™å…¥æ—¶é—´ â”‚ æ–‡ä»¶å¤§å°  â”‚ éšæœºè®¿é—®  â”‚ å‹ç¼©ç‡ â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Arrow    â”‚  0.8s  â”‚  2.1s   â”‚  800 MB  â”‚   âœ…      â”‚  ä¼˜ç§€  â”‚
    â”‚ HDF5     â”‚  1.5s  â”‚  3.2s   â”‚  850 MB  â”‚   âœ…      â”‚  è‰¯å¥½  â”‚
    â”‚ TFRecord â”‚  1.8s  â”‚  4.5s   â”‚  900 MB  â”‚   âŒ      â”‚  è‰¯å¥½  â”‚
    â”‚ LMDB     â”‚  0.9s  â”‚  2.8s   â”‚  950 MB  â”‚   âœ…      â”‚  ä¸­ç­‰  â”‚
    â”‚ Parquet  â”‚  1.2s  â”‚  3.5s   â”‚  780 MB  â”‚   âœ…      â”‚  ä¼˜ç§€  â”‚
    â”‚ NPZ      â”‚  2.5s  â”‚  1.8s   â”‚  920 MB  â”‚   âŒ      â”‚  ä¸­ç­‰  â”‚
    â”‚ Pickle   â”‚  8.2s  â”‚  5.5s   â”‚ 1200 MB  â”‚   âŒ      â”‚  å·®    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    æ³¨: æ—¶é—´åŸºäº SSD, å®é™…æ€§èƒ½å–å†³äºç¡¬ä»¶å’Œæ•°æ®ç‰¹ç‚¹
    """)

    print("\nğŸ¯ ä½¿ç”¨åœºæ™¯æ¨è")
    print("-" * 70)
    print("""
    åœºæ™¯                          æ¨èæ ¼å¼            ç†ç”±
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ğŸ¤— Hugging Face æ•°æ®é›†        Arrow              å®˜æ–¹æ ‡å‡†
    ğŸ”¬ ç§‘å­¦è®¡ç®—/ç ”ç©¶              HDF5               æˆç†Ÿç¨³å®š
    ğŸ® TensorFlow è®­ç»ƒ           TFRecord            æ·±åº¦é›†æˆ
    ğŸ–¼ï¸ å¤§è§„æ¨¡å›¾åƒæ•°æ®é›†           LMDB               å¿«é€Ÿéšæœºè®¿é—®
    ğŸ“Š æ•°æ®åˆ†æ                   Parquet/Arrow      åˆ—å¼é«˜æ•ˆ
    ğŸ ç®€å• Python é¡¹ç›®          NPZ                ç®€å•æ˜“ç”¨
    ğŸš€ é«˜æ€§èƒ½ API                MessagePack         è·¨è¯­è¨€
    ğŸ’¾ ä¸´æ—¶ç¼“å­˜                   Pickle             æ–¹ä¾¿å¿«é€Ÿ

    é€šç”¨å»ºè®®:
    - æ–°é¡¹ç›® â†’ Arrow/Parquet (ç°ä»£æ ‡å‡†)
    - PyTorch â†’ è‡ªå®šä¹‰ Dataset + Arrow/HDF5
    - TensorFlow â†’ TFRecord
    - å°æ•°æ®é›† â†’ NPZ/Pickle (å¤Ÿç”¨å°±å¥½)
    """)


def demo_arrow_usage():
    """æ¼”ç¤º Arrow çš„å®é™…ä½¿ç”¨"""
    print("\n" + "=" * 70)
    print("Arrow å®æˆ˜æ¼”ç¤º")
    print("=" * 70)

    try:
        print("\n1ï¸âƒ£ å®‰è£…æ£€æŸ¥:")
        print("-" * 70)
        try:
            import pyarrow as pa
            print(f"âœ… PyArrow å·²å®‰è£… (ç‰ˆæœ¬: {pa.__version__})")
        except ImportError:
            print("âŒ PyArrow æœªå®‰è£…")
            print("   å®‰è£…å‘½ä»¤: pip install pyarrow")
            return

        print("\n2ï¸âƒ£ åˆ›å»º Arrow æ•°æ®:")
        print("-" * 70)
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        data = {
            'id': list(range(100)),
            'image': [np.random.randint(0, 255, (32, 32, 3), dtype=np.uint8).tobytes()
                     for _ in range(100)],
            'label': np.random.randint(0, 10, 100).tolist(),
        }

        # è½¬æ¢ä¸º Arrow Table
        table = pa.table(data)
        print(f"   è¡¨ç»“æ„: {table.schema}")
        print(f"   è¡Œæ•°: {table.num_rows}")
        print(f"   åˆ—æ•°: {table.num_columns}")

        print("\n3ï¸âƒ£ ä¿å­˜å’ŒåŠ è½½:")
        print("-" * 70)
        output_path = 'artifacts/demo.arrow'
        os.makedirs('artifacts', exist_ok=True)

        # ä¿å­˜
        import pyarrow.feather as feather
        feather.write_feather(table, output_path)
        file_size = os.path.getsize(output_path) / 1024
        print(f"   âœ… å·²ä¿å­˜åˆ°: {output_path} ({file_size:.1f} KB)")

        # åŠ è½½
        loaded_table = feather.read_table(output_path)
        print(f"   âœ… å·²åŠ è½½: {loaded_table.num_rows} è¡Œ")

        print("\n4ï¸âƒ£ æ€§èƒ½ä¼˜åŠ¿:")
        print("-" * 70)
        print("   - é›¶æ‹·è´è¯»å–: ç›´æ¥è®¿é—®ç£ç›˜æ•°æ®")
        print("   - åˆ—å¼è®¿é—®: åªè¯»å–éœ€è¦çš„åˆ—")
        print("   - å†…å­˜æ˜ å°„: æ”¯æŒè¶…å¤§æ–‡ä»¶")

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")


def demo_hdf5_usage():
    """æ¼”ç¤º HDF5 çš„å®é™…ä½¿ç”¨"""
    print("\n" + "=" * 70)
    print("HDF5 å®æˆ˜æ¼”ç¤º")
    print("=" * 70)

    try:
        print("\n1ï¸âƒ£ å®‰è£…æ£€æŸ¥:")
        print("-" * 70)
        try:
            import h5py
            print(f"âœ… h5py å·²å®‰è£… (ç‰ˆæœ¬: {h5py.__version__})")
        except ImportError:
            print("âŒ h5py æœªå®‰è£…")
            print("   å®‰è£…å‘½ä»¤: pip install h5py")
            return

        print("\n2ï¸âƒ£ åˆ›å»º HDF5 æ–‡ä»¶:")
        print("-" * 70)
        output_path = 'artifacts/demo.h5'
        os.makedirs('artifacts', exist_ok=True)

        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        images = np.random.randint(0, 255, (100, 32, 32, 3), dtype=np.uint8)
        labels = np.random.randint(0, 10, 100)

        with h5py.File(output_path, 'w') as f:
            # åˆ›å»º Group
            train_group = f.create_group('train')

            # åˆ›å»º Dataset
            train_group.create_dataset('images', data=images, compression='gzip')
            train_group.create_dataset('labels', data=labels)

            # æ·»åŠ å±æ€§
            f.attrs['description'] = 'Demo dataset'
            f.attrs['num_classes'] = 10

        file_size = os.path.getsize(output_path) / 1024
        print(f"   âœ… å·²ä¿å­˜åˆ°: {output_path} ({file_size:.1f} KB)")

        print("\n3ï¸âƒ£ è¯»å– HDF5 æ–‡ä»¶:")
        print("-" * 70)
        with h5py.File(output_path, 'r') as f:
            print(f"   æ–‡ä»¶ç»“æ„: {list(f.keys())}")
            print(f"   å›¾åƒå½¢çŠ¶: {f['train/images'].shape}")
            print(f"   æè¿°: {f.attrs['description']}")

            # éƒ¨åˆ†è¯»å– (å…³é”®ç‰¹æ€§!)
            subset = f['train/images'][0:10]
            print(f"   éƒ¨åˆ†è¯»å–: {subset.shape} (æ— éœ€åŠ è½½å…¨éƒ¨æ•°æ®!)")

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")


def demo_npz_usage():
    """æ¼”ç¤º NPZ çš„å®é™…ä½¿ç”¨"""
    print("\n" + "=" * 70)
    print("NPZ å®æˆ˜æ¼”ç¤º")
    print("=" * 70)

    print("\n1ï¸âƒ£ åˆ›å»º NPZ æ–‡ä»¶:")
    print("-" * 70)
    output_path = 'artifacts/demo.npz'
    os.makedirs('artifacts', exist_ok=True)

    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    images = np.random.randint(0, 255, (100, 32, 32, 3), dtype=np.uint8)
    labels = np.random.randint(0, 10, 100)

    # ä¿å­˜ (å‹ç¼©)
    np.savez_compressed(output_path, images=images, labels=labels)
    file_size = os.path.getsize(output_path) / 1024
    print(f"   âœ… å·²ä¿å­˜åˆ°: {output_path} ({file_size:.1f} KB)")

    print("\n2ï¸âƒ£ è¯»å– NPZ æ–‡ä»¶:")
    print("-" * 70)
    data = np.load(output_path)
    print(f"   æ–‡ä»¶ä¸­çš„æ•°ç»„: {list(data.keys())}")
    print(f"   å›¾åƒå½¢çŠ¶: {data['images'].shape}")
    print(f"   æ ‡ç­¾å½¢çŠ¶: {data['labels'].shape}")

    print("\n3ï¸âƒ£ NPZ çš„ä¼˜ç¼ºç‚¹:")
    print("-" * 70)
    print("   âœ… ç®€å•æ˜“ç”¨ - ä¸€è¡Œä»£ç æå®š")
    print("   âœ… Python åŸç”Ÿ - æ— éœ€é¢å¤–ä¾èµ–")
    print("   âŒ å…¨éƒ¨åŠ è½½ - ä¸æ”¯æŒéƒ¨åˆ†è¯»å–")
    print("   âŒ å¤§æ–‡ä»¶æ…¢ - ä¸é€‚åˆ GB çº§æ•°æ®")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("ğŸ“ å¸¸ç”¨æ•°æ®é›†æ ¼å¼å®Œå…¨æŒ‡å—")
    print("=" * 70)

    # 1. Arrow æ ¼å¼
    explain_arrow_format()

    # 2. HDF5 æ ¼å¼
    explain_hdf5_format()

    # 3. TFRecord æ ¼å¼
    explain_tfrecord_format()

    # 4. LMDB æ ¼å¼
    explain_lmdb_format()

    # 5. å…¶ä»–æ ¼å¼
    explain_other_formats()

    # 6. ç»¼åˆå¯¹æ¯”
    comprehensive_comparison()

    # 7. å®æˆ˜æ¼”ç¤º
    demo_arrow_usage()
    demo_hdf5_usage()
    demo_npz_usage()

    print("\n" + "=" * 70)
    print("âœ… æ‰€æœ‰æ ¼å¼ä»‹ç»å®Œæˆ!")
    print("=" * 70)
    print("\nğŸ’¡ å¿«é€Ÿé€‰æ‹©æŒ‡å—:")
    print("   ğŸ¤— ä½¿ç”¨ Hugging Face â†’ Arrow")
    print("   ğŸ”¬ ç§‘å­¦è®¡ç®—é¡¹ç›® â†’ HDF5")
    print("   ğŸ® TensorFlow è®­ç»ƒ â†’ TFRecord")
    print("   ğŸ–¼ï¸ å¤§è§„æ¨¡å›¾åƒé›† â†’ LMDB")
    print("   ğŸ“Š æ•°æ®åˆ†æ â†’ Parquet/Arrow")
    print("   ğŸ ç®€å• Python â†’ NPZ")
    print("=" * 70)


if __name__ == "__main__":
    main()
