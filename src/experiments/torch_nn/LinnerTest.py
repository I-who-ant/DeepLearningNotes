
# 例子：构造一个简单的尾部结构

import torch
from torch import nn

# 假设卷积部分输出 256@4×4 → flatten 后 4096 维
conv_output = torch.randn(8, 4096)  # batch=8的意思是每次输入8张图片，每张图片的特征向量是4096维


## 你决定的超参数 , 512 是一个常见的选择，也可以根据任务需求调整 ,
hidden_units = 512

classifier = nn.Sequential(

    # 4096 是卷积部分输出的特征向量维度，hidden_units 是你决定的超参数
    # 全连接层将 4096 维特征映射到 hidden_units 维, 得到 [batch, hidden_units],
    nn.Linear(4096, hidden_units),


    nn.ReLU(),# 对全连接层的输出应用ReLU激活函数，引入非线性
    nn.Linear(hidden_units, 10)  # 输出 10 个类别的 logits
)

logits = classifier(conv_output)
print(logits.shape)  # torch.Size([8, 10])


# - 第一层
# Linear(4096, 512)：随机初始化
# 512×4096
# 的权重矩阵，每次前向传播时乘以输入、加偏置，输出形状[batch,
# 512]。
# - 第二层
# Linear(512, 10)：再把
# 512
# 维特征投射到
# 10
# 维空间，得到[batch, 10]
# 的
# logits。
# - 训练过程中，网络自动调整这两层的权重，使得
# logits
# 对真实标签的交叉熵损失最小。
#
# 6.
# hidden_units
# 取多少？
#
# - 没有唯一答案，取决于模型复杂度、数据规模和过拟合风险。
# - 值太小，表达能力不足；太大，参数暴增，易过拟合或训练慢。
# - 常见做法是从
# 256 / 512
# 起步，结合
# Dropout、正则、BatchNorm
# 等手段平衡效果与泛化。
#
# 总之：
#
# - hidden_units
# 是你设计网络时决定的隐藏层大小，不是外部固定的。
# - 线性层的权重都是随机初始化后靠训练学习的。
# - 通过
# Linear(hidden_units, 10)，你明确告诉网络“我要输出
# 10
# 个类别的打分”，这就是输出
# logit
# 数量与具体分类任
# 务的一对一对应关系。



#
#
# • 先澄清几个关键点，再解释 hidden_units 究竟是什么、来自哪里、为什么能输出 10 个类别的得分：
#
#   1. hidden_units 不是数据集给的固定数字
#
#   - 这是你在设计网络结构时人为选择的一个超参数（hyperparameter），常见取值如 256、512、1024 等。
#   - 它表示全连接隐藏层的神经元数量：Linear(4096, hidden_units) 的输出向量长度就是 hidden_units。这个层位于“卷积
#     特征 → 最终分类”之间，用来进行更灵活的组合、抽象。
#
#   2. 隐藏层权重怎么来的？
#
#   - 当你实例化 nn.Linear(4096, hidden_units) 时，PyTorch 会随机初始化一组权重矩阵 W_hidden ∈
#     ℝ^{hidden_units×4096} 和偏置向量 b_hidden ∈ ℝ^{hidden_units}。
#   - 这些权重在训练过程中通过反向传播不断更新。它们不是 CIFAR-10 提供的，而是模型在训练时学习出来的“如何把卷积特
#     征组合得更有判别力”的参数。
#
#   3. 为什么还要有隐藏层？
#
#   - 卷积输出 flatten 后的 4096 维向量（举例）已经包含了空间 → 语义的高度摘要。
#   - 隐藏层充当进一步的非线性变换：每个隐藏神经元都学会“撷取”特定模式，比如“如果这几组卷积特征同时激活，可能意味
#     着有猫耳朵样式”；不同神经元对应不同组合模式。
#   - 有了这层缓冲，你的最终输出层就能基于“更抽象、组合良好的特征”来做分类，提高准确率。
#
#   4. 为什么输出层 Linear(hidden_units, 10) 会得到 10 个类别的得分？
#
#   - 同样地，nn.Linear(hidden_units, 10) 会初始化 W_out ∈ ℝ^{10×hidden_units} 和 b_out ∈ ℝ^{10}。
#   - 这层的输出向量长度是 10，恰好对应 CIFAR-10 数据集的 10 个类别。
#   - 每个输出神经元（第 j 行）对隐藏层的所有特征做加权求和：
#
#     logit_j = Σ_{i=1}^{hidden_units} W_out[j, i] * hidden_vector[i] + b_out[j]
#     这就产生了类别 j 的“打分”（logit）。训练时，这些权重会被调整，让正确类别的 logit 尽可能大，错误类别尽可能
#     小。最后用 CrossEntropyLoss 或 Softmax 把 logits 转成概率，完成分类


#举个例子 :
# 假设我们要根据人的身高与体重预测“是否运动员”（二分类
# 0 / 1），就算一个极简的两层网络：
#
# import torch
# from torch import nn
#
# model = nn.Sequential(
#     nn.Linear(2, 3),  # 隐藏层：输入2维 → 输出3维
#     nn.ReLU(),
#     nn.Linear(3, 1),  # 输出层：隐藏3维 → 输出1维（logit）
# )
#
# - 输入层：2
# 个神经元，对应身高、体重。
# - 隐藏层：nn.Linear(2, 3)，有
# 3
# 个神经元。它会学习
# 3
# 套不同的权重，将“身高 / 体重”组合成
# 3
# 个新的特征，类似于
# 学会判断“是否高且瘦”“是否重且不高”等模式。
# - ReLU
# 激活：让隐藏层输出带有非线性，能够描绘更多样的决策边界。
# - 输出层：nn.Linear(3, 1)，将这
# 3
# 个中间特征再组合成一个最终得分（logit），之后通常接 Sigmoid或CrossEntropyLoss获得类别概率。
#
# 训练后，这个模型的内部计算类似：

# 输入(身高, 体重) → 隐藏层：3
# 个中间特征 → 输出层：二分类得分
#
# 这些“中间特征”就是隐藏层的输出，它们不是人可以直接理解的数据，但对网络来说是构建最终判断的“内在证据”。


# 卷积网络里的隐藏层
#
# 以典型
# CIFAR - 10 CNN
# 为例：› 创建一个3@32x32 经过 5x5 kernel 到32@32x32 ->32@16x16 ->32@8x8 -> 64@8x8 ->64@4x4 ->64 ->output
#   的CIFAR10 的例子在src/experiments/torch_nn中(class seeback(nn.Module):


# 输入(3 @ 32×32)
# → 卷积 + ReLU → 隐藏特征(32 @ 32×32) : 调整好卷积核大小、步长、填充等参数，就可以使得输出特征图与输入等大
#                                       (看公式：(输入尺寸 - 卷积核尺寸 + 2 * 填充) / 步长 + 1)
# → 卷积 + ReLU → 隐藏特征(64 @ 16×16)
# → 卷积 + ReLU → 隐藏特征(128 @ 8×8)
# → flatten → 隐藏特征向量(8192)
# → Linear(8192, 512) + ReLU → 隐藏特征(512)
# → Linear(512, 10) → 输出
# logits
#
# 这里的每一个卷积层、全连接层都可以看成隐藏层。它们不直接输出分类结果，但逐步把原始像素变成越来越有意义的抽象
# 描述（边缘 → 纹理 → 物体部件 → 类别线索）。最后那层
# Linear(512, 10)
# 才是真正的输出层，但能做出正确判断，依赖
# 的是前面一系列隐藏层学习到的丰富特征。

