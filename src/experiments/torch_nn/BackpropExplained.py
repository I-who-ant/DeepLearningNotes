"""
æ·±å…¥ç†è§£åå‘ä¼ æ’­ä¸æ¢¯åº¦
======================
æ ¸å¿ƒé—®é¢˜:
1. loss.backward() æ˜¯å¦‚ä½•è®¡ç®—æ¢¯åº¦çš„?
2. æ¢¯åº¦ä¸ºä»€ä¹ˆæ˜¯"æ–œç‡"?
3. ä¸ºä»€ä¹ˆèƒ½é€šè¿‡æ¢¯åº¦æ›´æ–°å‚æ•°æ¥å‡å°loss?
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# å°è¯•å¯¼å…¥matplotlib,å¦‚æœå¤±è´¥åˆ™è·³è¿‡å¯è§†åŒ–
try:
    import matplotlib.pyplot as plt
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("âš ï¸  matplotlibæœªå®‰è£…,å°†è·³è¿‡å¯è§†åŒ–éƒ¨åˆ†")


def demo_gradient_as_slope():
    """æ¼”ç¤º:æ¢¯åº¦å°±æ˜¯æ–œç‡"""
    print("="*70)
    print("Part 1: æ¢¯åº¦ = æ–œç‡çš„ç›´è§‚ç†è§£")
    print("="*70 + "\n")

    # ç®€å•å‡½æ•°: y = xÂ²
    print("è€ƒè™‘ç®€å•å‡½æ•°: L(w) = wÂ²")
    print()

    # åˆ›å»ºä¸€ä¸ªéœ€è¦æ¢¯åº¦çš„å˜é‡
    w = torch.tensor([2.0], requires_grad=True)

    # è®¡ç®—æŸå¤±
    L = w ** 2

    print(f"å½“ w = {w.item():.1f} æ—¶:")
    print(f"  L(w) = wÂ² = {L.item():.1f}")

    # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
    L.backward()

    print(f"\nåå‘ä¼ æ’­è®¡ç®—å¾—åˆ°:")
    print(f"  æ¢¯åº¦ dL/dw = {w.grad.item():.1f}")
    print()
    print(f"æ‰‹åŠ¨è®¡ç®—éªŒè¯:")
    print(f"  dL/dw = d(wÂ²)/dw = 2w = 2Ã—{w.item():.1f} = {2*w.item():.1f} âœ…")
    print()

    # å¯è§†åŒ–
    if not HAS_MATPLOTLIB:
        print("\n(è·³è¿‡å¯è§†åŒ–éƒ¨åˆ†,éœ€è¦å®‰è£…matplotlib)")
        return

    print("="*70)
    print("å¯è§†åŒ–:æ¢¯åº¦æ˜¯æ›²çº¿çš„æ–œç‡")
    print("="*70)

    # ç»˜åˆ¶å‡½æ•°æ›²çº¿
    w_values = np.linspace(-3, 3, 100)
    L_values = w_values ** 2

    plt.figure(figsize=(12, 5))

    # å·¦å›¾:æŸå¤±å‡½æ•°æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(w_values, L_values, 'b-', linewidth=2, label='L(w) = wÂ²')

    # æ ‡æ³¨å½“å‰ç‚¹
    w_point = 2.0
    L_point = w_point ** 2
    gradient = 2 * w_point

    plt.plot(w_point, L_point, 'ro', markersize=10, label=f'å½“å‰ç‚¹ (w={w_point})')

    # ç»˜åˆ¶åˆ‡çº¿(æ–œç‡ = æ¢¯åº¦)
    tangent_x = np.linspace(w_point-1, w_point+1, 10)
    tangent_y = L_point + gradient * (tangent_x - w_point)
    plt.plot(tangent_x, tangent_y, 'r--', linewidth=2,
             label=f'åˆ‡çº¿æ–œç‡={gradient:.1f}')

    plt.xlabel('æƒé‡ w', fontsize=12)
    plt.ylabel('æŸå¤± L(w)', fontsize=12)
    plt.title('æŸå¤±å‡½æ•°ä¸æ¢¯åº¦(æ–œç‡)', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å³å›¾:æ¢¯åº¦éšwçš„å˜åŒ–
    plt.subplot(1, 2, 2)
    gradients = 2 * w_values
    plt.plot(w_values, gradients, 'g-', linewidth=2, label='æ¢¯åº¦ dL/dw = 2w')
    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
    plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)
    plt.plot(w_point, gradient, 'ro', markersize=10)

    plt.xlabel('æƒé‡ w', fontsize=12)
    plt.ylabel('æ¢¯åº¦ dL/dw', fontsize=12)
    plt.title('æ¢¯åº¦çš„å˜åŒ–', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('artifacts/gradient_as_slope.png', dpi=100, bbox_inches='tight')
    print("\nğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ° artifacts/gradient_as_slope.png")
    plt.close()

    print("\nğŸ’¡ å…³é”®ç†è§£:")
    print("  1. æ¢¯åº¦ = å‡½æ•°åœ¨æŸç‚¹çš„æ–œç‡")
    print("  2. æ­£æ¢¯åº¦ â†’ å‡½æ•°åœ¨å¢é•¿ â†’ éœ€è¦å‡å°w")
    print("  3. è´Ÿæ¢¯åº¦ â†’ å‡½æ•°åœ¨ä¸‹é™ â†’ éœ€è¦å¢å¤§w")
    print("  4. æ¢¯åº¦ä¸º0 â†’ åˆ°è¾¾æå€¼ç‚¹(æœ€å°å€¼æˆ–æœ€å¤§å€¼)")
    print()


def demo_simple_network_backprop():
    """æ¼”ç¤ºç®€å•ç½‘ç»œçš„åå‘ä¼ æ’­è¿‡ç¨‹"""
    print("\n" + "="*70)
    print("Part 2: ç®€å•ç½‘ç»œçš„åå‘ä¼ æ’­è¯¦è§£")
    print("="*70 + "\n")

    print("ç½‘ç»œç»“æ„: è¾“å…¥(2) â†’ Linear(2,1) â†’ è¾“å‡º(1)")
    print()

    # åˆ›å»ºä¸€ä¸ªè¶…ç®€å•çš„ç½‘ç»œ
    torch.manual_seed(42)
    model = nn.Linear(2, 1, bias=True)

    print("åˆå§‹å‚æ•°:")
    print(f"  æƒé‡ w: {model.weight.data}")
    print(f"  åç½® b: {model.bias.data}")
    print()

    # è¾“å…¥å’Œæ ‡ç­¾
    x = torch.tensor([[1.0, 2.0]])  # è¾“å…¥
    y_true = torch.tensor([[5.0]])   # çœŸå®å€¼

    print("æ•°æ®:")
    print(f"  è¾“å…¥ x: {x}")
    print(f"  çœŸå®å€¼ y_true: {y_true.item()}")
    print()

    # === å‰å‘ä¼ æ’­ ===
    print("="*70)
    print("ã€å‰å‘ä¼ æ’­ã€‘ä»è¾“å…¥åˆ°æŸå¤±")
    print("="*70)

    y_pred = model(x)
    print(f"\næ­¥éª¤1: è®¡ç®—é¢„æµ‹å€¼")
    print(f"  y_pred = wÂ·x + b")
    print(f"         = {model.weight.data} Â· {x[0]} + {model.bias.data.item():.4f}")
    w1, w2 = model.weight.data[0, 0].item(), model.weight.data[0, 1].item()
    x1, x2 = x[0, 0].item(), x[0, 1].item()
    b = model.bias.data.item()
    print(f"         = ({w1:.4f}Ã—{x1:.1f} + {w2:.4f}Ã—{x2:.1f}) + {b:.4f}")
    print(f"         = {y_pred.item():.4f}")

    loss = F.mse_loss(y_pred, y_true)
    print(f"\næ­¥éª¤2: è®¡ç®—æŸå¤±(å‡æ–¹è¯¯å·®)")
    print(f"  loss = (y_pred - y_true)Â²")
    print(f"       = ({y_pred.item():.4f} - {y_true.item():.1f})Â²")
    print(f"       = {loss.item():.4f}")

    # === åå‘ä¼ æ’­ ===
    print("\n" + "="*70)
    print("ã€åå‘ä¼ æ’­ã€‘ä»æŸå¤±åˆ°å‚æ•°æ¢¯åº¦")
    print("="*70)

    print("\næ‰§è¡Œ: loss.backward()")
    loss.backward()

    print("\nè‡ªåŠ¨è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦:")
    print(f"  dL/dwâ‚ = {model.weight.grad[0, 0].item():.4f}")
    print(f"  dL/dwâ‚‚ = {model.weight.grad[0, 1].item():.4f}")
    print(f"  dL/db  = {model.bias.grad.item():.4f}")

    # === æ‰‹åŠ¨éªŒè¯æ¢¯åº¦ ===
    print("\n" + "="*70)
    print("ã€æ‰‹åŠ¨éªŒè¯ã€‘é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦")
    print("="*70)

    print("\né“¾å¼æ³•åˆ™:")
    print("  dL/dw = dL/dy_pred Ã— dy_pred/dw")
    print()

    # è®¡ç®—ä¸­é—´æ¢¯åº¦
    error = y_pred.item() - y_true.item()
    dL_dy = 2 * error  # MSEçš„å¯¼æ•°

    print(f"æ­¥éª¤1: è®¡ç®— dL/dy_pred")
    print(f"  dL/dy_pred = d/dy_pred[(y_pred - y_true)Â²]")
    print(f"             = 2(y_pred - y_true)")
    print(f"             = 2 Ã— ({y_pred.item():.4f} - {y_true.item():.1f})")
    print(f"             = {dL_dy:.4f}")

    print(f"\næ­¥éª¤2: è®¡ç®— dy_pred/dw")
    print(f"  å› ä¸º y_pred = wâ‚Ã—xâ‚ + wâ‚‚Ã—xâ‚‚ + b")
    print(f"  æ‰€ä»¥:")
    print(f"    dy_pred/dwâ‚ = xâ‚ = {x1:.1f}")
    print(f"    dy_pred/dwâ‚‚ = xâ‚‚ = {x2:.1f}")
    print(f"    dy_pred/db  = 1")

    print(f"\næ­¥éª¤3: åº”ç”¨é“¾å¼æ³•åˆ™")
    manual_grad_w1 = dL_dy * x1
    manual_grad_w2 = dL_dy * x2
    manual_grad_b = dL_dy * 1

    print(f"  dL/dwâ‚ = dL/dy_pred Ã— dy_pred/dwâ‚")
    print(f"         = {dL_dy:.4f} Ã— {x1:.1f}")
    print(f"         = {manual_grad_w1:.4f}")
    print(f"  PyTorchè®¡ç®—: {model.weight.grad[0, 0].item():.4f} âœ…")
    print()
    print(f"  dL/dwâ‚‚ = {dL_dy:.4f} Ã— {x2:.1f} = {manual_grad_w2:.4f}")
    print(f"  PyTorchè®¡ç®—: {model.weight.grad[0, 1].item():.4f} âœ…")
    print()
    print(f"  dL/db  = {dL_dy:.4f} Ã— 1 = {manual_grad_b:.4f}")
    print(f"  PyTorchè®¡ç®—: {model.bias.grad.item():.4f} âœ…")

    # === å‚æ•°æ›´æ–° ===
    print("\n" + "="*70)
    print("ã€å‚æ•°æ›´æ–°ã€‘æ¢¯åº¦ä¸‹é™")
    print("="*70)

    lr = 0.01
    print(f"\nå­¦ä¹ ç‡ lr = {lr}")
    print("\næ›´æ–°å…¬å¼: å‚æ•°_new = å‚æ•°_old - lr Ã— æ¢¯åº¦")
    print()

    w1_old = model.weight.data[0, 0].item()
    w1_new = w1_old - lr * model.weight.grad[0, 0].item()
    print(f"wâ‚: {w1_old:.4f} - {lr} Ã— {model.weight.grad[0, 0].item():.4f}")
    print(f"  = {w1_new:.4f}")

    w2_old = model.weight.data[0, 1].item()
    w2_new = w2_old - lr * model.weight.grad[0, 1].item()
    print(f"\nwâ‚‚: {w2_old:.4f} - {lr} Ã— {model.weight.grad[0, 1].item():.4f}")
    print(f"  = {w2_new:.4f}")

    print("\nğŸ’¡ ä¸ºä»€ä¹ˆå‡å»æ¢¯åº¦?")
    print(f"  æ¢¯åº¦ = {model.weight.grad[0, 0].item():.4f} > 0")
    print(f"  è¯´æ˜:å¢å¤§wä¼šå¢å¤§loss")
    print(f"  å› æ­¤:å‡å°wæ‰èƒ½å‡å°loss")
    print(f"  å³:w_new = w_old - lrÃ—grad(å¾€æ–œç‡åæ–¹å‘ç§»åŠ¨)")


def demo_cifar_backprop():
    """æ¼”ç¤ºCIFARæ¨¡å‹çš„åå‘ä¼ æ’­"""
    print("\n\n" + "="*70)
    print("Part 3: CIFARæ¨¡å‹çš„åå‘ä¼ æ’­")
    print("="*70 + "\n")

    # ç®€åŒ–çš„CIFARæ¨¡å‹
    class SimpleCIFAR(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 6, 5)
            self.fc1 = nn.Linear(6*28*28, 10)

        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = x.view(x.size(0), -1)
            x = self.fc1(x)
            return x

    model = SimpleCIFAR()
    criterion = nn.CrossEntropyLoss()

    # æ¨¡æ‹Ÿæ•°æ®
    images = torch.randn(2, 3, 32, 32)
    labels = torch.tensor([3, 7])

    print("æ¨¡å‹ç»“æ„:")
    print("  Conv2d(3â†’6) + ReLU")
    print("  Flatten")
    print("  Linear(4704â†’10)")
    print()

    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print()

    for name, param in model.named_parameters():
        print(f"  {name:20s}: {str(param.shape):20s} "
              f"({param.numel():>6,} ä¸ªå‚æ•°)")
    print()

    # å‰å‘ä¼ æ’­
    print("="*70)
    print("æ‰§è¡Œå‰å‘ä¼ æ’­...")
    print("="*70)
    logits = model(images)
    print(f"è¾“å‡º logits: {logits.shape}")
    print(f"æ ·æœ¬1çš„logits: {logits[0].detach().numpy()}")
    print()

    # è®¡ç®—æŸå¤±
    loss = criterion(logits, labels)
    print(f"äº¤å‰ç†µæŸå¤±: {loss.item():.4f}")
    print()

    # åå‘ä¼ æ’­
    print("="*70)
    print("æ‰§è¡Œåå‘ä¼ æ’­: loss.backward()")
    print("="*70)

    print("\nè¿™ä¸€æ­¥PyTorchè‡ªåŠ¨å®Œæˆäº†:")
    print("  1. ä»æŸå¤±å¼€å§‹,é€å±‚è®¡ç®—æ¢¯åº¦")
    print("  2. ä½¿ç”¨é“¾å¼æ³•åˆ™ä¼ æ’­æ¢¯åº¦")
    print("  3. ä¸ºæ¯ä¸ªå‚æ•°è®¡ç®— dL/d(å‚æ•°)")
    print()

    loss.backward()

    print("åå‘ä¼ æ’­å®Œæˆ!æŸ¥çœ‹å„å±‚æ¢¯åº¦:")
    print()

    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_mean = param.grad.abs().mean().item()
            grad_max = param.grad.abs().max().item()
            print(f"  {name:20s}:")
            print(f"    æ¢¯åº¦å½¢çŠ¶: {param.grad.shape}")
            print(f"    æ¢¯åº¦èŒƒå›´: [{param.grad.min().item():.6f}, "
                  f"{param.grad.max().item():.6f}]")
            print(f"    æ¢¯åº¦å¹³å‡: {grad_mean:.6f}")
            print()

    print("ğŸ’¡ å…³é”®ç†è§£:")
    print("  1. æ¯ä¸ªå‚æ•°éƒ½æœ‰ä¸€ä¸ªæ¢¯åº¦ (ä¸å‚æ•°å½¢çŠ¶ç›¸åŒ)")
    print("  2. æ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬:'è°ƒæ•´è¿™ä¸ªå‚æ•°å¤šå°‘,lossä¼šå‡å°å¤šå°‘'")
    print("  3. æ¢¯åº¦å¤§ â†’ è¿™ä¸ªå‚æ•°å¯¹losså½±å“å¤§ â†’ éœ€è¦å¤§å¹…è°ƒæ•´")
    print("  4. æ¢¯åº¦å° â†’ è¿™ä¸ªå‚æ•°å¯¹losså½±å“å° â†’ å¾®è°ƒå³å¯")


def visualize_gradient_descent():
    """å¯è§†åŒ–æ¢¯åº¦ä¸‹é™è¿‡ç¨‹"""
    print("\n\n" + "="*70)
    print("Part 4: å¯è§†åŒ–æ¢¯åº¦ä¸‹é™")
    print("="*70 + "\n")

    if not HAS_MATPLOTLIB:
        print("(è·³è¿‡å¯è§†åŒ–éƒ¨åˆ†,éœ€è¦å®‰è£…matplotlib)\n")
        return

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ä¼˜åŒ–é—®é¢˜
    def loss_function(w1, w2):
        """ä¸€ä¸ªäºŒå…ƒå‡½æ•°ä½œä¸ºæŸå¤±"""
        return (w1 - 2)**2 + (w2 - 3)**2 + 1

    # åˆå§‹åŒ–å‚æ•°
    w1 = torch.tensor([0.0], requires_grad=True)
    w2 = torch.tensor([0.0], requires_grad=True)

    lr = 0.2
    n_steps = 20

    # è®°å½•è½¨è¿¹
    trajectory_w1 = [w1.item()]
    trajectory_w2 = [w2.item()]
    trajectory_loss = [loss_function(w1.item(), w2.item())]

    print(f"åˆå§‹ç‚¹: w1={w1.item():.2f}, w2={w2.item():.2f}")
    print(f"åˆå§‹æŸå¤±: {trajectory_loss[0]:.4f}")
    print()

    # æ¢¯åº¦ä¸‹é™
    for step in range(n_steps):
        # è®¡ç®—æŸå¤±
        loss = loss_function(w1, w2)

        # åå‘ä¼ æ’­
        if w1.grad is not None:
            w1.grad.zero_()
            w2.grad.zero_()
        loss.backward()

        # æ›´æ–°å‚æ•°
        with torch.no_grad():
            w1 -= lr * w1.grad
            w2 -= lr * w2.grad

        # è®°å½•
        trajectory_w1.append(w1.item())
        trajectory_w2.append(w2.item())
        trajectory_loss.append(loss.item())

        if step % 5 == 0:
            print(f"æ­¥éª¤ {step:2d}: w1={w1.item():6.3f}, w2={w2.item():6.3f}, "
                  f"loss={loss.item():8.4f}")

    print(f"\næœ€ç»ˆç‚¹: w1={w1.item():.2f}, w2={w2.item():.2f}")
    print(f"æœ€ç»ˆæŸå¤±: {trajectory_loss[-1]:.4f}")
    print(f"æœ€ä¼˜ç‚¹: w1=2.00, w2=3.00 (ç†è®ºå€¼)")
    print()

    # ç»˜å›¾
    fig = plt.figure(figsize=(15, 5))

    # å·¦å›¾:ç­‰é«˜çº¿å›¾ + æ¢¯åº¦ä¸‹é™è½¨è¿¹
    ax1 = plt.subplot(1, 3, 1)
    w1_range = np.linspace(-1, 4, 100)
    w2_range = np.linspace(-1, 5, 100)
    W1, W2 = np.meshgrid(w1_range, w2_range)
    Z = (W1 - 2)**2 + (W2 - 3)**2 + 1

    contour = ax1.contour(W1, W2, Z, levels=20, cmap='viridis', alpha=0.6)
    ax1.clabel(contour, inline=True, fontsize=8)
    ax1.plot(trajectory_w1, trajectory_w2, 'r.-', linewidth=2,
             markersize=8, label='æ¢¯åº¦ä¸‹é™è½¨è¿¹')
    ax1.plot(2, 3, 'g*', markersize=20, label='å…¨å±€æœ€ä¼˜ç‚¹')
    ax1.set_xlabel('wâ‚')
    ax1.set_ylabel('wâ‚‚')
    ax1.set_title('æ¢¯åº¦ä¸‹é™è½¨è¿¹')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # ä¸­å›¾:æŸå¤±éšè¿­ä»£æ¬¡æ•°çš„å˜åŒ–
    ax2 = plt.subplot(1, 3, 2)
    ax2.plot(trajectory_loss, 'b.-', linewidth=2, markersize=8)
    ax2.set_xlabel('è¿­ä»£æ¬¡æ•°')
    ax2.set_ylabel('æŸå¤±å€¼')
    ax2.set_title('æŸå¤±ä¸‹é™æ›²çº¿')
    ax2.grid(True, alpha=0.3)

    # å³å›¾:å‚æ•°æ”¶æ•›è¿‡ç¨‹
    ax3 = plt.subplot(1, 3, 3)
    ax3.plot(trajectory_w1, 'r.-', label='wâ‚', linewidth=2, markersize=6)
    ax3.plot(trajectory_w2, 'b.-', label='wâ‚‚', linewidth=2, markersize=6)
    ax3.axhline(y=2, color='r', linestyle='--', alpha=0.5, label='wâ‚æœ€ä¼˜å€¼=2')
    ax3.axhline(y=3, color='b', linestyle='--', alpha=0.5, label='wâ‚‚æœ€ä¼˜å€¼=3')
    ax3.set_xlabel('è¿­ä»£æ¬¡æ•°')
    ax3.set_ylabel('å‚æ•°å€¼')
    ax3.set_title('å‚æ•°æ”¶æ•›è¿‡ç¨‹')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('artifacts/gradient_descent.png', dpi=100, bbox_inches='tight')
    print("ğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ° artifacts/gradient_descent.png")
    plt.close()


if __name__ == "__main__":
    # åˆ›å»ºè¾“å‡ºç›®å½•
    import os
    os.makedirs('artifacts', exist_ok=True)

    print("\n\n")
    print("â–ˆ" * 70)
    print(" " * 18 + "åå‘ä¼ æ’­ä¸æ¢¯åº¦å®Œå…¨è§£æ")
    print("â–ˆ" * 70)

    # Part 1: æ¢¯åº¦æ˜¯æ–œç‡
    demo_gradient_as_slope()

    # Part 2: ç®€å•ç½‘ç»œåå‘ä¼ æ’­
    demo_simple_network_backprop()

    # Part 3: CIFARæ¨¡å‹åå‘ä¼ æ’­
    demo_cifar_backprop()

    # Part 4: å¯è§†åŒ–æ¢¯åº¦ä¸‹é™
    visualize_gradient_descent()

    print("\n" + "="*70)
    print("æ€»ç»“:åå‘ä¼ æ’­çš„å®Œæ•´ç†è§£")
    print("="*70)
    print("""
1. æ¢¯åº¦ = æ–œç‡
   - å‘Šè¯‰æˆ‘ä»¬å‡½æ•°åœ¨æŸç‚¹çš„å˜åŒ–ç‡
   - æ­£æ¢¯åº¦ â†’ å‚æ•°å¢å¤§ä¼šå¢å¤§loss â†’ éœ€è¦å‡å°å‚æ•°
   - è´Ÿæ¢¯åº¦ â†’ å‚æ•°å¢å¤§ä¼šå‡å°loss â†’ éœ€è¦å¢å¤§å‚æ•°

2. åå‘ä¼ æ’­ = è‡ªåŠ¨å¾®åˆ†
   - PyTorchè‡ªåŠ¨è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
   - ä½¿ç”¨é“¾å¼æ³•åˆ™ä»è¾“å‡ºå±‚ä¼ æ’­åˆ°è¾“å…¥å±‚
   - loss.backward() ä¸€è¡Œä»£ç å®Œæˆæ‰€æœ‰æ¢¯åº¦è®¡ç®—

3. ä¸ºä»€ä¹ˆå«"åå‘"?
   - å‰å‘:è¾“å…¥ â†’ å±‚1 â†’ å±‚2 â†’ ... â†’ è¾“å‡º â†’ loss
   - åå‘:loss â†’ ... â†’ å±‚2çš„æ¢¯åº¦ â†’ å±‚1çš„æ¢¯åº¦ â†’ è¾“å…¥çš„æ¢¯åº¦
   - æ¢¯åº¦ä»losså¼€å§‹,æ²¿ç€ç½‘ç»œåå‘ä¼ æ’­

4. å‚æ•°æ›´æ–°
   - å‚æ•°_new = å‚æ•°_old - å­¦ä¹ ç‡ Ã— æ¢¯åº¦
   - å­¦ä¹ ç‡æ§åˆ¶æ­¥é•¿å¤§å°
   - é‡å¤"å‰å‘â†’æŸå¤±â†’åå‘â†’æ›´æ–°"ç›´åˆ°æ”¶æ•›

5. æ ¸å¿ƒå…¬å¼
   - é“¾å¼æ³•åˆ™: dL/dw = dL/dy Ã— dy/dw
   - æ¢¯åº¦ä¸‹é™: w := w - Î±Â·(dL/dw)
   - å…¶ä¸­ Î± æ˜¯å­¦ä¹ ç‡
""")
    print("="*70 + "\n")
