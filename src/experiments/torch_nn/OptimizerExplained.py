"""
æ·±å…¥ç†è§£ä¼˜åŒ–å™¨(Optimizer)
========================
æ ¸å¿ƒé—®é¢˜:
1. ä¼˜åŒ–å™¨æ˜¯ä»€ä¹ˆ?æœ‰ä»€ä¹ˆç”¨?
2. å„ç§ä¼˜åŒ–å™¨(SGD/Adam/RMSpropç­‰)æœ‰ä»€ä¹ˆåŒºåˆ«?
3. å‚æ•°(lr/momentum/weight_decayç­‰)æ˜¯ä»€ä¹ˆæ„æ€?
4. å¦‚ä½•é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–å™¨?
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np


def explain_optimizer_basics():
    """è§£é‡Šä¼˜åŒ–å™¨çš„åŸºç¡€æ¦‚å¿µ"""
    print("="*70)
    print("Part 1: ä¼˜åŒ–å™¨æ˜¯ä»€ä¹ˆ?")
    print("="*70 + "\n")

    print("ğŸ’¡ ä¼˜åŒ–å™¨çš„æ ¸å¿ƒä½œç”¨:")
    print("  è¾“å…¥: å‚æ•°çš„æ¢¯åº¦ (param.grad)")
    print("  è¾“å‡º: å‚æ•°çš„æ›´æ–°é‡ (å¦‚ä½•è°ƒæ•´å‚æ•°)")
    print("  ç›®æ ‡: è®© loss å°½å¿«ä¸‹é™åˆ°æœ€å°å€¼")
    print()

    print("åŸºç¡€å…¬å¼:")
    print("  å‚æ•°æ›´æ–° = å‚æ•°_old - å­¦ä¹ ç‡ Ã— æ¢¯åº¦")
    print("  param_new = param_old - lr Ã— grad")
    print()

    # åˆ›å»ºç®€å•ç¤ºä¾‹
    print("="*70)
    print("ç¤ºä¾‹:æ‰‹åŠ¨å®ç°ä¼˜åŒ–å™¨")
    print("="*70 + "\n")

    # ä¸€ä¸ªç®€å•çš„å‚æ•°
    param = torch.tensor([2.0], requires_grad=True)

    print(f"åˆå§‹å‚æ•°: {param.item():.4f}")

    # æ¨¡æ‹Ÿæ¢¯åº¦
    loss = param ** 2
    loss.backward()

    print(f"è®¡ç®—å¾—åˆ°æ¢¯åº¦: {param.grad.item():.4f}")

    # æ‰‹åŠ¨æ›´æ–°
    lr = 0.1
    with torch.no_grad():  # æ›´æ–°æ—¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦
        param -= lr * param.grad

    print(f"å­¦ä¹ ç‡: {lr}")
    print(f"æ›´æ–°åå‚æ•°: {param.item():.4f}")
    print()

    print("ğŸ’¡ ä¼˜åŒ–å™¨åšçš„å°±æ˜¯è¿™ä»¶äº‹!")
    print("  åªä¸è¿‡æ›´èªæ˜:ä¸æ˜¯ç®€å•åœ°å‡å»æ¢¯åº¦,è€Œæ˜¯ç”¨å„ç§æŠ€å·§åŠ é€Ÿæ”¶æ•›")


def compare_sgd_variants():
    """å¯¹æ¯”ä¸åŒçš„SGDå˜ä½“"""
    print("\n\n" + "="*70)
    print("Part 2: SGDå®¶æ—ä¼˜åŒ–å™¨è¯¦è§£")
    print("="*70 + "\n")

    # åˆ›å»ºä¸€ä¸ªæµ‹è¯•é—®é¢˜
    def rosenbrock(x, y):
        """Rosenbrockå‡½æ•°:ç»å…¸çš„ä¼˜åŒ–æµ‹è¯•å‡½æ•°"""
        return (1 - x)**2 + 100 * (y - x**2)**2

    # åˆå§‹ç‚¹
    start_x, start_y = -1.0, -1.0

    print("æµ‹è¯•å‡½æ•°: Rosenbrockå‡½æ•°")
    print(f"èµ·å§‹ç‚¹: ({start_x}, {start_y})")
    print(f"ç›®æ ‡æœ€å°å€¼ç‚¹: (1, 1)")
    print()

    # ========== 1. åŸºç¡€SGD ==========
    print("="*70)
    print("1. åŸºç¡€SGD (Stochastic Gradient Descent)")
    print("="*70)
    print()
    print("åŸç†:")
    print("  param_new = param_old - lr Ã— grad")
    print()
    print("ç‰¹ç‚¹:")
    print("  - æœ€ç®€å•çš„ä¼˜åŒ–å™¨")
    print("  - ä¸¥æ ¼æŒ‰ç…§æ¢¯åº¦æ–¹å‘æ›´æ–°")
    print("  - å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜")
    print("  - éœ‡è¡è¾ƒå¤§")
    print()

    # æ¨¡æ‹ŸSGD
    x1 = torch.tensor([start_x], requires_grad=True)
    y1 = torch.tensor([start_y], requires_grad=True)
    optimizer1 = optim.SGD([x1, y1], lr=0.001)

    print("ä»£ç :")
    print("  optimizer = optim.SGD(model.parameters(), lr=0.001)")
    print()

    for step in range(5):
        optimizer1.zero_grad()
        loss = rosenbrock(x1, y1)
        loss.backward()
        optimizer1.step()

        if step == 0 or step == 4:
            print(f"  æ­¥éª¤{step}: ä½ç½®=({x1.item():.4f}, {y1.item():.4f}), "
                  f"loss={loss.item():.4f}")

    print()
    print("ğŸ“Š è§‚å¯Ÿ:")
    print("  - æ¯æ­¥ä¸¥æ ¼æŒ‰æ¢¯åº¦æ–¹å‘ç§»åŠ¨")
    print("  - æ²¡æœ‰'è®°å¿†',æ¯æ­¥ç‹¬ç«‹å†³ç­–")
    print()

    # ========== 2. SGD + Momentum ==========
    print("="*70)
    print("2. SGD + Momentum (åŠ¨é‡)")
    print("="*70)
    print()
    print("åŸç†:")
    print("  velocity = momentum Ã— velocity_old + grad")
    print("  param_new = param_old - lr Ã— velocity")
    print()
    print("ğŸ’¡ ç±»æ¯”:")
    print("  æƒ³è±¡ä¸€ä¸ªçƒä»å±±å¡æ»šä¸‹æ¥:")
    print("  - ä¸ä»…å—å½“å‰å¡åº¦å½±å“(æ¢¯åº¦)")
    print("  - è¿˜ä¿æŒä¹‹å‰çš„é€Ÿåº¦(åŠ¨é‡)")
    print("  - èƒ½å†²è¿‡å°å±±ä¸˜(é¿å…å±€éƒ¨æœ€ä¼˜)")
    print()

    x2 = torch.tensor([start_x], requires_grad=True)
    y2 = torch.tensor([start_y], requires_grad=True)
    optimizer2 = optim.SGD([x2, y2], lr=0.001, momentum=0.9)

    print("ä»£ç :")
    print("  optimizer = optim.SGD(model.parameters(),")
    print("                        lr=0.001,")
    print("                        momentum=0.9)  # ä¿æŒ90%çš„å†å²é€Ÿåº¦")
    print()

    for step in range(5):
        optimizer2.zero_grad()
        loss = rosenbrock(x2, y2)
        loss.backward()
        optimizer2.step()

        if step == 0 or step == 4:
            print(f"  æ­¥éª¤{step}: ä½ç½®=({x2.item():.4f}, {y2.item():.4f}), "
                  f"loss={loss.item():.4f}")

    print()
    print("å‚æ•°è§£é‡Š:")
    print("  momentum âˆˆ [0, 1]:")
    print("    - 0.0: é€€åŒ–ä¸ºåŸºç¡€SGD,æ— åŠ¨é‡")
    print("    - 0.9: ä¿æŒ90%çš„å†å²é€Ÿåº¦(å¸¸ç”¨å€¼)")
    print("    - 0.99: ä¿æŒ99%çš„å†å²é€Ÿåº¦(ç”¨äºå¤§batch)")
    print()
    print("ä¼˜ç‚¹:")
    print("  âœ“ åŠ é€Ÿæ”¶æ•›(åˆ©ç”¨å†å²ä¿¡æ¯)")
    print("  âœ“ å‡å°‘éœ‡è¡(å¹³æ»‘æ¢¯åº¦)")
    print("  âœ“ æ›´å®¹æ˜“è·³å‡ºå±€éƒ¨æœ€ä¼˜")
    print()

    # ========== 3. SGD + Nesterov Momentum ==========
    print("="*70)
    print("3. SGD + Nesterov Momentum (NesterovåŠ é€Ÿæ¢¯åº¦)")
    print("="*70)
    print()
    print("åŸç†:")
    print("  1. å…ˆæŒ‰åŠ¨é‡ç§»åŠ¨åˆ°'é¢„æµ‹ä½ç½®'")
    print("  2. åœ¨é¢„æµ‹ä½ç½®è®¡ç®—æ¢¯åº¦")
    print("  3. æ ¹æ®é¢„æµ‹ä½ç½®çš„æ¢¯åº¦ä¿®æ­£æ–¹å‘")
    print()
    print("ğŸ’¡ ç±»æ¯”:")
    print("  æ™®é€šåŠ¨é‡: çœ‹å½“å‰ä½ç½®çš„è·¯æ ‡")
    print("  Nesterov: å…ˆå¾€å‰çœ‹ä¸€æ­¥,çœ‹å‰é¢çš„è·¯æ ‡(æ›´èªæ˜!)")
    print()

    x3 = torch.tensor([start_x], requires_grad=True)
    y3 = torch.tensor([start_y], requires_grad=True)
    optimizer3 = optim.SGD([x3, y3], lr=0.001, momentum=0.9, nesterov=True)

    print("ä»£ç :")
    print("  optimizer = optim.SGD(model.parameters(),")
    print("                        lr=0.001,")
    print("                        momentum=0.9,")
    print("                        nesterov=True)  # å¯ç”¨Nesterov")
    print()

    # ========== 4. Weight Decay ==========
    print("="*70)
    print("4. Weight Decay (æƒé‡è¡°å‡ = L2æ­£åˆ™åŒ–)")
    print("="*70)
    print()
    print("åŸç†:")
    print("  grad_new = grad + weight_decay Ã— param")
    print("  param_new = param_old - lr Ã— grad_new")
    print()
    print("ğŸ’¡ ä½œç”¨:")
    print("  - é˜²æ­¢æƒé‡è¿‡å¤§")
    print("  - ç›¸å½“äºç»™æŸå¤±å‡½æ•°åŠ ä¸Š Î»||w||Â²")
    print("  - é˜²æ­¢è¿‡æ‹Ÿåˆ")
    print()

    print("ä»£ç :")
    print("  optimizer = optim.SGD(model.parameters(),")
    print("                        lr=0.001,")
    print("                        weight_decay=1e-4)  # L2æ­£åˆ™åŒ–ç³»æ•°")
    print()

    print("å‚æ•°è§£é‡Š:")
    print("  weight_decay âˆˆ [0, âˆ):")
    print("    - 0: æ— æ­£åˆ™åŒ–")
    print("    - 1e-5 ~ 1e-3: å¸¸ç”¨èŒƒå›´")
    print("    - è¿‡å¤§: æƒé‡è¢«å‹åˆ¶å¾—å¤ªå°,æ¬ æ‹Ÿåˆ")
    print()


def compare_adaptive_optimizers():
    """å¯¹æ¯”è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–å™¨"""
    print("\n\n" + "="*70)
    print("Part 3: è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–å™¨")
    print("="*70 + "\n")

    print("ğŸ’¡ æ ¸å¿ƒæ€æƒ³:")
    print("  ä¸åŒå‚æ•°åº”è¯¥ç”¨ä¸åŒçš„å­¦ä¹ ç‡!")
    print("  - æ¢¯åº¦å¤§çš„å‚æ•° â†’ ç”¨å°å­¦ä¹ ç‡(é˜²æ­¢éœ‡è¡)")
    print("  - æ¢¯åº¦å°çš„å‚æ•° â†’ ç”¨å¤§å­¦ä¹ ç‡(åŠ é€Ÿæ”¶æ•›)")
    print()

    # ========== 1. Adagrad ==========
    print("="*70)
    print("1. Adagrad (Adaptive Gradient)")
    print("="*70)
    print()
    print("åŸç†:")
    print("  sum_squared_grad += gradÂ²")
    print("  adjusted_lr = lr / sqrt(sum_squared_grad + Îµ)")
    print("  param_new = param_old - adjusted_lr Ã— grad")
    print()
    print("ğŸ’¡ ç‰¹ç‚¹:")
    print("  - ç´¯ç§¯å†å²æ¢¯åº¦çš„å¹³æ–¹")
    print("  - å­¦ä¹ ç‡ä¼šä¸æ–­å‡å°")
    print("  - é€‚åˆç¨€ç–æ¢¯åº¦(å¦‚NLP)")
    print()

    print("ä»£ç :")
    print("  optimizer = optim.Adagrad(model.parameters(), lr=0.01)")
    print()

    print("ä¼˜ç‚¹:")
    print("  âœ“ è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡")
    print("  âœ“ é€‚åˆå¤„ç†ç¨€ç–æ•°æ®")
    print()
    print("ç¼ºç‚¹:")
    print("  âœ— å­¦ä¹ ç‡å•è°ƒé€’å‡")
    print("  âœ— è®­ç»ƒåæœŸå¯èƒ½å¤ªæ…¢")
    print()

    # ========== 2. RMSprop ==========
    print("="*70)
    print("2. RMSprop (Root Mean Square Propagation)")
    print("="*70)
    print()
    print("åŸç†:")
    print("  squared_avg = Î± Ã— squared_avg + (1-Î±) Ã— gradÂ²")
    print("  adjusted_lr = lr / sqrt(squared_avg + Îµ)")
    print("  param_new = param_old - adjusted_lr Ã— grad")
    print()
    print("ğŸ’¡ æ”¹è¿›:")
    print("  - ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ä»£æ›¿ç´¯ç§¯å’Œ")
    print("  - å­¦ä¹ ç‡ä¸ä¼šæ— é™å‡å°")
    print("  - é€‚åˆRNNè®­ç»ƒ")
    print()

    print("ä»£ç :")
    print("  optimizer = optim.RMSprop(model.parameters(),")
    print("                            lr=0.01,")
    print("                            alpha=0.99)  # ç§»åŠ¨å¹³å‡ç³»æ•°")
    print()

    print("å‚æ•°è§£é‡Š:")
    print("  alpha âˆˆ [0, 1]:")
    print("    - 0.99: ä¿ç•™99%çš„å†å²ä¿¡æ¯(å¸¸ç”¨)")
    print("    - 0.9:  æ›´å¿«é€‚åº”æ–°æ¢¯åº¦")
    print()

    # ========== 3. Adam ==========
    print("="*70)
    print("3. Adam (Adaptive Moment Estimation) â­æœ€å¸¸ç”¨â­")
    print("="*70)
    print()
    print("åŸç†:ç»“åˆMomentumå’ŒRMSprop")
    print("  m = Î²â‚ Ã— m + (1-Î²â‚) Ã— grad           # ä¸€é˜¶çŸ©(åŠ¨é‡)")
    print("  v = Î²â‚‚ Ã— v + (1-Î²â‚‚) Ã— gradÂ²          # äºŒé˜¶çŸ©(æ–¹å·®)")
    print("  m_hat = m / (1 - Î²â‚^t)                # åå·®ä¿®æ­£")
    print("  v_hat = v / (1 - Î²â‚‚^t)")
    print("  param_new = param_old - lr Ã— m_hat / (sqrt(v_hat) + Îµ)")
    print()
    print("ğŸ’¡ é›†å¤§æˆè€…:")
    print("  - æœ‰åŠ¨é‡(åˆ©ç”¨å†å²æ¢¯åº¦æ–¹å‘)")
    print("  - æœ‰è‡ªé€‚åº”å­¦ä¹ ç‡(æ ¹æ®æ¢¯åº¦å¤§å°è°ƒæ•´)")
    print("  - æœ‰åå·®ä¿®æ­£(è®­ç»ƒåˆæœŸæ›´å‡†ç¡®)")
    print()

    print("ä»£ç :")
    print("  optimizer = optim.Adam(model.parameters(),")
    print("                         lr=0.001,")
    print("                         betas=(0.9, 0.999),  # (Î²â‚, Î²â‚‚)")
    print("                         eps=1e-8,")
    print("                         weight_decay=0)")
    print()

    print("å‚æ•°è§£é‡Š:")
    print("  lr: å­¦ä¹ ç‡")
    print("    - 0.001: é»˜è®¤å€¼,é€‚åˆå¤§å¤šæ•°æƒ…å†µ")
    print("    - 0.0001: å¾®è°ƒæ—¶ä½¿ç”¨")
    print()
    print("  betas = (Î²â‚, Î²â‚‚):")
    print("    - Î²â‚=0.9: ä¸€é˜¶çŸ©(åŠ¨é‡)è¡°å‡ç‡")
    print("    - Î²â‚‚=0.999: äºŒé˜¶çŸ©(æ–¹å·®)è¡°å‡ç‡")
    print("    - é€šå¸¸ä¸éœ€è¦æ”¹")
    print()
    print("  eps: æ•°å€¼ç¨³å®šæ€§")
    print("    - é˜²æ­¢é™¤é›¶")
    print("    - é»˜è®¤1e-8å³å¯")
    print()

    print("ä¼˜ç‚¹:")
    print("  âœ“ æ”¶æ•›å¿«")
    print("  âœ“ é²æ£’æ€§å¥½")
    print("  âœ“ è¶…å‚æ•°é»˜è®¤å€¼å°±å¾ˆå¥½ç”¨")
    print("  âœ“ é€‚ç”¨èŒƒå›´å¹¿")
    print()
    print("ç¼ºç‚¹:")
    print("  âœ— å¯èƒ½è¿‡æ‹Ÿåˆ")
    print("  âœ— æœ‰æ—¶æ³›åŒ–æ€§ä¸å¦‚SGD+Momentum")
    print()

    # ========== 4. AdamW ==========
    print("="*70)
    print("4. AdamW (Adam with Weight Decay) â­æ¨èâ­")
    print("="*70)
    print()
    print("åŸç†:")
    print("  - Adamçš„æ”¹è¿›ç‰ˆæœ¬")
    print("  - ä¿®æ­£äº†weight_decayçš„å®ç°æ–¹å¼")
    print("  - æ›´å¥½çš„æ³›åŒ–æ€§èƒ½")
    print()

    print("ä»£ç :")
    print("  optimizer = optim.AdamW(model.parameters(),")
    print("                          lr=0.001,")
    print("                          weight_decay=0.01)  # å¸¸ç”¨å€¼")
    print()

    print("ğŸ’¡ AdamW vs Adam:")
    print("  Adam:   grad = grad + weight_decay Ã— param  (é”™è¯¯çš„L2)")
    print("  AdamW:  param = param Ã— (1 - weight_decay)  (æ­£ç¡®çš„L2)")
    print()
    print("ğŸ“Š ç°ä»£æœ€ä½³å®è·µ:")
    print("  - å¤§å¤šæ•°æƒ…å†µä¼˜å…ˆé€‰æ‹©AdamW")
    print("  - weight_decay=0.01~0.1")
    print()


def demo_optimizer_comparison():
    """å®æˆ˜å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨"""
    print("\n\n" + "="*70)
    print("Part 4: å®æˆ˜å¯¹æ¯” - è®­ç»ƒç®€å•æ¨¡å‹")
    print("="*70 + "\n")

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„åˆ†ç±»ä»»åŠ¡
    torch.manual_seed(42)

    # æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(10, 50),
        nn.ReLU(),
        nn.Linear(50, 3)
    )

    # æ•°æ®
    X = torch.randn(100, 10)
    y = torch.randint(0, 3, (100,))

    criterion = nn.CrossEntropyLoss()

    # æµ‹è¯•ä¸åŒä¼˜åŒ–å™¨
    optimizers = {
        'SGD': optim.SGD(model.parameters(), lr=0.01),
        'SGD+Momentum': optim.SGD(model.parameters(), lr=0.01, momentum=0.9),
        'RMSprop': optim.RMSprop(model.parameters(), lr=0.01),
        'Adam': optim.Adam(model.parameters(), lr=0.01),
        'AdamW': optim.AdamW(model.parameters(), lr=0.01),
    }

    print("ä»»åŠ¡: 100ä¸ªæ ·æœ¬,10ç»´è¾“å…¥,3åˆ†ç±»")
    print("æ¨¡å‹: 10â†’50â†’3çš„å…¨è¿æ¥ç½‘ç»œ")
    print()

    for name, optimizer in optimizers.items():
        # é‡æ–°åˆå§‹åŒ–æ¨¡å‹
        for layer in model:
            if hasattr(layer, 'reset_parameters'):
                layer.reset_parameters()

        # è®­ç»ƒ10æ­¥
        losses = []
        for step in range(10):
            optimizer.zero_grad()
            outputs = model(X)
            loss = criterion(outputs, y)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())

        print(f"{name:15s}: åˆå§‹loss={losses[0]:.4f}, "
              f"æœ€ç»ˆloss={losses[-1]:.4f}, "
              f"ä¸‹é™={losses[0]-losses[-1]:.4f}")

    print()
    print("ğŸ“Š è§‚å¯Ÿ:")
    print("  - Adam/AdamWé€šå¸¸æ”¶æ•›æœ€å¿«")
    print("  - SGD+Momentumæ¯”çº¯SGDå¿«")
    print("  - ä½†SGD+Momentumé•¿æœŸè®­ç»ƒå¯èƒ½æ³›åŒ–æ›´å¥½")


def explain_optimizer_parameters():
    """è¯¦è§£ä¼˜åŒ–å™¨çš„å„ç§å‚æ•°"""
    print("\n\n" + "="*70)
    print("Part 5: ä¼˜åŒ–å™¨å‚æ•°å®Œå…¨æŒ‡å—")
    print("="*70 + "\n")

    print("ã€1. å­¦ä¹ ç‡ (lr / learning_rate)ã€‘")
    print("  æœ€é‡è¦çš„è¶…å‚æ•°!")
    print()
    print("  ä½œç”¨: æ§åˆ¶å‚æ•°æ›´æ–°çš„æ­¥é•¿")
    print("  param_new = param_old - lr Ã— grad")
    print()
    print("  é€‰æ‹©æŒ‡å—:")
    print("    - å¤ªå¤§: éœ‡è¡,ä¸æ”¶æ•›,lossçˆ†ç‚¸")
    print("    - å¤ªå°: æ”¶æ•›æ…¢,å®¹æ˜“å¡ä½")
    print("    - åˆé€‚: ç¨³å®šä¸‹é™")
    print()
    print("  å¸¸ç”¨èŒƒå›´:")
    print("    SGD:     0.01 ~ 0.1")
    print("    SGD+Momentum: 0.01 ~ 0.1")
    print("    Adam:    0.0001 ~ 0.001")
    print("    AdamW:   0.0001 ~ 0.001")
    print()
    print("  è°ƒå‚æŠ€å·§:")
    print("    1. ä»å°å¼€å§‹(å¦‚1e-4)")
    print("    2. è§‚å¯Ÿlossæ›²çº¿")
    print("    3. é€æ­¥å¢å¤§,ç›´åˆ°å‡ºç°éœ‡è¡")
    print("    4. é€‰æ‹©éœ‡è¡å‰çš„æœ€å¤§å€¼")
    print()

    print("ã€2. åŠ¨é‡ (momentum)ã€‘")
    print("  ç”¨äºSGD")
    print()
    print("  ä½œç”¨: ä¿ç•™å†å²æ¢¯åº¦ä¿¡æ¯,åŠ é€Ÿæ”¶æ•›")
    print()
    print("  å¸¸ç”¨å€¼:")
    print("    - 0.9: æ ‡å‡†é€‰æ‹©")
    print("    - 0.95~0.99: å¤§batch sizeæ—¶")
    print()

    print("ã€3. æƒé‡è¡°å‡ (weight_decay)ã€‘")
    print("  L2æ­£åˆ™åŒ–")
    print()
    print("  ä½œç”¨: é˜²æ­¢è¿‡æ‹Ÿåˆ,é™åˆ¶æƒé‡å¤§å°")
    print()
    print("  å¸¸ç”¨èŒƒå›´:")
    print("    - 0: æ— æ­£åˆ™åŒ–")
    print("    - 1e-5 ~ 1e-3: å°æ•°æ®é›†")
    print("    - 0.01 ~ 0.1: å¤§æ¨¡å‹(å¦‚Transformer)")
    print()
    print("  æ³¨æ„:")
    print("    - Adamé…åˆweight_decayæ•ˆæœä¸å¥½")
    print("    - æ¨èç”¨AdamW")
    print()

    print("ã€4. Betas (Î²â‚, Î²â‚‚)ã€‘")
    print("  ç”¨äºAdam/AdamW")
    print()
    print("  Î²â‚: ä¸€é˜¶çŸ©(åŠ¨é‡)è¡°å‡ç‡")
    print("  Î²â‚‚: äºŒé˜¶çŸ©(æ–¹å·®)è¡°å‡ç‡")
    print()
    print("  é»˜è®¤å€¼: (0.9, 0.999)")
    print("  é€šå¸¸ä¸éœ€è¦è°ƒæ•´!")
    print()
    print("  ç‰¹æ®Šæƒ…å†µ:")
    print("    - NLP/å¤§batch: Î²â‚=0.9, Î²â‚‚=0.98")
    print("    - å™ªå£°å¤§çš„ä»»åŠ¡: Î²â‚=0.5")
    print()

    print("ã€5. Epsilon (eps)ã€‘")
    print("  æ•°å€¼ç¨³å®šæ€§å‚æ•°")
    print()
    print("  ä½œç”¨: é˜²æ­¢é™¤é›¶é”™è¯¯")
    print("  adjusted_lr = lr / (sqrt(variance) + eps)")
    print()
    print("  é»˜è®¤å€¼: 1e-8")
    print("  é€šå¸¸ä¸éœ€è¦æ”¹!")
    print()


def provide_practical_guide():
    """æä¾›å®ç”¨é€‰æ‹©æŒ‡å—"""
    print("\n\n" + "="*70)
    print("Part 6: ä¼˜åŒ–å™¨é€‰æ‹©å®ç”¨æŒ‡å—")
    print("="*70 + "\n")

    print("ğŸ¯ å¿«é€Ÿå†³ç­–æ ‘:")
    print()
    print("1. ä½ åœ¨åšä»€ä¹ˆä»»åŠ¡?")
    print()
    print("   ã€è®¡ç®—æœºè§†è§‰ (CNN)ã€‘")
    print("     é¦–é€‰: SGD + Momentum")
    print("       optimizer = optim.SGD(model.parameters(),")
    print("                             lr=0.1,")
    print("                             momentum=0.9,")
    print("                             weight_decay=1e-4)")
    print("     åŸå› : æ³›åŒ–æ€§èƒ½æœ€å¥½,ä¸šç•ŒéªŒè¯")
    print()
    print("     å¤‡é€‰: AdamW (å¿«é€ŸåŸå‹)")
    print("       optimizer = optim.AdamW(model.parameters(),")
    print("                               lr=0.001,")
    print("                               weight_decay=0.01)")
    print()

    print("   ã€è‡ªç„¶è¯­è¨€å¤„ç† (Transformer)ã€‘")
    print("     é¦–é€‰: AdamW")
    print("       optimizer = optim.AdamW(model.parameters(),")
    print("                               lr=1e-4,")
    print("                               betas=(0.9, 0.98),")
    print("                               weight_decay=0.01)")
    print("     åŸå› : å¤„ç†ç¨€ç–æ¢¯åº¦å¥½,è®­ç»ƒç¨³å®š")
    print()

    print("   ã€å¼ºåŒ–å­¦ä¹ ã€‘")
    print("     é¦–é€‰: Adam æˆ– RMSprop")
    print("       optimizer = optim.Adam(model.parameters(), lr=1e-4)")
    print("     åŸå› : æ¢¯åº¦å™ªå£°å¤§,éœ€è¦è‡ªé€‚åº”å­¦ä¹ ç‡")
    print()

    print("   ã€GANã€‘")
    print("     é¦–é€‰: Adam")
    print("       optimizer_G = optim.Adam(generator.parameters(),")
    print("                                lr=0.0002, betas=(0.5, 0.999))")
    print("       optimizer_D = optim.Adam(discriminator.parameters(),")
    print("                                lr=0.0002, betas=(0.5, 0.999))")
    print("     æ³¨æ„: Î²â‚=0.5 (é™ä½åŠ¨é‡,å¢åŠ ç¨³å®šæ€§)")
    print()

    print("="*70)
    print("ğŸ“Š ä¼˜åŒ–å™¨å¯¹æ¯”æ€»ç»“è¡¨")
    print("="*70)
    print()
    print("ä¼˜åŒ–å™¨          æ”¶æ•›é€Ÿåº¦  æ³›åŒ–æ€§èƒ½  è¶…å‚æ•°æ•æ„Ÿåº¦  é€‚ç”¨åœºæ™¯")
    print("-" * 70)
    print("SGD             æ…¢        ä¼˜        é«˜           CVå¤§æ¨¡å‹")
    print("SGD+Momentum    ä¸­        ä¼˜        ä¸­           CVé€šç”¨,æ¨è")
    print("Adagrad         ä¸­        ä¸­        ä½           ç¨€ç–æ•°æ®")
    print("RMSprop         å¿«        ä¸­        ä½           RNN")
    print("Adam            å¿«        ä¸­        ä½           é€šç”¨åŸå‹")
    print("AdamW           å¿«        ä¼˜        ä½           NLP,å¤§æ¨¡å‹")
    print()

    print("="*70)
    print("ğŸ”§ è°ƒå‚å»ºè®®")
    print("="*70)
    print()
    print("1. å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥:")
    print("   - ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨(lr_scheduler)")
    print("   - å¸¸ç”¨: StepLR, CosineAnnealingLR, ReduceLROnPlateau")
    print()
    print("   ç¤ºä¾‹:")
    print("     optimizer = optim.SGD(model.parameters(), lr=0.1)")
    print("     scheduler = optim.lr_scheduler.StepLR(optimizer,")
    print("                                            step_size=30,")
    print("                                            gamma=0.1)")
    print("     # æ¯30ä¸ªepoch,å­¦ä¹ ç‡Ã—0.1")
    print()

    print("2. å­¦ä¹ ç‡é¢„çƒ­(Warmup):")
    print("   - è®­ç»ƒåˆæœŸç”¨å°å­¦ä¹ ç‡")
    print("   - é€æ­¥å¢å¤§åˆ°ç›®æ ‡å­¦ä¹ ç‡")
    print("   - å¯¹å¤§batch sizeå¾ˆé‡è¦")
    print()

    print("3. æ¢¯åº¦è£å‰ª(Gradient Clipping):")
    print("   - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸")
    print("   - ç‰¹åˆ«æ˜¯RNN/Transformer")
    print()
    print("   ä»£ç :")
    print("     loss.backward()")
    print("     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)")
    print("     optimizer.step()")
    print()


def demo_complete_training_loop():
    """å®Œæ•´è®­ç»ƒå¾ªç¯ç¤ºä¾‹"""
    print("\n\n" + "="*70)
    print("Part 7: å®Œæ•´è®­ç»ƒå¾ªç¯ç¤ºä¾‹")
    print("="*70 + "\n")

    code = '''
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR

# 1. åˆ›å»ºæ¨¡å‹
model = MyModel()

# 2. é€‰æ‹©ä¼˜åŒ–å™¨
optimizer = optim.AdamW(
    model.parameters(),
    lr=0.001,           # åˆå§‹å­¦ä¹ ç‡
    weight_decay=0.01   # L2æ­£åˆ™åŒ–
)

# 3. å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = StepLR(
    optimizer,
    step_size=10,  # æ¯10ä¸ªepoch
    gamma=0.5      # å­¦ä¹ ç‡Ã—0.5
)

# 4. æŸå¤±å‡½æ•°
criterion = nn.CrossEntropyLoss()

# 5. è®­ç»ƒå¾ªç¯
for epoch in range(100):

    for batch_images, batch_labels in train_loader:
        # 5.1 å‰å‘ä¼ æ’­
        outputs = model(batch_images)
        loss = criterion(outputs, batch_labels)

        # 5.2 åå‘ä¼ æ’­
        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
        loss.backward()         # è®¡ç®—æ¢¯åº¦

        # 5.3 æ¢¯åº¦è£å‰ª(å¯é€‰)
        torch.nn.utils.clip_grad_norm_(
            model.parameters(),
            max_norm=1.0
        )

        # 5.4 å‚æ•°æ›´æ–°
        optimizer.step()

    # 5.5 å­¦ä¹ ç‡è°ƒæ•´
    scheduler.step()

    # 5.6 æ‰“å°ä¿¡æ¯
    current_lr = optimizer.param_groups[0]['lr']
    print(f"Epoch {epoch}: loss={loss.item():.4f}, lr={current_lr:.6f}")
'''
    print(code)

    print("="*70)
    print("å…³é”®æ­¥éª¤è§£é‡Š:")
    print("="*70)
    print()
    print("1. optimizer.zero_grad()")
    print("   - æ¸…ç©ºä¸Šä¸€æ¬¡çš„æ¢¯åº¦")
    print("   - å¿…é¡»åœ¨æ¯æ¬¡backwardå‰è°ƒç”¨!")
    print()
    print("2. loss.backward()")
    print("   - è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦")
    print("   - æ¢¯åº¦ç´¯ç§¯åœ¨param.gradä¸­")
    print()
    print("3. optimizer.step()")
    print("   - æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°")
    print("   - å®ç°å…·ä½“çš„ä¼˜åŒ–ç®—æ³•")
    print()
    print("4. scheduler.step()")
    print("   - è°ƒæ•´å­¦ä¹ ç‡")
    print("   - åœ¨æ¯ä¸ªepochç»“æŸåè°ƒç”¨")
    print()


if __name__ == "__main__":
    print("\n\n")
    print("â–ˆ" * 70)
    print(" " * 20 + "ä¼˜åŒ–å™¨å®Œå…¨æŒ‡å—")
    print("â–ˆ" * 70)

    # Part 1: åŸºç¡€æ¦‚å¿µ
    explain_optimizer_basics()

    # Part 2: SGDå®¶æ—
    compare_sgd_variants()

    # Part 3: è‡ªé€‚åº”ä¼˜åŒ–å™¨
    compare_adaptive_optimizers()

    # Part 4: å®æˆ˜å¯¹æ¯”
    demo_optimizer_comparison()

    # Part 5: å‚æ•°è¯¦è§£
    explain_optimizer_parameters()

    # Part 6: é€‰æ‹©æŒ‡å—
    provide_practical_guide()

    # Part 7: å®Œæ•´ç¤ºä¾‹
    demo_complete_training_loop()

    print("\n" + "="*70)
    print("æ€»ç»“")
    print("="*70)
    print("""
ğŸ¯ è®°ä½è¿™äº›å…³é”®ç‚¹:

1. ä¼˜åŒ–å™¨çš„æœ¬è´¨
   - è¾“å…¥: æ¢¯åº¦
   - è¾“å‡º: å‚æ•°æ›´æ–°é‡
   - ç›®æ ‡: å¿«é€Ÿæ‰¾åˆ°lossæœ€å°å€¼

2. å¿«é€Ÿé€‰æ‹©
   - CVä»»åŠ¡: SGD + Momentum
   - NLPä»»åŠ¡: AdamW
   - å¿«é€ŸåŸå‹: Adam
   - ä¸ç¡®å®š: è¯•è¯•AdamW

3. å…³é”®å‚æ•°
   - lr: æœ€é‡è¦,éœ€è¦ä»”ç»†è°ƒ
   - momentum: SGDç”¨0.9
   - weight_decay: 0.01å·¦å³
   - Adamçš„betas: ç”¨é»˜è®¤å€¼

4. è®­ç»ƒæŠ€å·§
   - ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
   - å¤§æ¨¡å‹ç”¨warmup
   - RNNç”¨æ¢¯åº¦è£å‰ª
   - ç›‘æ§lossæ›²çº¿

5. è®°å¿†å£è¯€
   "æ¢¯åº¦å‘Šè¯‰æ–¹å‘,ä¼˜åŒ–å™¨å†³å®šæ­¥æ³•"
""")
    print("="*70 + "\n")
