import torch
from torch import nn



class seeback(nn.Module):
    """CIFAR-10 ç¤ºä¾‹ï¼šå·ç§¯â†’æ± åŒ–â†’å…¨è¿æ¥ï¼Œå®ç° 3Ã—32Ã—32 åˆ° 10 ç±»æ‰“åˆ†çš„å®Œæ•´æµç¨‹ã€‚"""

    def __init__(self):# ä½œç”¨ : åˆå§‹åŒ–æ¨¡å‹, å®šä¹‰å·ç§¯å±‚, æ± åŒ–å±‚, å…¨è¿æ¥å±‚
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), # 32@32x32
            nn.ReLU(inplace=True), #
            nn.MaxPool2d(kernel_size=2, stride=2), # 32@16x16
            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),#ä½œç”¨ :æå–ç‰¹å¾
            nn.ReLU(inplace=True), # 32@16x16
            nn.MaxPool2d(kernel_size=2, stride=2), # 32@8x8
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),# ä½œç”¨ :æå–ç‰¹å¾, ä»32@8x8 åˆ° 64@8x8
            nn.ReLU(inplace=True), # 64@8x8
            nn.MaxPool2d(kernel_size=2, stride=2), # 64@4x4, ä½œç”¨ : æ± åŒ–, ä»64@8x8 åˆ° 64@4x4
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),  # ä½œç”¨ : å±•å¹³, ä»64@4x4 åˆ° 64*4*4 ä¸ªç‰¹å¾å›¾
            nn.Linear(in_features=64 * 4 * 4, out_features=64),  # ä½œç”¨ : å…¨è¿æ¥å±‚, ä»64*4*4 åˆ° 64 ç»´
            nn.ReLU(inplace=True),
            nn.Linear(in_features=64, out_features=10),  # ä½œç”¨ : å…¨è¿æ¥å±‚, ä»64 åˆ° 10 ç±»æ‰“åˆ†
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.features(x) # ä½œç”¨ : æå–ç‰¹å¾, ä»3@32x32 åˆ° 64@4x4, æå–å‡º64ä¸ªç‰¹å¾å›¾
        x = self.classifier(x) # ä½œç”¨ : åˆ†ç±», ä»64@4x4 åˆ° 10 ç±»æ‰“åˆ†
        return x


def describe_flow():
    """æ‰“å°å„é˜¶æ®µå¼ é‡å½¢çŠ¶ï¼Œå¸®åŠ©ç†è§£é€šé“ä¸å°ºå¯¸çš„æ¼”åŒ–ã€‚"""
    model = seeback()
    x = torch.randn(1, 3, 32, 32) # ä½œç”¨ : éšæœºç”Ÿæˆä¸€ä¸ªæ ·æœ¬, 3@32x32
    print("è¾“å…¥:", x.shape)  # 3@32x32, ä½œç”¨ : è¾“å…¥æ ·æœ¬, 3é€šé“, 32x32 å°ºå¯¸

    for layer in model.features: # ä½œç”¨ : æå–ç‰¹å¾, ä»3@32x32 åˆ° 64@4x4, æå–å‡º64ä¸ªç‰¹å¾å›¾
        x = layer(x)
        print(f"{layer.__class__.__name__:>12}: {x.shape}") # ä½œç”¨ : æ‰“å°å½“å‰å±‚çš„è¾“å‡ºå½¢çŠ¶, å¸®åŠ©ç†è§£é€šé“ä¸å°ºå¯¸çš„æ¼”åŒ–


    x = model.classifier[0](x) # ä½œç”¨ : å±•å¹³, ä»64@4x4 åˆ° 64*4*4
    print(f"{model.classifier[0].__class__.__name__:>12}: {x.shape}")  # flatten -> 64*4*4

    x = model.classifier[1](x)# ä½œç”¨ : å…¨è¿æ¥å±‚, ä»64*4*4 åˆ° 64 ç»´
    print(f"{model.classifier[1].__class__.__name__:>12}: {x.shape}")  # Linear -> 64, ä½œç”¨ : å…¨è¿æ¥å±‚, ä»64*4*4 åˆ° 64 ç»´

    x = model.classifier[2](x)
    print(f"{model.classifier[2].__class__.__name__:>12}: {x.shape}")  # ReLU ä¿æŒ 64 ç»´

    x = model.classifier[3](x)# ä½œç”¨ : å…¨è¿æ¥å±‚, ä»64 åˆ° 10 ç±»æ‰“åˆ†
    print(f"{model.classifier[3].__class__.__name__:>12}: {x.shape}")  # Linear -> 10, ä½œç”¨ : å…¨è¿æ¥å±‚, ä»64 åˆ° 10 ç±»æ‰“åˆ†
    #ç»“æœæ˜¯: 10 ç±»æ‰“åˆ†, æ¯ä¸ªç±»åˆ«çš„æ‰“åˆ†èŒƒå›´åœ¨ [-inf, inf] ä¹‹é—´
    # ä½œç”¨ : æ‰“å°æ¨¡å‹çš„è¾“å‡ºå½¢çŠ¶, å¸®åŠ©ç†è§£æ¨¡å‹çš„è¾“å‡º :




def demo_cross_entropy():
    """æ¼”ç¤ºäº¤å‰ç†µæŸå¤±çš„è®¡ç®—è¿‡ç¨‹"""
    import torch.nn.functional as F

    print("\n" + "="*70)
    print("äº¤å‰ç†µæŸå¤±è®¡ç®—æ¼”ç¤º")
    print("="*70)

    # 1. åˆ›å»ºæ¨¡å‹å’ŒæŸå¤±å‡½æ•°
    model = seeback()
    criterion = nn.CrossEntropyLoss()

    # 2. æ¨¡æ‹Ÿä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ® (batch_size=4)
    batch_images = torch.randn(4, 3, 32, 32)  # 4å¼ CIFAR-10å›¾åƒ, 3é€šé“, 32x32 å°ºå¯¸
    batch_labels = torch.tensor([3, 7, 2, 5])  # çœŸå®æ ‡ç­¾ (0-9ä¹‹é—´), 4ä¸ªæ ·æœ¬

    print(f"\nè¾“å…¥å½¢çŠ¶: {batch_images.shape}")# 4å¼ CIFAR-10å›¾åƒ, 3é€šé“, 32x32 å°ºå¯¸
    print(f"æ ‡ç­¾: {batch_labels}")# çœŸå®æ ‡ç­¾ (0-9ä¹‹é—´), 4ä¸ªæ ·æœ¬

    # 3. å‰å‘ä¼ æ’­å¾—åˆ° logits
    logits = model(batch_images) # ä½œç”¨ : å‰å‘ä¼ æ’­, ä»3@32x32 åˆ° 10 ç±»æ‰“åˆ†
    print(f"\næ¨¡å‹è¾“å‡º(logits)å½¢çŠ¶: {logits.shape}")  # (4, 10), ä½œç”¨ : æ¨¡å‹çš„è¾“å‡º, 4ä¸ªæ ·æœ¬, æ¯ä¸ªæ ·æœ¬æœ‰10ä¸ªç±»åˆ«çš„æ‰“åˆ†
    print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬çš„logits:\n{logits[0].detach()}")# ç¬¬ä¸€ä¸ªæ ·æœ¬çš„10ä¸ªç±»åˆ«çš„æ‰“åˆ†, èŒƒå›´åœ¨ [-inf, inf] ä¹‹é—´

    # 4. æŸ¥çœ‹ softmax åçš„æ¦‚ç‡åˆ†å¸ƒ
    probs = F.softmax(logits, dim=1)
    print(f"\nç¬¬ä¸€ä¸ªæ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒ:")
    for i in range(10):
        marker = "âœ“" if i == batch_labels[0] else " "
        print(f"  [{marker}] ç±»åˆ«{i}: {probs[0, i].item():.4f} ({probs[0, i].item()*100:.2f}%)")

    # 5. è®¡ç®—äº¤å‰ç†µæŸå¤±
    loss = criterion(logits, batch_labels) #
    print(f"\näº¤å‰ç†µæŸå¤±: {loss.item():.4f}")

    # 6. æ‰‹åŠ¨éªŒè¯ç¬¬ä¸€ä¸ªæ ·æœ¬çš„æŸå¤±
    print(f"\næ‰‹åŠ¨è®¡ç®—éªŒè¯:")
    correct_class = batch_labels[0].item()
    correct_prob = probs[0, correct_class].item()
    manual_loss = -torch.log(probs[0, correct_class])
    print(f"  æ ·æœ¬1çš„æ­£ç¡®ç±»åˆ«: {correct_class}")
    print(f"  æ¨¡å‹ç»™æ­£ç¡®ç±»åˆ«çš„æ¦‚ç‡: {correct_prob:.4f}")
    print(f"  å•æ ·æœ¬æŸå¤± -log({correct_prob:.4f}): {manual_loss.item():.4f}")

    # 7. è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„æŸå¤±å¹¶æ±‚å¹³å‡
    print(f"\nå„æ ·æœ¬çš„æŸå¤±è¯¦æƒ…:")
    individual_losses = []
    for i in range(len(batch_labels)):
        correct_cls = batch_labels[i].item()
        prob = probs[i, correct_cls].item()
        loss_val = -torch.log(probs[i, correct_cls]).item()
        individual_losses.append(loss_val)
        print(f"  æ ·æœ¬{i+1} | çœŸå®ç±»åˆ«:{correct_cls} | "
              f"é¢„æµ‹æ¦‚ç‡:{prob:.4f} | æŸå¤±:{loss_val:.4f}")

    avg_loss = sum(individual_losses) / len(individual_losses)
    print(f"\næ‰‹åŠ¨è®¡ç®—çš„å¹³å‡æŸå¤±: {avg_loss:.4f}")
    print(f"PyTorchè®¡ç®—çš„æŸå¤±: {loss.item():.4f}")
    print(f"ä¸¤è€…å·®å¼‚: {abs(avg_loss - loss.item()):.6f} (åº”è¯¥éå¸¸æ¥è¿‘0)")

    # 8. è§£é‡Š
    print(f"\n" + "="*70)
    print("å…³é”®ç†è§£:")
    print("="*70)
    print("1. æ¨¡å‹è¾“å‡ºçš„æ˜¯ logits (æœªå½’ä¸€åŒ–çš„åˆ†æ•°)")
    print("2. CrossEntropyLoss å†…éƒ¨è‡ªåŠ¨åš softmax è½¬æ¢ä¸ºæ¦‚ç‡")
    print("3. æŸå¤±å€¼ = -log(æ­£ç¡®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡)")
    print("4. é¢„æµ‹æ¦‚ç‡è¶Šé«˜ â†’ æŸå¤±è¶Šå° â†’ æ¨¡å‹è¶Šå¥½")
    print("5. é¢„æµ‹æ¦‚ç‡è¶Šä½ â†’ æŸå¤±è¶Šå¤§ â†’ æ¨¡å‹éœ€è¦æ”¹è¿›")
    print("="*70 + "\n")


def demo_sgd_training():
    """æ¼”ç¤ºä½¿ç”¨SGDä¼˜åŒ–å™¨è®­ç»ƒæ¨¡å‹"""
    import torch.nn.functional as F

    try:
        import matplotlib.pyplot as plt
        HAS_MATPLOTLIB = True
    except ImportError:
        HAS_MATPLOTLIB = False
        print("âš ï¸  matplotlibæœªå®‰è£…,å°†è·³è¿‡å¯è§†åŒ–éƒ¨åˆ†")

    print("\n" + "="*70)
    print("SGDä¼˜åŒ–å™¨è®­ç»ƒæ¼”ç¤º")
    print("="*70)

    # 1. åˆ›å»ºæ¨¡å‹
    model = seeback()

    # ç»Ÿè®¡å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"\næ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")

    # 2. åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    print(f"ä½¿ç”¨ä¼˜åŒ–å™¨: SGD (lr=0.01, momentum=0)")

    # 3. åˆ›å»ºæŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()

    # 4. å‡†å¤‡è®­ç»ƒæ•°æ® (æ¨¡æ‹ŸCIFAR-10æ•°æ®)
    num_samples = 100
    batch_size = 16
    num_batches = (num_samples + batch_size - 1) // batch_size

    # ç”Ÿæˆéšæœºæ•°æ®
    X = torch.randn(num_samples, 3, 32, 32)
    y = torch.randint(0, 10, (num_samples,))

    print(f"è®­ç»ƒæ•°æ®: {num_samples}ä¸ªæ ·æœ¬, batch_size={batch_size}, æ€»æ‰¹æ¬¡={num_batches}")

    # 5. è®­ç»ƒå¾ªç¯
    num_epochs = 50
    loss_history = []  # è®°å½•æ¯ä¸ªbatchçš„loss

    print(f"\nå¼€å§‹è®­ç»ƒ ({num_epochs} epochs)...")
    print("-" * 70)

    for epoch in range(num_epochs):
        epoch_losses = []

        # åˆ†æ‰¹è®­ç»ƒ
        for batch_idx in range(num_batches):
            # è·å–å½“å‰æ‰¹æ¬¡æ•°æ®
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, num_samples)

            batch_X = X[start_idx:end_idx]
            batch_y = y[start_idx:end_idx]

            # å‰å‘ä¼ æ’­
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
            loss.backward()        # è®¡ç®—æ¢¯åº¦
            optimizer.step()       # æ›´æ–°å‚æ•°

            # è®°å½•loss
            current_loss = loss.item()
            epoch_losses.append(current_loss)
            loss_history.append(current_loss)

            # æ‰“å°loss (æ¯ä¸ªbatch)
            print(f"Epoch [{epoch+1:2d}/{num_epochs}] "
                  f"Batch [{batch_idx+1}/{num_batches}] "
                  f"Loss: {current_loss:.4f}")

        # æ¯ä¸ªepochç»“æŸåæ‰“å°å¹³å‡loss
        avg_loss = sum(epoch_losses) / len(epoch_losses)
        print(f"  â†’ Epoch {epoch+1} å¹³å‡Loss: {avg_loss:.4f}")
        print()

    print("-" * 70)
    print("è®­ç»ƒå®Œæˆ!")
    print(f"åˆå§‹loss: {loss_history[0]:.4f}")
    print(f"æœ€ç»ˆloss: {loss_history[-1]:.4f}")
    print(f"æ€»ä¸‹é™: {loss_history[0] - loss_history[-1]:.4f}")

    # 6. å¯è§†åŒ–lossæ›²çº¿
    if HAS_MATPLOTLIB:
        import os
        os.makedirs('artifacts', exist_ok=True)

        plt.figure(figsize=(12, 6))

        # ç»˜åˆ¶lossæ›²çº¿
        plt.plot(loss_history, 'b-', linewidth=1, alpha=0.6, label='Batch Loss')

        # è®¡ç®—å¹¶ç»˜åˆ¶ç§»åŠ¨å¹³å‡ (å¹³æ»‘æ›²çº¿)
        window_size = 10
        if len(loss_history) >= window_size:
            moving_avg = []
            for i in range(len(loss_history) - window_size + 1):
                avg = sum(loss_history[i:i+window_size]) / window_size
                moving_avg.append(avg)
            plt.plot(range(window_size-1, len(loss_history)), moving_avg,
                    'r-', linewidth=2, label=f'{window_size}-Batch Moving Average')

        # æ ‡æ³¨å…³é”®ç‚¹
        plt.scatter([0], [loss_history[0]], color='green', s=100,
                   zorder=5, label=f'Start: {loss_history[0]:.4f}')
        plt.scatter([len(loss_history)-1], [loss_history[-1]], color='red',
                   s=100, zorder=5, label=f'End: {loss_history[-1]:.4f}')

        plt.xlabel('è®­ç»ƒæ­¥æ•° (Batch)', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.title('SGDè®­ç»ƒLossä¸‹é™æ›²çº¿', fontsize=14, fontweight='bold')
        plt.legend(loc='upper right')
        plt.grid(True, alpha=0.3)

        # ä¿å­˜å›¾ç‰‡
        save_path = 'artifacts/sgd_training_loss.png'
        plt.savefig(save_path, dpi=100, bbox_inches='tight')
        print(f"\nğŸ“Š Lossæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
        plt.close()

    # 7. æµ‹è¯•æœ€ç»ˆæ¨¡å‹æ€§èƒ½
    print("\n" + "="*70)
    print("æµ‹è¯•æœ€ç»ˆæ¨¡å‹æ€§èƒ½")
    print("="*70)

    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
    with torch.no_grad():
        # åœ¨è®­ç»ƒæ•°æ®ä¸Šæµ‹è¯•
        outputs = model(X)
        _, predicted = torch.max(outputs, 1)
        correct = (predicted == y).sum().item()
        accuracy = 100 * correct / num_samples

        print(f"è®­ç»ƒæ•°æ®å‡†ç¡®ç‡: {correct}/{num_samples} = {accuracy:.2f}%")

        # æ˜¾ç¤ºæ¦‚ç‡åˆ†å¸ƒç¤ºä¾‹
        probs = F.softmax(outputs[0:1], dim=1)
        print(f"\nç¬¬ä¸€ä¸ªæ ·æœ¬é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ:")
        print(f"  çœŸå®ç±»åˆ«: {y[0].item()}")
        print(f"  é¢„æµ‹ç±»åˆ«: {predicted[0].item()}")
        for i in range(10):
            marker = "âœ“" if i == y[0].item() else " "
            pred_marker = "ğŸ‘‰" if i == predicted[0].item() else "  "
            print(f"  [{marker}] {pred_marker} ç±»åˆ«{i}: {probs[0, i].item():.4f} "
                  f"({probs[0, i].item()*100:.2f}%)")


#ç§‘æ™®ï¼šåå‘ä¼ æ’­æ„æ€å°±æ˜¯ï¼Œ
#å°è¯•å¦‚ä½•è°ƒæ•´ç½‘ç»œè¿‡ç¨‹ä¸­çš„å‚æ•°æ‰ä¼šå¯¼è‡´æœ€ç»ˆçš„losså˜å°ï¼ˆå› ä¸ºæ˜¯ä»losså¼€å§‹æ¨å¯¼å‚æ•°ï¼Œå’Œç½‘ç»œçš„é¡ºåºç›¸åï¼Œæ‰€ä»¥å«åå‘ä¼ æ’­ï¼‰ï¼Œä»¥åŠæ¢¯åº¦çš„ç†è§£å¯ä»¥ç›´æ¥å½“æˆ"æ–œç‡"

if __name__ == "__main__":
    describe_flow() # ä½œç”¨ : æ‰“å°å„é˜¶æ®µå¼ é‡å½¢çŠ¶, å¸®åŠ©ç†è§£é€šé“ä¸å°ºå¯¸çš„æ¼”åŒ–
    demo_cross_entropy()  # æ¼”ç¤ºäº¤å‰ç†µæŸå¤±è®¡ç®—
    demo_sgd_training()  # æ¼”ç¤ºSGDä¼˜åŒ–å™¨è®­ç»ƒ














"""
PyTorch Module è°ƒç”¨é“¾è¡¥å……
=========================

1. å®ä¾‹åŒ–æ¨¡å‹è§¦å‘ __init__
   model = seeback() æ—¶ï¼Œå…ˆè°ƒç”¨ nn.Module.__init__ï¼Œéšåæ‰§è¡Œè‡ªå®šä¹‰ __init__ï¼Œ
   å°†å·ç§¯å±‚ã€æ± åŒ–å±‚ã€å…¨è¿æ¥å±‚æ³¨å†Œä¸ºå­æ¨¡å—ã€‚åªè¦å†™æˆ self.xxx = nn.Module()ï¼Œ
   PyTorch å°±ä¼šè‡ªåŠ¨æŠŠå…¶å‚æ•°åŠ å…¥å¯è®­ç»ƒåˆ—è¡¨ã€‚

2. model(x) è°ƒç”¨ __call__
   æ‰§è¡Œ model(inputs) æ—¶ï¼Œnn.Module.__call__ ä¼šï¼š
     - å¤„ç†å‰å‘/åå‘ hook
     - ç¡®è®¤è®­ç»ƒæˆ–è¯„ä¼°æ¨¡å¼ self.training
     - è°ƒç”¨ forward(inputs)
   å› æ­¤ classifier[0](x) å®é™…ç­‰ä»·äº classifier[0].forward(x)ã€‚

3. Sequential æ”¯æŒç´¢å¼•è®¿é—®
   nn.Sequential å®ç°äº† __getitem__ï¼Œå¯ä»¥ç”¨ä¸‹æ ‡å–å‡ºå…¶ä¸­çš„å­å±‚ã€‚
   æ¯”å¦‚ classifier[0] å¯¹åº” Flattenï¼Œclassifier[1] å¯¹åº”ç¬¬ä¸€ä¸ª Linearã€‚

4. è‡ªåŠ¨æ±‚å¯¼æµç¨‹
   loss.backward() ä¼šæ²¿ç€å‰å‘æ„å»ºçš„è®¡ç®—å›¾å›ä¼ æ¢¯åº¦ï¼›optimizer.step()
   æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°ï¼Œæ— éœ€æ‰‹åŠ¨æ±‚å¯¼ã€‚

å…¸å‹è®­ç»ƒè¿­ä»£ï¼š
   model = seeback()
   logits = model(batch_images)
   loss = criterion(logits, labels)
   loss.backward()
   optimizer.step()
   optimizer.zero_grad()
"""
