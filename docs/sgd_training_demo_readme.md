# SGDä¼˜åŒ–å™¨è®­ç»ƒæ¼”ç¤ºè¯´æ˜

## ğŸ“‹ åŠŸèƒ½è¯´æ˜

åœ¨ `src/experiments/torch_nn/CIFARSeeback.py` ä¸­æ–°å¢äº† `demo_sgd_training()` å‡½æ•°,å®Œæ•´æ¼”ç¤ºäº†ä½¿ç”¨SGDä¼˜åŒ–å™¨è®­ç»ƒCIFAR-10æ¨¡å‹çš„è¿‡ç¨‹ã€‚

## ğŸ¯ å®ç°å†…å®¹

### 1. è®­ç»ƒé…ç½®
- **æ¨¡å‹**: `seeback` (CIFAR-10åˆ†ç±»æ¨¡å‹, 96,426ä¸ªå‚æ•°)
- **ä¼˜åŒ–å™¨**: SGD (lr=0.01, æ— momentum)
- **æŸå¤±å‡½æ•°**: CrossEntropyLoss
- **è®­ç»ƒæ•°æ®**: 100ä¸ªæ ·æœ¬ (æ¨¡æ‹Ÿæ•°æ®)
- **Batch size**: 16
- **Epochs**: 50
- **æ€»æ‰¹æ¬¡**: 7ä¸ªbatch/epoch

### 2. è®­ç»ƒæµç¨‹

```python
for epoch in range(50):
    for batch in batches:
        # 1. å‰å‘ä¼ æ’­
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)

        # 2. åå‘ä¼ æ’­
        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
        loss.backward()        # è®¡ç®—æ¢¯åº¦
        optimizer.step()       # æ›´æ–°å‚æ•°

        # 3. æ‰“å°loss
        print(f"Epoch [{epoch}/{50}] Batch [{batch}/{7}] Loss: {loss:.4f}")
```

### 3. è¾“å‡ºæ ¼å¼

#### è®­ç»ƒè¿‡ç¨‹è¾“å‡º
```
======================================================================
SGDä¼˜åŒ–å™¨è®­ç»ƒæ¼”ç¤º
======================================================================

æ¨¡å‹å‚æ•°æ€»æ•°: 96,426
ä½¿ç”¨ä¼˜åŒ–å™¨: SGD (lr=0.01, momentum=0)
è®­ç»ƒæ•°æ®: 100ä¸ªæ ·æœ¬, batch_size=16, æ€»æ‰¹æ¬¡=7

å¼€å§‹è®­ç»ƒ (50 epochs)...
----------------------------------------------------------------------
Epoch [ 1/50] Batch [1/7] Loss: 2.3041
Epoch [ 1/50] Batch [2/7] Loss: 2.3495
Epoch [ 1/50] Batch [3/7] Loss: 2.3062
...
Epoch [50/50] Batch [7/7] Loss: 2.1160
  â†’ Epoch 50 å¹³å‡Loss: 2.2267

----------------------------------------------------------------------
è®­ç»ƒå®Œæˆ!
åˆå§‹loss: 2.3297
æœ€ç»ˆloss: 2.1160
æ€»ä¸‹é™: 0.2137
```

#### æ¨¡å‹æ€§èƒ½æµ‹è¯•
```
======================================================================
æµ‹è¯•æœ€ç»ˆæ¨¡å‹æ€§èƒ½
======================================================================
è®­ç»ƒæ•°æ®å‡†ç¡®ç‡: 12/100 = 12.00%

ç¬¬ä¸€ä¸ªæ ·æœ¬é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ:
  çœŸå®ç±»åˆ«: 2
  é¢„æµ‹ç±»åˆ«: 8
  [ ]    ç±»åˆ«0: 0.1096 (10.96%)
  [ ]    ç±»åˆ«1: 0.1120 (11.20%)
  [âœ“]    ç±»åˆ«2: 0.0990 (9.90%)  â† çœŸå®ç±»åˆ«
  ...
  [ ] ğŸ‘‰ ç±»åˆ«8: 0.1354 (13.54%)  â† é¢„æµ‹ç±»åˆ«
  ...
```

### 4. å¯è§†åŒ–åŠŸèƒ½

ä»£ç åŒ…å«lossæ›²çº¿å¯è§†åŒ–åŠŸèƒ½,éœ€è¦å®‰è£…matplotlib:

```bash
pip install matplotlib
```

è¿è¡Œåä¼šç”Ÿæˆ:
- **æ–‡ä»¶**: `artifacts/sgd_training_loss.png`
- **å†…å®¹**:
  - è“è‰²æ›²çº¿: æ¯ä¸ªbatchçš„loss
  - çº¢è‰²æ›²çº¿: ç§»åŠ¨å¹³å‡loss (10-batchçª—å£)
  - ç»¿ç‚¹: åˆå§‹loss
  - çº¢ç‚¹: æœ€ç»ˆloss

## ğŸ” ä»£ç å…³é”®ç‚¹è§£æ

### 1. ä¼˜åŒ–å™¨åˆ›å»º
```python
optimizer = torch.optim.SGD(
    model.parameters(),  # éœ€è¦ä¼˜åŒ–çš„å‚æ•°
    lr=0.01              # å­¦ä¹ ç‡
)
```

### 2. è®­ç»ƒæ­¥éª¤
```python
# âš ï¸ å…³é”®æ­¥éª¤,é¡ºåºä¸èƒ½é”™!

optimizer.zero_grad()  # 1. æ¸…ç©ºä¹‹å‰çš„æ¢¯åº¦
loss.backward()        # 2. è®¡ç®—å½“å‰æ¢¯åº¦
optimizer.step()       # 3. æ›´æ–°å‚æ•°
```

### 3. Lossè®°å½•
```python
loss_history = []  # è®°å½•æ¯ä¸ªbatchçš„loss

# è®­ç»ƒå¾ªç¯ä¸­
current_loss = loss.item()  # è·å–æ ‡é‡å€¼
loss_history.append(current_loss)
```

### 4. æ¨¡å‹è¯„ä¼°
```python
model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
    outputs = model(X)
    _, predicted = torch.max(outputs, 1)
    accuracy = (predicted == y).sum().item() / len(y)
```

## ğŸ“Š è§‚å¯Ÿä¸åˆ†æ

### 1. Lossä¸‹é™è¶‹åŠ¿
```
åˆå§‹loss: ~2.33
æœ€ç»ˆloss: ~2.12
æ€»ä¸‹é™: ~0.21
```

**åˆ†æ:**
- Lossæœ‰æ‰€ä¸‹é™,è¯´æ˜æ¨¡å‹åœ¨å­¦ä¹ 
- ä¸‹é™å¹…åº¦è¾ƒå°,å¯èƒ½åŸå› :
  - å­¦ä¹ ç‡è¾ƒå° (0.01)
  - æ²¡æœ‰ä½¿ç”¨momentum
  - è®­ç»ƒæ•°æ®è¾ƒå°‘ (100ä¸ªæ ·æœ¬)
  - æ¨¡å‹åˆå§‹åŒ–çš„éšæœºæ€§

### 2. å‡†ç¡®ç‡åˆ†æ
```
è®­ç»ƒæ•°æ®å‡†ç¡®ç‡: ~12%
```

**åˆ†æ:**
- 10åˆ†ç±»éšæœºçŒœæµ‹å‡†ç¡®ç‡ä¸º10%
- å½“å‰å‡†ç¡®ç‡ç•¥é«˜äºéšæœº
- è¯´æ˜æ¨¡å‹åˆšå¼€å§‹å­¦ä¹ ,è¿˜éœ€è¦æ›´å¤šè®­ç»ƒ

### 3. æ¦‚ç‡åˆ†å¸ƒ
```
å„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€ (9%-13%)
```

**åˆ†æ:**
- æ¨¡å‹è¿˜æœªå……åˆ†å­¦ä¹ 
- å¯¹å„ç±»åˆ«çš„åˆ¤æ–­ä¸å¤Ÿè‡ªä¿¡
- éœ€è¦æ›´é•¿æ—¶é—´è®­ç»ƒæˆ–æ›´å¥½çš„è¶…å‚æ•°

## ğŸš€ æ”¹è¿›å»ºè®®

### 1. æé«˜å­¦ä¹ æ•ˆç‡
```python
# æ·»åŠ momentum
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9  # æ·»åŠ åŠ¨é‡
)
```

### 2. å¢åŠ è®­ç»ƒæ•°æ®
```python
num_samples = 1000  # å¢åŠ åˆ°1000ä¸ªæ ·æœ¬
```

### 3. ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦
```python
scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer,
    step_size=20,
    gamma=0.5
)

# è®­ç»ƒå¾ªç¯ä¸­
for epoch in range(100):
    # ... è®­ç»ƒä»£ç  ...
    scheduler.step()  # è°ƒæ•´å­¦ä¹ ç‡
```

### 4. å°è¯•å…¶ä»–ä¼˜åŒ–å™¨
```python
# ä½¿ç”¨Adamä¼˜åŒ–å™¨
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=0.001
)
```

## ğŸ“ è¿è¡Œæ–¹æ³•

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /home/seeback/PycharmProjects/DeepLearning

# è¿è¡Œå®Œæ•´æ¼”ç¤º
python src/experiments/torch_nn/CIFARSeeback.py

# æˆ–åªè¿è¡ŒSGDè®­ç»ƒéƒ¨åˆ†
python -c "from src.experiments.torch_nn.CIFARSeeback import demo_sgd_training; demo_sgd_training()"
```

## ğŸ“ å­¦ä¹ è¦ç‚¹

### 1. ä¼˜åŒ–å™¨çš„ä½œç”¨
- æ¥æ”¶æ¢¯åº¦ (param.grad)
- å†³å®šå‚æ•°æ›´æ–°ç­–ç•¥
- ç›®æ ‡: è®©lossä¸‹é™

### 2. è®­ç»ƒå¾ªç¯çš„æ ¸å¿ƒæ­¥éª¤
```
å‰å‘ä¼ æ’­ â†’ è®¡ç®—loss â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•°
```

### 3. å¿…é¡»è®°ä½çš„é¡ºåº
```python
optimizer.zero_grad()  # 1. å…ˆæ¸…ç©º
loss.backward()        # 2. å†è®¡ç®—
optimizer.step()       # 3. æœ€åæ›´æ–°
```

### 4. Lossæ‰“å°çš„æ„ä¹‰
- è§‚å¯Ÿè®­ç»ƒè¶‹åŠ¿
- åˆ¤æ–­æ˜¯å¦æ”¶æ•›
- å‘ç°å¼‚å¸¸æƒ…å†µ (å¦‚lossçˆ†ç‚¸)

## ğŸ”— ç›¸å…³æ–‡ä»¶

- **ä¸»æ–‡ä»¶**: `src/experiments/torch_nn/CIFARSeeback.py`
- **ä¼˜åŒ–å™¨è¯¦è§£**: `src/experiments/torch_nn/OptimizerExplained.py`
- **åå‘ä¼ æ’­è¯¦è§£**: `src/experiments/torch_nn/BackpropExplained.py`
- **é€ŸæŸ¥è¡¨**: `docs/optimizer_cheatsheet.md`

---

**Happy Learning!** ğŸ‰
