{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6473a049",
   "metadata": {},
   "source": [
    "# TorchData 节点式数据管线速查笔记\n",
    "\n",
    "> 记录 2024 年 6 月 TorchData 路线更新、`torchdata.nodes` Beta 能力与迁移建议，帮助在 `torch.utils.data` 基础上升级多线程/多进程并行、流式管线与中途断点恢复。若环境尚未安装 `torchdata`，可先阅读文本理解设计理念。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8112e4",
   "metadata": {},
   "source": [
    "## 状态更新：DataPipes 与 DataLoader V2 退场\n",
    "- **2024-06 公告**：官方停更 `DataPipes` 与 `DataLoaderV2`，TorchData 仓库回归逐步增强 `torch.utils.data.DataLoader` 的路线。\n",
    "- **弃用计划**：`torchdata==0.8.0`（2024-07）开始标记弃用，`0.10.0`（2024 年底）彻底删除；需使用旧实现请锁定 `torchdata<=0.9.0`。\n",
    "- **迁移建议**：提前评估新 API，尤其是 `torchdata.nodes` 与 `torchdata.stateful_dataloader`，避免后期被动升级。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c600ca",
   "metadata": {},
   "source": [
    "## torchdata.nodes (Beta) 快速概览\n",
    "- **定位**：提供一组可组合的 *迭代器*（Iterator），将数据加载拆为节点式流水线，兼容流式、Map-style、Sampler 等模式。\n",
    "- **核心特性**：\n",
    "  - 同时支持多线程与多进程，并可按节点粒度选择并行方式。\n",
    "  - 通过 `state_dict` / `load_state_dict` 支持中途 checkpoint。\n",
    "  - 强制迭代器范式（实现 `next`, `reset`, `get_state`），简化状态管理。\n",
    "- **安装**：`pip install torchdata>=0.10.0`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dab14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 torchdata 可用性并输出版本\n",
    "try:\n",
    "    import torchdata\n",
    "    print('torchdata 版本:', torchdata.__version__)\n",
    "    from torchdata import nodes as tn\n",
    "    print('torchdata.nodes 可用：', hasattr(torchdata, 'nodes'))\n",
    "except Exception as err:\n",
    "    print('未检测到 torchdata，或导入失败 ->', err)\n",
    "    print('请先安装: pip install torchdata>=0.10.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db4e57",
   "metadata": {},
   "source": [
    "## 为什么需要 torchdata.nodes？\n",
    "- **多进程痛点**：传统 DataLoader 需复制 Dataset 内存、IPC 队列慢、必须在 worker 端做批处理。\n",
    "- **多线程重回舞台**：配合 GIL 释放函数与 Free-Threaded Python，多线程不再受限，`nodes` 可自由切换线程/进程。\n",
    "- **流式数据模型**：原 Map-style 难以扩展至大规模和多数据集场景，`nodes` 采用迭代器链路，天然支持多源融合。\n",
    "- **多数据集策略**：现有 Sampler 仅面向单数据集，难以实现加权采样、Round-Robin 等策略；`nodes` 通过组合节点解决。\n",
    "- **IterableDataset 分片**：传统方案需手动调用 `get_worker_info`，`nodes` 则让分片逻辑内聚在节点中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e96a39",
   "metadata": {},
   "source": [
    "## 架构要点与设计选择\n",
    "- **BaseNode**：所有节点需继承 `torchdata.nodes.BaseNode` 并实现 `next()`, `reset(initial_state)`, `get_state()`；禁止使用生成器以确保状态显式可控。\n",
    "- **Loader**：将 `BaseNode` 包装成熟悉的 Iterable，统一处理 `reset()`、`state_dict()`。\n",
    "- **迭代器优先**：所有组件处理的是 Iterator，而非 Iterable，避免“多活迭代器”状态难题。\n",
    "- **状态管理**：通过 `reset(initial_state)` 复现任意时刻状态，支持 mid-epoch checkpoint。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c91e8b",
   "metadata": {},
   "source": [
    "### 常用节点速览\n",
    "- `IterableWrapper`：将任意 Iterable 包装为 BaseNode。\n",
    "- `SamplerWrapper`：包装 `torch.utils.data.Sampler` 并保持 `set_epoch` 钩子。\n",
    "- `Batcher` / `Unbatcher`：批处理与扁平化。\n",
    "- `Mapper` / `ParallelMapper`：串行或并行映射函数，可选线程/进程、是否保持顺序。\n",
    "- `Prefetcher` / `PinMemory`：预取与页锁内存。\n",
    "- `MultiNodeWeightedSampler`：权重调度多数据源。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1382a5",
   "metadata": {},
   "source": [
    "## 入门示例\n",
    "以下示例在未安装 torchdata 时不会执行，可在 Notebook 中观察或待安装后运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29425089",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torchdata.nodes import IterableWrapper, ParallelMapper, Loader\n",
    "    node = IterableWrapper(range(10))\n",
    "    node = ParallelMapper(node, map_fn=lambda x: x ** 2, num_workers=3, method=\"thread\")\n",
    "    loader = Loader(node)\n",
    "    print(list(loader))\n",
    "except ImportError as err:\n",
    "    print('缺少 torchdata，跳过示例 ->', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ad91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch.utils.data\n",
    "    from torch.utils.data import RandomSampler\n",
    "    from torchdata.nodes import SamplerWrapper, ParallelMapper, Loader\n",
    "\n",
    "    class SquaredDataset(torch.utils.data.Dataset):\n",
    "        def __getitem__(self, i: int) -> int:\n",
    "            return i ** 2\n",
    "        def __len__(self):\n",
    "            return 10\n",
    "\n",
    "    dataset = SquaredDataset()\n",
    "    sampler = RandomSampler(dataset)\n",
    "    node = SamplerWrapper(sampler)\n",
    "    node = ParallelMapper(node, map_fn=dataset.__getitem__, num_workers=3, method=\"thread\")\n",
    "    loader = Loader(node)\n",
    "    print(list(loader))\n",
    "except ImportError as err:\n",
    "    print('缺少 torchdata，跳过示例 ->', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8fef0",
   "metadata": {},
   "source": [
    "## 迁移指南：从 DataLoader 到 Nodes 管线\n",
    "- 构建顺序：`SamplerWrapper` → `Batcher` → `Mapper/ParallelMapper` → 可选 `PinMemory`/`Prefetcher` → `Loader`。\n",
    "- `Loader` 负责在 epoch 之间调用 `reset()` 并暴露 `state_dict()` 接口。\n",
    "- `map_fn` 可封装 `Dataset.__getitem__` + `collate_fn`，与原 DataLoader 行为匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf2c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "\n",
    "try:\n",
    "    import torchdata.nodes as tn\n",
    "    from torch.utils.data import RandomSampler, SequentialSampler, default_collate, Dataset\n",
    "\n",
    "    class MapAndCollate:\n",
    "        def __init__(self, dataset: Dataset, collate_fn: Callable):\n",
    "            self.dataset = dataset\n",
    "            self.collate_fn = collate_fn\n",
    "        def __call__(self, batch_indices: List[int]):\n",
    "            batch = [self.dataset[i] for i in batch_indices]\n",
    "            return self.collate_fn(batch)\n",
    "\n",
    "    def nodes_dataloader(\n",
    "        dataset: Dataset,\n",
    "        batch_size: int,\n",
    "        shuffle: bool,\n",
    "        num_workers: int,\n",
    "        collate_fn: Callable | None,\n",
    "        pin_memory: bool,\n",
    "        drop_last: bool,\n",
    "    ):\n",
    "        sampler = RandomSampler(dataset) if shuffle else SequentialSampler(dataset)\n",
    "        node = tn.SamplerWrapper(sampler)\n",
    "        node = tn.Batcher(node, batch_size=batch_size, drop_last=drop_last)\n",
    "        map_fn = MapAndCollate(dataset, collate_fn or default_collate)\n",
    "        node = tn.ParallelMapper(node, map_fn=map_fn, num_workers=num_workers, method=\"process\", in_order=True)\n",
    "        if pin_memory:\n",
    "            node = tn.PinMemory(node)\n",
    "        node = tn.Prefetcher(node, prefetch_factor=max(1, num_workers * 2))\n",
    "        return tn.Loader(node)\n",
    "except ImportError as err:\n",
    "    print('缺少 torchdata，示例仅供参考 ->', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436881fa",
   "metadata": {},
   "source": [
    "### 示例：状态恢复\n",
    "- `Loader.state_dict()` 返回上一迭代器的状态，可在中途保存。\n",
    "- `Loader.load_state_dict(sd)` 恢复后续迭代位置，便于断点续训。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1afc77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torchdata.nodes as tn\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    class SquaredDataset(Dataset):\n",
    "        def __len__(self):\n",
    "            return 14\n",
    "        def __getitem__(self, idx: int):\n",
    "            return idx ** 2\n",
    "\n",
    "    loader = nodes_dataloader(\n",
    "        dataset=SquaredDataset(),\n",
    "        batch_size=3,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        collate_fn=None,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    cached = []\n",
    "    state = None\n",
    "    for i, batch in enumerate(loader):\n",
    "        cached.append(batch)\n",
    "        if i == 2:\n",
    "            state = loader.state_dict()\n",
    "            break\n",
    "    if state is not None:\n",
    "        loader.load_state_dict(state)\n",
    "        resumed = list(loader)\n",
    "        print('断点后批次数:', len(resumed))\n",
    "        print('前后批次一致:', cached[3:] == resumed)\n",
    "except Exception as err:\n",
    "    print('运行示例失败 ->', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c76ea8",
   "metadata": {},
   "source": [
    "## 性能要点\n",
    "- 多线程与多进程可按节点粒度选择；在 Free-Threaded Python (3.13t) 中多线程可饱和内存带宽。\n",
    "- 可将繁重预处理放在 `ParallelMapper`，GPU 预处理也可在多线程模式下进行。\n",
    "- 早期基准表明视频解码场景中 `nodes` 与原 DataLoader 持平或略优；详见官方 PyTorch Conf 2024 分享。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130ef79",
   "metadata": {},
   "source": [
    "## 设计决策回顾\n",
    "- 禁止生成器：生成器隐式存储栈状态，难以落地通用的 `state_dict`。\n",
    "- 显式 `reset(initial_state)`：集中处理初始化逻辑，并确保状态恢复可控。\n",
    "- Loader 统一管理 epoch 生命周期，用户无需显式调用 `reset()`。\n",
    "- 警惕 `StopIteration`：节点在流式管线中需明确何时结束，Loader 可配置 `restart_on_stop_iteration`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d6ca8",
   "metadata": {},
   "source": [
    "## StatefulDataLoader：DataLoader 版断点续训\n",
    "- `torchdata.stateful_dataloader.StatefulDataLoader` 为现有 DataLoader 增强版，提供 `state_dict`/`load_state_dict`。\n",
    "- 支持聚合 Sampler/Dataset 状态，并在多进程间转发。\n",
    "- `snapshot_every_n_steps` 可调节同步频率，权衡 checkpoint 精度与开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73376796",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torchdata.stateful_dataloader import StatefulDataLoader\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    class NoisyRange(Dataset):\n",
    "        def __init__(self, high: int, mean: float = 0.0, std: float = 1.0):\n",
    "            self.high, self.mean, self.std = high, mean, std\n",
    "        def __len__(self):\n",
    "            return self.high\n",
    "        def __getitem__(self, idx: int):\n",
    "            noise = torch.randn(1).item() * self.std + self.mean\n",
    "            return idx + noise\n",
    "\n",
    "    dl = StatefulDataLoader(NoisyRange(8), batch_size=2, num_workers=0)\n",
    "    state = None\n",
    "    for i, batch in enumerate(dl):\n",
    "        print('batch', i, batch)\n",
    "        if i == 1:\n",
    "            state = dl.state_dict()\n",
    "            break\n",
    "    if state:\n",
    "        dl.load_state_dict(state)\n",
    "        print('恢复后继续:', list(dl))\n",
    "except ImportError as err:\n",
    "    print('缺少 torchdata，跳过 StatefulDataLoader 示例 ->', err)\n",
    "except Exception as err:\n",
    "    print('StatefulDataLoader 示例运行失败 ->', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c829a6",
   "metadata": {},
   "source": [
    "## 迁移与版本策略\n",
    "- **短期策略**：依赖 DataPipes / DataLoaderV2 的项目需锁定 `torchdata<=0.9.0`，并规划迁移到 `nodes` 或 `StatefulDataLoader`。\n",
    "- **测试验证**：引入 `nodes` 后需重点验证并行方式、状态恢复、性能指标。\n",
    "- **反馈渠道**：官方建议通过 GitHub issue 提供建议，帮助完善未来路线。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f1e10",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "- 官方公告：`torchdata` June 2024 Status Update\n",
    "- 文档：Getting Started With torchdata.nodes (beta)\n",
    "- 教程：Migrating to torchdata.nodes from torch.utils.data\n",
    "- API：`torchdata.nodes`、`torchdata.stateful_dataloader`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
