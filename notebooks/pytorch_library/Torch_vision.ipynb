{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef18352",
   "metadata": {},
   "source": [
    "# TorchVision 综合实践手册\n",
    "\n",
    "> 本手册旨在系统梳理 `torchvision` 的核心能力，覆盖变换、模型、数据集、工具函数、算子、编解码与特征提取等模块。结合项目 `dataset/arch_linux` 提供的示例图像，通过代码演示帮助快速上手与对照查阅。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview-table",
   "metadata": {},
   "source": [
    "## 模块总览表\n",
    "\n",
    "| 模块 | 典型入口 | 功能要点 | 主要用途 |\n",
    "| --- | --- | --- | --- |\n",
    "| `torchvision.transforms` / `transforms.v2` | `T.Compose`, `v2.RandomResizedCrop` | 图像/视频/目标的增广与预处理，V2 支持 `TVTensor` 与批处理 | 构建训练/推理的统一输入流水线 |\n",
    "| `torchvision.datasets` | `datasets.CIFAR10`, `ImageFolder` | 内置经典数据集、文件夹读取与自定义基类 | 快速获得样本或包装本地数据 |\n",
    "| `torchvision.models` | `resnet18`, `detection.fasterrcnn_resnet50_fpn` | 分类、检测、分割、视频、光流等预训练模型 | 迁移学习、特征抽取、端到端推理 |\n",
    "| `torchvision.ops` | `box_convert`, `sigmoid_focal_loss` | 检测/分割常用算子、损失、层模块 | 自定义检测头、后处理 |\n",
    "| `torchvision.utils` | `draw_bounding_boxes`, `save_image` | 可视化注释、保存图像网格 | 训练过程调试与结果展示 |\n",
    "| `torchvision.io` | `read_image`, `encode_jpeg`, `decode_video` | 图像/视频的编解码与张量读取 | 构建高效的 IO / streaming 管道 |\n",
    "| `torchvision.feature_extraction` | `create_feature_extractor` | 抽取中间特征用于可视化、蒸馏 | 模型解释、迁移学习 |\n",
    "| `torchvision.tv_tensors` | `tv_t.Image`, `BoundingBoxes` | 携带元数据的增强张量类型 | 同步处理图像、框、关键点、掩码 |\n",
    "| `torchvision.prototype`（可选） | `torchvision.prototype.transforms` | 实验性 API，跟踪前沿特性 | 评估新特性或未来 API 变更 |"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.tv_tensors as tv_t\n",
    "from torchvision.utils import (\n",
    "    draw_bounding_boxes,\n",
    "    draw_segmentation_masks,\n",
    "    draw_keypoints,\n",
    "    flow_to_image,\n",
    "    make_grid,\n",
    "    save_image,\n",
    ")\n",
    "from torchvision.io import (\n",
    "    read_image,\n",
    "    decode_image,\n",
    "    encode_jpeg,\n",
    "    decode_video,\n",
    "    write_png,\n",
    ")\n",
    "from torchvision.models import (\n",
    "    resnet18,\n",
    "    ResNet18_Weights,\n",
    "    segmentation,\n",
    "    detection,\n",
    "    video as video_models,\n",
    "    optical_flow,\n",
    "    feature_extraction,\n",
    ")\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "os.environ.setdefault('OMP_NUM_THREADS', '1')\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "#  示例数据集目录 : 目的是展示 torchvision 对图像的处理能力\n",
    "DATASET_ROOT = Path('dataset/arch_linux')# 示例数据集目录\n",
    "SAMPLE_IMAGE_PATH = sorted(DATASET_ROOT.glob('*.png'))[0]# 示例图像路径\n",
    "base_pil = Image.open(SAMPLE_IMAGE_PATH).convert('RGB')# 转换为 RGB 模式\n",
    "base_tensor = T.ToTensor()(base_pil) # 转换为 tensor 类型，范围 [0, 1]\n",
    "base_uint8 = (base_tensor * 255).to(torch.uint8) # 转换为 uint8 类型，范围 [0, 255]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 4) # 图像显示大小\n",
    "plt.rcParams['figure.facecolor'] = 'white' # 白色背景\n",
    "\n",
    "\n",
    "# 可视化工具函数\n",
    "def show_images(images, titles=None, cols=3, figsize=(12, 4)):\n",
    "    \"\"\"\n",
    "    可视化多个图像，支持 PIL.Image 和 torch.Tensor。\n",
    "    :param images: 图像列表，每个元素可以是 PIL.Image 或 torch.Tensor。\n",
    "    :param titles: 图像标题列表，与 images 长度相同。\n",
    "    :param cols: 每行显示的图像数量。\n",
    "    :param figsize: 图像显示大小。\n",
    "    \"\"\"\n",
    "    pil_images = []\n",
    "    for img in images:\n",
    "        if isinstance(img, Image.Image):\n",
    "            pil_images.append(img)\n",
    "        elif isinstance(img, torch.Tensor):\n",
    "            tensor = img.detach().clone()\n",
    "            if tensor.ndim == 4:\n",
    "                tensor = make_grid(tensor)\n",
    "            if tensor.dtype != torch.float32:\n",
    "                tensor = tensor.float() / 255.0\n",
    "            tensor = tensor.clamp(0, 1)\n",
    "            pil_images.append(T.ToPILImage()(tensor))\n",
    "        else:\n",
    "            raise TypeError(f'不支持的图像类型: {type(img)}')\n",
    "    total = len(pil_images)\n",
    "    if total == 0:\n",
    "        return\n",
    "    cols = min(cols, total)\n",
    "    rows = (total + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    titles = list(titles or [])\n",
    "    if len(titles) < total:\n",
    "        titles.extend([''] * (total - len(titles)))\n",
    "    for idx, (ax, img, title) in enumerate(zip(axes, pil_images, titles)):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "    for ax in axes[total:]:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def make_demo_video(num_frames: int = 4, size: int = 128) -> tv_t.Video:\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        angle = i * 10\n",
    "        frame = F.affine(base_tensor, angle=angle, translate=[0, 0], scale=1.0, shear=[0, 0])\n",
    "        frames.append(frame)\n",
    "    stacked = torch.stack(frames, dim=1)  # C, T, H, W\n",
    "    return tv_t.Video(stacked)\n",
    "\n",
    "\n",
    "def make_demo_boxes() -> tv_t.BoundingBoxes:\n",
    "    boxes = torch.tensor([[50, 50, 300, 300], [150, 120, 420, 420]], dtype=torch.float32)\n",
    "    return tv_t.BoundingBoxes(boxes, format=tv_t.BoundingBoxFormat.XYXY, canvas_size=base_tensor.shape[1:])\n",
    "\n",
    "\n",
    "def make_demo_keypoints(num_points: int = 5) -> tv_t.Keypoints:\n",
    "    xs = torch.linspace(80, 400, steps=num_points)\n",
    "    ys = torch.linspace(90, 360, steps=num_points)\n",
    "    keypoints = torch.stack([xs, ys], dim=1)\n",
    "    return tv_t.Keypoints(keypoints, format='xy')\n",
    "\n",
    "\n",
    "def describe_version():\n",
    "    print('torchvision 版本:', torchvision.__version__)\n",
    "    print('支持 transforms.v2:', hasattr(torchvision.transforms, 'v2'))\n",
    "\n",
    "describe_version()"
   ],
   "id": "6e32fd540849a13"
  },
  {
   "cell_type": "markdown",
   "id": "c47efc08",
   "metadata": {},
   "source": [
    "## Transforming images, videos, boxes and more 图像/视频/目标变换\n",
    "\n",
    "本节概览 torchvision 变换体系，从输入约定、V1/V2 差异到性能注意事项，并通过 TVTensors 演示如何在现代 API 中同时处理图像、视频、目标框、关键点与掩码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad599ef",
   "metadata": {},
   "source": [
    "### Start here 入门指南\n",
    "- **定位**：`torchvision.transforms` 提供数据增强与预处理工具，覆盖经典 `V1` API 以及推荐使用的 `V2` API。\n",
    "- **核心思路**：将输入统一为张量或 `TVTensor`，以便组合操作并共享元数据（尺寸、坐标系等）。\n",
    "- **V2 优势**：支持批处理、自动处理 `TVTensor` 元数据、与 `torch.compile` 更契合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 V2 随机组合同时处理图像、视频和边界框\n",
    "reset_seed()\n",
    "img = tv_t.Image(base_tensor.clone())\n",
    "video = make_demo_video()\n",
    "boxes = make_demo_boxes()\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0)),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "img_out, boxes_out = transform(img, boxes)\n",
    "video_out, _ = transform(video, boxes)  # 视频共享相同空间变换\n",
    "\n",
    "print('图像输出类型:', type(img_out), img_out.shape)\n",
    "print('边界框输出:', boxes_out)\n",
    "print('视频输出形状:', video_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ffba4",
   "metadata": {},
   "source": [
    "### Supported input types and conventions 输入类型与约定\n",
    "- **PIL / Tensor / TVTensor**：V1 支持 PIL 与张量，V2 在此基础上引入 `tv_tensors`，记录空间信息。\n",
    "- **尺寸约定**：图像张量采用 `C×H×W`，视频采用 `C×T×H×W`，边界框使用 `(xmin, ymin, xmax, ymax)`。\n",
    "- **批处理**：V2 支持批量维度，输入 `(B, C, H, W)` 时无需循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c24942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统一输入类型的示例\n",
    "batch = torch.stack([base_tensor, T.Resize((256, 256))(base_tensor)], dim=0)\n",
    "print('批量维度:', batch.shape)\n",
    "normalized = v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))(batch)\n",
    "print('归一化成功，数据范围:', normalized.amin().item(), normalized.amax().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0fb02",
   "metadata": {},
   "source": [
    "### V1 or V2? 该如何选择\n",
    "- **推荐**：新项目优先使用 `torchvision.transforms.v2`，享受类型安全与批处理能力。\n",
    "- **兼容**：V1 仍广泛使用，可逐步迁移；`v2.functional` 提供与 V1 对齐的函数式接口。\n",
    "- **混用策略**：通过 `v2.Compose([v2.ToImage(), v2.ToDtype(...)])` 作为入口，将旧有张量适配到 V2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e73827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 V1 Compose 迁移到 V2 的示例\n",
    "v1_pipeline = T.Compose([\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "v2_pipeline = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomResizedCrop(224),\n",
    "    v2.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "])\n",
    "\n",
    "v1_out = v1_pipeline(base_pil)\n",
    "v2_out = v2_pipeline(base_pil)\n",
    "print('V1/V2 输出差异 (L1):', (v1_out - v2_out).abs().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9e267",
   "metadata": {},
   "source": [
    "### Performance considerations 性能注意事项\n",
    "- **避免重复 PIL ↔ Tensor 转换**：V2 `ToImage`/`ToDtype` 只需调用一次。\n",
    "- **批量与并行**：结合 `torch.utils.data.DataLoader` 的 `batch_transforms` 提升吞吐。\n",
    "- **GPU/CPU 切换**：`v2` 支持 `transform = transform.to(device)` 将参数移动到 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 V2 变换部署到 CUDA（若可用）\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transform_cuda = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]).to(device)\n",
    "\n",
    "sample = base_tensor.to(device)\n",
    "output = transform_cuda(sample)\n",
    "print('输出设备:', output.device, '范围:', (output.min().item(), output.max().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02c646",
   "metadata": {},
   "source": [
    "### Transform classes, functionals, and kernels 变换类型结构\n",
    "- **Classes**：`v2.RandomCrop` 等面向对象接口，可在 `Compose` 中复用。\n",
    "- **Functionals**：`v2.functional` 对应纯函数版本，适合自定义逻辑复用。\n",
    "- **Kernels**：底层 C++/CUDA 算子实现，高性能且复用于 `torchvision.ops`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abd8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F_v1\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "# V1/V2 函数式调用对比\n",
    "rotated_v1 = F_v1.rotate(base_pil, angle=30, interpolation=InterpolationMode.BILINEAR)\n",
    "rotated_v2 = v2.functional.rotate(base_tensor, angle=30)\n",
    "show_images([rotated_v1, rotated_v2], [\"V1 rotate\", \"V2 rotate\"], cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a7964",
   "metadata": {},
   "source": [
    "### TorchScript support TorchScript 支持\n",
    "- V2 变换遵循纯函数式设计，可通过 `torch.jit.script`。\n",
    "- 自定义变换需将随机数使用 `torch.rand` 等 TorchScript 兼容 API。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8969d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScriptableRandomInvert(v2.Transform):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, image):\n",
    "        if torch.rand(1).item() < self.p:\n",
    "            return 1.0 - image\n",
    "        return image\n",
    "\n",
    "script_transform = torch.jit.script(ScriptableRandomInvert())\n",
    "print('TorchScript OK:', script_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4caa3",
   "metadata": {},
   "source": [
    "### V2 API reference - Recommended 推荐文档导航\n",
    "- **核心入口**：`torchvision.transforms.v2` 模块文档覆盖类、函数式 API 与 `auto_augment` 策略。\n",
    "- **补充**：`torchvision.tv_tensors` 文档描述各类张量包装及坐标格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf57b8fa",
   "metadata": {},
   "source": [
    "### V1 API Reference 兼容文档提醒\n",
    "- V1 文档仍在：`torchvision.transforms`。\n",
    "- 查看历史项目时注意 `transforms.ToTensor`、`RandomCrop` 等默认行为差异（例如整型 -> float 的归一化）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d137c3",
   "metadata": {},
   "source": [
    "### TVTensors 生态\n",
    "- **Image / Video**：携带尺寸信息，自动随变换更新。\n",
    "- **KeyPoints**：支持 `xy`/`xyz` 格式，常用于姿态估计。\n",
    "- **BoundingBoxes**：封装 `format`、`canvas_size`，简化坐标变换。\n",
    "- **Mask**：继承 `TVTensor`，便于语义/实例分割掩码同步变换。\n",
    "- **set_return_type / wrap**：控制自定义函数返回类型，保持 TVTensor 元信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58acda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TVTensor 组合示例\n",
    "img = tv_t.Image(base_tensor.clone())\n",
    "boxes = make_demo_boxes()\n",
    "mask = tv_t.Mask(torch.zeros_like(base_tensor[0], dtype=torch.bool))\n",
    "mask[..., 120:360, 160:400] = True\n",
    "keypoints = make_demo_keypoints()\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.RandomRotation(degrees=(-20, 20)),\n",
    "    v2.RandomHorizontalFlip(p=1.0),\n",
    "])\n",
    "\n",
    "img_out, boxes_out, mask_out, keypoints_out = transform(img, boxes, mask, keypoints)\n",
    "print('TVTensor 类型:', type(img_out), type(boxes_out), type(mask_out), type(keypoints_out))\n",
    "print('关键点示例:', keypoints_out[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0115a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 set_return_type/wrap 创建自定义函数保持 TVTensor\n",
    "@tv_t.wrap()\n",
    "def brighten(image: tv_t.Image, delta: float = 0.1):\n",
    "    return (image.float() + delta).clamp(0, 1)\n",
    "\n",
    "bright_img = brighten(tv_t.Image(base_tensor), 0.2)\n",
    "print('保持类型:', type(bright_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36cbdc3",
   "metadata": {},
   "source": [
    "## Models and pre-trained weights 模型与预训练权重\n",
    "\n",
    "本节总结 torchvision 模型库的加载方式与常见任务覆盖范围，并给出处理权重元数据、类别映射、推理与特征抽取的示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a780ea",
   "metadata": {},
   "source": [
    "### General information on pre-trained weights 通用信息\n",
    "- 预训练权重通过 `WeightsEnum` 管理，访问 `weights.meta` 获取类别、输入归一化等元数据。\n",
    "- 初次加载权重需要网络，离线环境可使用 `weights.get_state_dict(progress=False)` 结合本地缓存。\n",
    "- 建议在推理/评估阶段使用 `model.eval()` 并关闭梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "print('类别数:', len(weights.meta['categories']))\n",
    "print('推荐输入归一化:', weights.meta['mean'], weights.meta['std'])\n",
    "\n",
    "try:\n",
    "    model = resnet18(weights=weights)\n",
    "    print('成功加载预训练权重。')\n",
    "except Exception as err:\n",
    "    print('无法下载预训练权重，回退到随机初始化:', err)\n",
    "    model = resnet18(weights=None)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b7f91",
   "metadata": {},
   "source": [
    "### Classification 图像分类\n",
    "- `torchvision.models` 提供 ResNet、ConvNeXt、ViT 等架构。\n",
    "- 推理流程：加载权重 → 预处理 → 前向计算 → softmax。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34088726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.Resize(ResNet18_Weights.DEFAULT.transforms().crop_size),\n",
    "    v2.CenterCrop(ResNet18_Weights.DEFAULT.transforms().crop_size),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=ResNet18_Weights.DEFAULT.meta['mean'], std=ResNet18_Weights.DEFAULT.meta['std'])\n",
    "])\n",
    "\n",
    "sample = transform(base_pil).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    logits = model(sample)\n",
    "    probs = logits.softmax(dim=1)\n",
    "    top5_prob, top5_catid = probs.topk(5)\n",
    "\n",
    "for prob, catid in zip(top5_prob[0], top5_catid[0]):\n",
    "    label = ResNet18_Weights.DEFAULT.meta['categories'][catid]\n",
    "    print(f'{label}: {prob.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bcd8ee",
   "metadata": {},
   "source": [
    "### Semantic Segmentation 语义分割\n",
    "- `torchvision.models.segmentation` 包含 DeepLabV3/FCN 等。\n",
    "- 输出为 `dict`，需插值回原尺寸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = segmentation.deeplabv3_resnet50(weights=None).eval()\n",
    "with torch.no_grad():\n",
    "    seg_out = seg_model(base_tensor.unsqueeze(0))['out']\n",
    "print('语义分割输出形状:', seg_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ada92f",
   "metadata": {},
   "source": [
    "### Object Detection, Instance Segmentation and Person Keypoint Detection 目标检测/实例分割/关键点\n",
    "- `torchvision.models.detection` 提供 Faster R-CNN、Mask R-CNN、Keypoint R-CNN。\n",
    "- 输入张量需为列表，输出同样是字典列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_model = detection.fasterrcnn_resnet50_fpn(weights=None).eval()\n",
    "with torch.no_grad():\n",
    "    det_out = det_model([base_tensor])\n",
    "print('检测输出 keys:', det_out[0].keys())\n",
    "print('预测框数量:', det_out[0]['boxes'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822cd51f",
   "metadata": {},
   "source": [
    "### Video Classification 视频分类\n",
    "- `torchvision.models.video` 提供 R(2+1)D、MC3 等模型。\n",
    "- 输入张量形状 `(B, C, T, H, W)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dcd883",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_model = video_models.r2plus1d_18(weights=None).eval()\n",
    "video_clip = make_demo_video(num_frames=8, size=112).unsqueeze(0)  # B, C, T, H, W\n",
    "with torch.no_grad():\n",
    "    video_logits = video_model(video_clip)\n",
    "print('视频分类输出维度:', video_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefc9bb",
   "metadata": {},
   "source": [
    "### Optical Flow 光流估计\n",
    "- `torchvision.models.optical_flow` 提供 RAFT 系列。\n",
    "- 输入两个连续帧，输出流场 `(B, 2, H, W)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43cc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_model = optical_flow.raft_small(weights=None).eval()\n",
    "pair = base_tensor.unsqueeze(0).repeat(2, 1, 1, 1)  # 两帧相同\n",
    "with torch.no_grad():\n",
    "    flow = flow_model(pair[0].unsqueeze(0), pair[1].unsqueeze(0))[0]\n",
    "print('光流张量形状:', flow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0fb4b7",
   "metadata": {},
   "source": [
    "## Datasets 数据集\n",
    "\n",
    "- **Built-in datasets**：`torchvision.datasets` 提供 ImageNet、COCO、CIFAR 等。\n",
    "- **Base classes for custom datasets**：`VisionDataset`、`ImageFolder`、`DatasetFolder` 简化自定义加载。\n",
    "- **Transforms v2**：在 `datasets` 中通过 `transform=` 与 `target_transform=` 传入 V2 Pipeline。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 ImageFolder 读取本地 arch_linux 图像\n",
    "folder_dataset = datasets.ImageFolder(root='dataset', transform=v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)]))\n",
    "print('ImageFolder 类别:', folder_dataset.classes)\n",
    "print('样本数量:', len(folder_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c90d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 FakeData 构造训练样例，并演示 batch_transforms\n",
    "train_transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "])\n",
    "\n",
    "fake_dataset = datasets.FakeData(size=4, image_size=(3, 224, 224), num_classes=10, transform=train_transforms)\n",
    "images, labels = zip(*[fake_dataset[i] for i in range(4)])\n",
    "show_images(images, [f'标签 {label}' for label in labels], cols=4, figsize=(12, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a36fd5",
   "metadata": {},
   "source": [
    "## Utils 可视化与工具集\n",
    "\n",
    "- `draw_bounding_boxes`、`draw_segmentation_masks`、`draw_keypoints`：用于快速调试目标任务。\n",
    "- `flow_to_image`：将光流 `(2, H, W)` 转换为 RGB 可视化。\n",
    "- `make_grid`、`save_image`：导出批量图像、保存结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = make_demo_boxes()\n",
    "img_uint8 = (base_tensor * 255).to(torch.uint8)\n",
    "annotated = draw_bounding_boxes(img_uint8, boxes.as_subclass(torch.Tensor), colors=['red', 'blue'], labels=['objectA', 'objectB'])\n",
    "keypoints = make_demo_keypoints()\n",
    "annotated_kp = draw_keypoints(annotated, keypoints.as_subclass(torch.Tensor).unsqueeze(0), colors='lime', radius=4)\n",
    "show_images([img_uint8, annotated_kp], ['原始', '框+关键点'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros_like(base_uint8[0], dtype=torch.bool)\n",
    "mask[120:360, 200:440] = True\n",
    "mask_img = draw_segmentation_masks(base_uint8, mask.unsqueeze(0), colors=['magenta'], alpha=0.6)\n",
    "show_images([mask_img], ['分割掩码叠加'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 光流可视化\n",
    "toy_flow = torch.stack(torch.meshgrid(\n",
    "    torch.linspace(-1, 1, steps=base_tensor.shape[1]),\n",
    "    torch.linspace(-1, 1, steps=base_tensor.shape[2]),\n",
    "    indexing='ij'\n",
    "), dim=0)\n",
    "flow_vis = flow_to_image(toy_flow)\n",
    "show_images([flow_vis], ['flow_to_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ff99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_grid + save_image 示例\n",
    "batch = torch.stack([base_tensor, torch.flip(base_tensor, dims=[2])])\n",
    "grid = make_grid(batch, nrow=2)\n",
    "output_path = Path('souce_codeExplain/Transforms/output_grid.png')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "save_image(batch, output_path)\n",
    "show_images([grid], ['make_grid 预览'])\n",
    "print('保存路径:', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557d16b",
   "metadata": {},
   "source": [
    "## Operators 算子库\n",
    "\n",
    "- **Detection/Segmentation Operators**：`torchvision.ops` 提供 NMS、RoIAlign、DeformConv2d 等。\n",
    "- **Box Operators**：如 `box_convert`、`generalized_box_iou`。\n",
    "- **Losses**：`sigmoid_focal_loss`、`complete_box_iou_loss`。\n",
    "- **Layers**：`MLP`、`DeformConv2d`、`StochasticDepth`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d3d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_convert, generalized_box_iou, sigmoid_focal_loss, DeformConv2d\n",
    "\n",
    "boxes_xyxy = torch.tensor([[50, 80, 200, 220], [120, 140, 260, 280]], dtype=torch.float32)\n",
    "boxes_cxcywh = box_convert(boxes_xyxy, in_fmt='xyxy', out_fmt='cxcywh')\n",
    "print('中心宽高格式:', boxes_cxcywh)\n",
    "\n",
    "giou = generalized_box_iou(boxes_xyxy, boxes_xyxy)\n",
    "print('广义 IoU:', giou)\n",
    "\n",
    "logits = torch.randn(2, requires_grad=True)\n",
    "target = torch.tensor([1.0, 0.0])\n",
    "loss = sigmoid_focal_loss(logits, target)\n",
    "print('Focal Loss:', loss.item())\n",
    "\n",
    "layer = DeformConv2d(3, 8, kernel_size=3, padding=1)\n",
    "input_feat = torch.rand(1, 3, 32, 32)\n",
    "offset = torch.zeros(1, 18, 32, 32)  # 2*k*k*C_{out}\n",
    "out_feat = layer(input_feat, offset)\n",
    "print('DeformConv 输出形状:', out_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de0c658",
   "metadata": {},
   "source": [
    "## Decoding / Encoding images and videos 图像/视频编解码\n",
    "- `torchvision.io.decode_image`/`encode_jpeg`：直接在张量级别处理压缩数据。\n",
    "- `torchvision.io.read_image`：读取为 `uint8` 张量，支持灰度/颜色模式。\n",
    "- `torchvision.io.decode_video`：读取视频帧（注意旧 API 标记为 deprecated）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb9942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bytes = SAMPLE_IMAGE_PATH.read_bytes()\n",
    "decoded = decode_image(torch.frombuffer(image_bytes, dtype=torch.uint8))\n",
    "print('decode_image 输出:', decoded.shape, decoded.dtype)\n",
    "\n",
    "encoded = encode_jpeg(decoded, quality=70)\n",
    "print('重新编码 JPEG 大小:', encoded.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0cdc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = read_image(str(SAMPLE_IMAGE_PATH))\n",
    "print('read_image dtype:', loaded.dtype, 'range:', (loaded.min().item(), loaded.max().item()))\n",
    "write_path = Path('souce_codeExplain/Transforms/reencoded.png')\n",
    "write_png(loaded, write_path)\n",
    "print('PNG 已写入:', write_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f15187",
   "metadata": {},
   "source": [
    "## IO operations 输入输出\n",
    "- `torchvision.io` 提供读取/写入图像与视频的底层接口。\n",
    "- 结合 `torch.utils.data.DataLoader` 可构建流式加载流水线。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af964253",
   "metadata": {},
   "source": [
    "## Video - DEPRECATED 视频旧接口\n",
    "- 旧版 `torchvision.io.VideoClips` 与 `read_video` 部分接口标记为 deprecated。\n",
    "- 建议迁移到新的视频 transforms 与 `torchvision.prototype`（若可用）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b4bca",
   "metadata": {},
   "source": [
    "## Feature extraction for model inspection 模型特征提取\n",
    "- 通过 `torchvision.models.feature_extraction.create_feature_extractor` 获取中间层输出。\n",
    "- 可用于可视化、蒸馏或特征分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_feature = resnet18(weights=None).eval()\n",
    "return_nodes = {\n",
    "    'layer1.1.relu': 'low_level',\n",
    "    'layer4.1.relu': 'high_level',\n",
    "}\n",
    "feature_extractor = feature_extraction.create_feature_extractor(model_for_feature, return_nodes=return_nodes)\n",
    "features = feature_extractor(base_tensor.unsqueeze(0))\n",
    "for name, feat in features.items():\n",
    "    print(name, feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ce067",
   "metadata": {},
   "source": [
    "## API Reference 文档速查\n",
    "- 官方文档入口：https://pytorch.org/vision/stable/\n",
    "- 推荐结合 `help(torchvision.module)` 快速查看函数签名与 docstring。\n",
    "- 保持本地/离线笔记，记录项目使用到的具体 API 组合与参数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
