{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde5bfd4",
   "metadata": {},
   "source": [
    "# TorchRL 学习笔记\n",
    "\n",
    "> 汇总 TorchRL 的核心概念、模块结构、典型训练流程及关键 API，帮助快速在 PyTorch 生态中构建强化学习（RL）实验。TorchRL 是 Meta AI 开源的 RL 库，提供环境封装、数据管线、策略训练、推理与部署组件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afea016",
   "metadata": {},
   "source": [
    "## TorchRL 概览\n",
    "- **定位**：PyTorch 原生的 RL 库，覆盖环境抽象、数据收集、策略学习、评估与部署。\n",
    "- **设计目标**：\n",
    "  - 模块化：环境、策略、数据管线等组件可组合使用。\n",
    "  - 可扩展：支持多 GPU、多环境并行收集、分布式训练。\n",
    "  - 实用性：内置常见算法与工具，兼容 Gymnasium/DeepMind 控件等生态。\n",
    "- **核心组件**：\n",
    "  - 环境接口 (`EnvBase`, `TransformedEnv`)\n",
    "  - 数据迭代 (`TensorDict`, `TensorDictReplayBuffer`)\n",
    "  - 策略/模块 (`rl.modules`)\n",
    "  - 训练循环 (`rl.trainers`, `collectors`)\n",
    "  - 评估与部署工具 (`RLTrainer`, `policy.eval`, TorchScript/ONNX 导出)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7212f9",
   "metadata": {},
   "source": [
    "## 安装与检查\n",
    "- **依赖**：PyTorch >= 2.1（建议最新版）、torchrl、gymnasium（或其它环境库）。\n",
    "- **安装示例**：\n",
    "  ```bash\n",
    "  pip install torchrl\n",
    "  pip install gymnasium\n",
    "  pip install gymnasium[classic-control]\n",
    "  ```\n",
    "- TorchRL npm 包发布频率较快，建议关注 <https://pytorch.org/rl/> 的安装说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 TorchRL 安装状态\n",
    "try:\n",
    "    import torch, torchrl\n",
    "    from torchrl.envs import GymnasiumEnv\n",
    "    print('PyTorch 版本:', torch.__version__)\n",
    "    print('TorchRL 版本:', torchrl.__version__)\n",
    "    env = GymnasiumEnv('CartPole-v1', device='cpu')\n",
    "    td = env.reset()\n",
    "    print('环境初始 observation keys:', td.keys())\n",
    "except Exception as err:\n",
    "    print('TorchRL 或相关依赖未正常安装 ->', err)\n",
    "    print('请按文档安装 torchrl / gymnasium 等依赖')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715076f9",
   "metadata": {},
   "source": [
    "## 核心概念\n",
    "### TensorDict\n",
    "- TorchRL 以 `TensorDict` 作为数据容器，统一存放观测、动作、奖励等字段。\n",
    "- 支持批处理、并行维度、设备迁移，与 PyTorch Tensor 操作高度兼容。\n",
    "- 常用方法：`td.get(...)`、`td.set(...)`、`td.to(device)`、`td.batch_size`。\n",
    "\n",
    "### 环境 (EnvBase, TransformedEnv)\n",
    "- `EnvBase`：统一步进接口，返回 TensorDict。\n",
    "- `GymnasiumEnv`, `DMEnvWrapper`：对第三方环境的封装。\n",
    "- `TransformedEnv`：叠加一系列 `Transform`（归一化、裁剪、奖励重塑等），便于预处理。\n",
    "\n",
    "### 策略与模块\n",
    "- `rl.modules`：提供 `Actor`, `ValueOperator`, `DistributionalActor` 等模块化结构。\n",
    "- 支持与 `nn.Module` 集成，实现自定义策略网络。\n",
    "\n",
    "### 数据收集与缓冲\n",
    "- `Collectors`：如 `SyncDataCollector`, `MultiaSyncDataCollector` 并行样本采集。\n",
    "- `ReplayBuffer`：`TensorDictReplayBuffer`、`LazyTensorStorage` 等，用于经验回放。\n",
    "\n",
    "### 训练器 (Trainers)\n",
    "- `RLTrainer`：组织收集、优化、评估流程。\n",
    "- 内置多种常用算法：DQN、A2C、PPO、SAC、TD3 等，位于 `torchrl.trainers.helpers`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20726416",
   "metadata": {},
   "source": [
    "## 快速入门示例：DQN 训练 CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a16059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torchrl.envs import GymnasiumEnv, TransformedEnv, DoubleToFloat\n",
    "from torchrl.data.replay_buffers import TensorDictReplayBuffer, LazyTensorStorage\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.modules import QValueModule, EGreedyModule\n",
    "from torchrl.objectives import DQNLoss\n",
    "from torchrl.objectives.value import DQNLossDefaultActorCritic\n",
    "\n",
    "# 环境定义\n",
    "env = TransformedEnv(\n",
    "    GymnasiumEnv('CartPole-v1', device='cpu'),\n",
    "    DoubleToFloat(),\n",
    ")\n",
    "td = env.reset()\n",
    "obs_size = td.observation.shape[-1]\n",
    "num_actions = env.action_spec.space.n\n",
    "\n",
    "# Q 网络\n",
    "def make_qnet():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(obs_size, 128), nn.ReLU(),\n",
    "        nn.Linear(128, 128), nn.ReLU(),\n",
    "        nn.Linear(128, num_actions)\n",
    "    )\n",
    "\n",
    "qnet = QValueModule(make_qnet())\n",
    "policy = EGreedyModule(qnet, eps_init=0.1)\n",
    "\n",
    "# 经验回放\n",
    "buffer = TensorDictReplayBuffer(\n",
    "    storage=LazyTensorStorage(10000),\n",
    "    batch_size=128,\n",
    ")\n",
    "\n",
    "# 收集器\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    frames_per_batch=200,\n",
    "    total_frames=10_000,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "optimizer = Adam(qnet.parameters(), lr=3e-4)\n",
    "loss_module = DQNLoss(\n",
    "    value_network=qnet,\n",
    "    value_type='qvalue',\n",
    "    double_dqn=True,\n",
    "    gamma=0.99\n",
    ")\n",
    "\n",
    "for i, data in enumerate(collector):\n",
    "    buffer.extend(data)\n",
    "    for _ in range(10):\n",
    "        sample = buffer.sample()\n",
    "        loss = loss_module(sample)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if i % 5 == 0:\n",
    "        print(f'Batch {i}, loss = {loss.item():.4f}')\n",
    "\n",
    "print('训练完成，测试策略...')\n",
    "with torch.no_grad():\n",
    "    td = env.reset()\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = policy(td).get('action')\n",
    "        td = env.step(action)\n",
    "        reward = td.get('reward').item()\n",
    "        total_reward += reward\n",
    "        done = td.get('done').item()\n",
    "    print('测试回合总奖励:', total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0ef90",
   "metadata": {},
   "source": [
    "## 常见算法与模块\n",
    "- **值函数类**：`DQNLoss`, `TD0Loss`, `A2CLoss`, `PPOLoss`, `SACLoss`, `REDQLoss` 等。\n",
    "- **策略分布**：`GaussianActor`, `TanhNormal`, `CategoricalActor`。\n",
    "- **辅助工具**：\n",
    "  - `torchrl.record.loggers`：TensorBoard、CSV、W&B 等日志。\n",
    "  - `torchrl.trainers.helpers`：封装了训练构建流程（环境、策略、损失、日志器等）。\n",
    "  - `rl.utils`：seed 控制、探索策略、TDError 计算等。\n",
    "\n",
    "常用模块结构：\n",
    "```\n",
    "torchrl\n",
    " ├── data             # TensorDict、ReplayBuffer\n",
    " ├── envs             # 环境与 Transform\n",
    " ├── modules          # 策略/价值模块\n",
    " ├── objectives       # 损失函数\n",
    " ├── collectors       # 数据采集器\n",
    " ├── trainers         # 训练器及 helper\n",
    " └── record           # 日志与分析\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31b4349",
   "metadata": {},
   "source": [
    "## 高级主题\n",
    "- **多环境并行**：`ParallelEnv`, `SerialEnv`, `MultiprocessingEnv`\n",
    "- **分布式训练**：结合 `torch.distributed`，通过 `MultiaSyncDataCollector` 等实现分布式采集/训练。\n",
    "- **离线 RL**：支持从离线数据加载 `TensorDict`，结合 `ReplayBuffer` 进行训练。\n",
    "- **安全与可解释**：利用 `Transforms` 对奖励、动作空间进行裁剪或约束。\n",
    "- **部署**：策略可导出为 TorchScript/ONNX，用于 C++/移动端部署。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba664ed",
   "metadata": {},
   "source": [
    "## 参考资料\n",
    "- 官方文档：<https://pytorch.org/rl/>\n",
    "- GitHub 仓库：<https://github.com/pytorch/rl>\n",
    "- 教程与示例：文档中 “Tutorials” 与 “Examples” 栏目。\n",
    "- 论文参考：TorchRL 白皮书、Meta AI 强化学习相关研究。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}