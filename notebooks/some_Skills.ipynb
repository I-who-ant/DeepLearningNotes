{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 执行reshape操作\n",
    "\n",
    "例如 , reshaped = torch.reshape(input_tensor, (-1, 1, 2, 2))\n",
    "\n",
    "        (-1, 1, 2, 2) 表示：\n",
    "\n",
    "        -1：自动计算批量大小\n",
    "\n",
    "        1：通道数为1（单通道）\n",
    "\n",
    "        2：高度为2\n",
    "\n",
    "        2：宽度为2\n",
    "\n",
    "        场景 :\n",
    "            1. 转换为CNN可接受的格式（批量×通道×高度×宽度）\n",
    "            2.转换为批量处理格式 （批量×特征）\n",
    "                    # 如果有多个2×2的图像块\n",
    "                    patches = [\n",
    "                        [[1, 2], [-1, 3]],    # 第一个patch\n",
    "                        [[4, 5], [6, 7]],     # 第二个patch\n",
    "                        [[8, 9], [10, 11]]    # 第三个patch\n",
    "                    ]\n",
    "\n",
    "                #意思是3个2×2的图像块，每个图像块有1个通道\n",
    "                batch_patches = torch.reshape(torch.tensor(patches), (-1, 1, 2, 2))\n",
    "                print(\"批量patch形状:\", batch_patches.shape)  # torch.Size([3, 1, 2, 2])\n",
    "\n",
    "            3.  ReLU激活函数\n",
    "\n",
    "\n"
   ],
   "id": "394733b341c8ac5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "例如 , reshaped = torch.reshape(input_tensor, (1, 1, 1, -1))\n",
    "\n",
    "        (1, 1, 1, -1) 表示：\n",
    "\n",
    "        1：批量大小固定为 1（单样本输入）\n",
    "\n",
    "        1：通道数为 1，方便复用卷积/归一化等层\n",
    "\n",
    "        1：高度设为 1，仅占位无需真实二维结构\n",
    "\n",
    "        -1：让 PyTorch 自动推断剩余长度作为宽度，保持元素总数不变\n",
    "        (也就是说 , 把向量展平成 1×1×1×n 的形式),\n",
    "        (-1替换为别的数字, 就是指 一行有 n 个元素)\n",
    "\n",
    "        场景 :\n",
    "            1. 将任意向量快速包装成 4 维形状（批量×通道×高度×宽度），复用现有 CNN/PyTorch API。\n",
    "            2. 临时对齐模型接口：某些层要求输入至少四维，可用 1 作为占位维度。\n",
    "            3. 调试或可视化时，把长向量“铺平”到最后一维，便于打印或绘图。\n",
    "\n",
    "        示例 :\n",
    "            import torch\n",
    "\n",
    "            input_tensor = torch.arange(6)# 生成一个包含6个元素的向量\n",
    "\n",
    "            reshaped = torch.reshape(input_tensor, (1, 1, 1, -1)) # 和flatten()效果相同\n",
    "            print('原始形状:', input_tensor.shape)  # torch.Size([6])\n",
    "            print('新形状:', reshaped.shape)       # torch.Size([1, 1, 1, 6]),化为1×1×1×6的4维张量\n",
    "            print('数据保持不变:', reshaped)\n",
    "\n",
    "        理解:\n",
    "              - 如果你的下一层是线性层或任何接受二维 (batch, features) 输入的模块，直接 flatten()\n",
    "                就行；再 reshape(1, 1, 1, -1) 只是多绕了一圈。\n",
    "              - 只有当下一层依赖 4 维输入（如你想临时把向量喂给卷积层试验）时，才会用 (1, 1, 1, -1)\n",
    "                这种“伪”4D 包装。\n",
    "\n",
    "              总结：常规分类流程里 flatten() 已满足需求，无需再 reshape(1, 1, 1, -1)；两者都能保留\n",
    "              元素顺序，但面向的下游接口不同。\n",
    "\n",
    "        例子 :\n",
    "                  如果把规模缩小到 Linear(4, 2) 会更直观：\n",
    "\n",
    "                  权重 W =\n",
    "                  [[w_11, w_12, w_13, w_14],   ← 输出神经元 1 的 4 个权重\n",
    "                   [w_21, w_22, w_23, w_24]]   ← 输出神经元 2 的 4 个权重\n",
    "\n",
    "                  输入向量 x = [x1, x2, x3, x4]，那么：\n",
    "\n",
    "                  logit_1 = w_11*x1 + w_12*x2 + w_13*x3 + w_14*x4 + b_1\n",
    "                  logit_2 = w_21*x1 + w_22*x2 + w_23*x3 + w_24*x4 + b_2\n",
    "\n",
    "                  . Linear(4096, 512) 做了什么？\n",
    "\n",
    "                  - 这层本质是一次矩阵乘法：y = W_hidden x + b_hidden。\n",
    "                  - W_hidden 的形状是 512 × 4096，也就是 512 行、4096 列。\n",
    "                  - b_hidden 是长度 512 的偏置向量。\n",
    "        于是 4096 维向量被“压缩/组合”成 512 维新的特征向量 y。这一层允许网络学到比卷积层更灵活、更高阶的特征组合"
   ],
   "id": "819e2974401b89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T13:43:45.249538Z",
     "start_time": "2025-10-23T13:43:45.244513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 理解argmax函数\n",
    "import torch\n",
    "# 示例张量\n",
    "logits = torch.tensor([[1.0, 2.0, 3.0],  # 第一个样本的 logit 向量\n",
    "                        [2.0, 1.0, 3.0]]) # 第二个样本的 logit 向量\n",
    "\n",
    "\n",
    "# 对每个样本的 logit 向量应用 argmax\n",
    "#  dim=1 表示在每个样本的 logit 向量上应用 argmax，得到每个样本的预测类别\n",
    "# 解释 :\n",
    "#  第一个样本的 logit 向量为 [1.0, 2.0, 3.0]，最大元素是 3.0，对应索引 2。\n",
    "#  第二个样本的 logit 向量为 [2.0, 1.0, 3.0]，最大元素是 3.0，对应索引 2。\n",
    "#  所以，argmax 函数返回的预测类别为 [2, 2]，表示第一个样本预测为类别 2，第二个样本也预测为类别 2。\n",
    "#\n",
    "#  dim=0 表示在每个特征维度上应用 argmax，得到每个特征的最大元素索引\n",
    "# 解释 :\n",
    "#  第一个样本的 logit 向量为 [1.0, 2.0, 3.0]，最大元素是 3.0，对应索引 2。\n",
    "#  第二个样本的 logit 向量为 [2.0, 1.0, 3.0]，最大元素是 3.0，对应索引 2。\n",
    "#  所以，argmax 函数返回的预测类别为 [2, 2]，表示第一个样本预测为类别 2，第二个样本也预测为类别 2。\n",
    "predicted_classes = torch.argmax(logits, dim=1)\n",
    "print(\"预测类别:\", predicted_classes)"
   ],
   "id": "8ab3ad8de6399bc8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "86b4e7cb1ba0925"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "69e5364cf840d593"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
