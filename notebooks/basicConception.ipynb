{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "### 通道理解\n",
    "\n",
    "  - 对输入图像：RGB 彩图就是典型例子，每个像素有红、绿、蓝三个数值，对应三个输入通道；灰度图则只有一个通道。\n",
    "  - 对中间特征图：某一层输出往往包含很多特征图，每个特征图就是一个通道，用来表示模型从不同角度提取的局部模式（边缘、纹理、形状等）。\n",
    "\n",
    "  换句话说，通道是“在同样的高×宽坐标上，堆叠的独立信息层”。卷积核在处理多通道输入时，会为每个通道准备一套 kH×kW 权重，\n",
    "  并把所有通道的局部结果加总，生成一个输出通道；输出通道数由我们配置，代表想提取的特征类型数量。"
   ],
   "id": "a9ba59b97472bdcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 张量形状与通道的关系\n",
    "形状 (C, H, W) 表示：通道数 × 高度 × 宽度\n",
    "三个通道共享同一个张量，不是三个独立的张量\n",
    "所有通道的形状相同（都是 H×W），但值不同\n",
    "通道是张量的第一个维度切片：tensor[0]=红色通道，tensor[1]=绿色通道，tensor[2]=蓝色通道\n",
    "\n",
    "\n"
   ],
   "id": "415c171a29c214fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 图像格式转换的可逆性\n",
    "PIL.Image → NumPy数组\n",
    "→ PyTorch张量 → NumPy数组 → PIL.Image\n",
    "\n",
    "PIL.Image ↔ NumPy数组：直接的数据转换\n",
    "NumPy数组 ↔ PyTorch张量：内存共享的高效转换\n",
    "整个过程是可逆的：图像可以无损（或接近无损）地来回转换"
   ],
   "id": "a556e3f9f5fa01f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 核心理解要点：\n",
    "形状相同，值不同 - 通道关系\n",
    "归一化提高稳定性 - 数据预处理\n",
    "格式转换可逆 - 图像与张量的双向转换\n",
    "张量=图像数据 - 深度学习的基础\n"
   ],
   "id": "5a359002b3e59685"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 卷积神经网络中通道数\n",
    "\n",
    "-------------------------------------\n",
    "> 输入通道 (in_channels)\n",
    "\n",
    "由数据决定：RGB图像=3，灰度图=1\n",
    "不可改变：必须匹配输入数据的通道数\n",
    "示例：in_channels=3（RGB图像）\n",
    "\n",
    "-------------------------------------\n",
    "> 输出通道 (out_channels)\n",
    "\n",
    "设计选择：人为设定的超参数\n",
    "代表特征数量：每个通道学习一种特征模式\n",
    "示例：out_channels=64（学习64种特征）\n",
    "\n",
    "----------------------------------\n",
    "> 通道数变化的意义\n",
    "        (输入: (batch, 3, H, W) → 卷积层 → 输出: (batch, 64, H', W')\n",
    "- 增加输出通道数：模型学习更多特征，表达能力更强\n",
    "- 减少输出通道数：模型学习 fewer features，表达能力更弱\n",
    "- 保持通道数不变：模型不学习新特征，仅对输入进行简单变换\n",
    "\n",
    "\n",
    "----------------------------------\n",
    "> 通道数变化的意义\n",
    "\n",
    "任务复杂度：复杂任务需要更多特征通道\n",
    "计算资源：更多通道=更多计算和内存\n",
    "过拟合风险：通道太多可能过拟合训练数据\n",
    "经验法则：通常使用2的幂次（16,32,64,128...）\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "\n"
   ],
   "id": "2feeb37c8124958"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 卷积层输出尺寸的完整公式\n",
    "\n",
    "H_out = ⌊(H + 2p - kH) / s⌋ + 1\n",
    "W_out = ⌊(W + 2p - kW) / s⌋ + 1\n",
    "\n",
    "> 其中：\n",
    "\n",
    "> H_out = 输出高度\n",
    "\n",
    "> W_out = 输出宽度\n",
    "\n",
    "> H = 输入高度\n",
    "\n",
    "> W = 输入宽度\n",
    "\n",
    "> kH = 卷积核高度\n",
    "\n",
    "> kW = 卷积核宽度\n",
    "\n",
    "> p = 填充（padding）\n",
    "\n",
    "> s = 步幅（stride）\n",
    "\n",
    "> ⌊x⌋ 表示向下取整"
   ],
   "id": "bb306871f65a1dc2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 池化层的基本概念\n",
    "池化层（Pooling Layer）是卷积神经网络（CNN）中常用的一种层类型，主要用于特征压缩和降维，同时保留重要特征。\n",
    "池化层的基本概念包括：\n",
    "\n",
    "1. 池化核（Pooling Kernel）：与卷积核类似，池化核在输入特征图上滑动，对每个位置的小区域进行操作。\n",
    "2. 操作方式：常用的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）。\n",
    "\n",
    "Max Pooling：在每个小区域内取最大值，保留最强特征。\n",
    "Average Pooling：在每个小区域内取平均值，平滑特征。\n",
    "\n",
    "\n",
    "> 池化层的\"跳跃\"机制 :\n",
    "--------------------------------------------\n",
    "池化层的设计理念是信息压缩，而不是信息重复利用：\n",
    "\n",
    "每个区域只取一个代表值（最大值或平均值）\n",
    "步长决定了跳跃距离\n",
    "目的是降维，不是保留所有细节\n",
    "\n"
   ],
   "id": "ac3efdea9b0b21a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "(后续卷积层正是处理这份压缩特征：它们在更低分辨率上继续学习更抽象的模式，既节省计算，又扩大感受视野。",
   "id": "201a74fe199a6ab1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "## 池化层输出尺寸的完整公式\n",
    "\n",
    "H_out = ⌊(H - kH) / s⌋ + 1\n",
    "W_out = ⌊(W - kW) / s⌋ + 1\n",
    "\n",
    "> 其中：\n",
    "\n",
    "> H_out = 输出高度\n",
    "\n",
    "> W_out = 输出宽度\n",
    "\n",
    "> H = 输入高度\n",
    "\n",
    "> W = 输入宽度\n",
    "\n",
    "> kH = 池化核高度\n",
    "\n",
    "> kW = 池化核宽度\n",
    "\n",
    "> s = 步幅（stride）\n",
    "\n",
    "> ⌊x⌋ 表示向下取整\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "81f20c6095a73597"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8902acbef23af4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 非线性激活层\n",
    "(在神经网络中打破简单的线性映射，赋予模型拟合复杂函数的能力)\n",
    "\n",
    "具体来说：\n",
    "\n",
    "  - 引入非线性表达：若仅堆叠线性层，整体仍是线性变换，只能拟合\n",
    "    超平面。激活函数在每层的加权和后施加非线性，使网络能逼近任\n",
    "    意复杂的非线性关系。\n",
    "  - 分段特征提取：ReLU、LeakyReLU、GELU 等在不同输入区间呈现不\n",
    "    同响应，帮助网络学习多样化的特征模式。\n",
    "  - 梯度传播调节：某些激活（如Sigmoid、Tanh）在早期被用来平衡梯\n",
    "    度，现代网络常选ReLU类以减轻梯度消失并加速收敛。\n",
    "  - 概率或门控解释：Sigmoid 在二分类中可输出概率；LSTM/GRU 的门\n",
    "    控结构依赖激活函数调节信息通过量。\n",
    "\n",
    "  总之，激活函数是深度模型由“线性叠加”跃迁为“可表达复杂模式”的\n",
    "  关键，使网络具备强非线性拟合能力。\n",
    "\n",
    "\n",
    "把神经网络想成一台“自动调味机”：\n",
    "\n",
    "  - 只用线性层（不加非线性）\n",
    "    就像只会按比例混合盐和糖。无论你堆多少层，最后的味道仍旧是“咸甜\n",
    "    线性组合”，做不出酸辣、苦甜这些复杂口味，复杂数据就学不会。例如\n",
    "    经典的 XOR（异或）逻辑问题，没有非线性层怎么堆都解不了。\n",
    "  - 加上非线性激活（ReLU、Sigmoid 等）\n",
    "    等于给每层加了不同的“调味决策”，某些输入激活、某些不激活，从而\n",
    "    在不同区域呈现不同反应：ReLU 遇到负数直接归零、遇到正数保持原\n",
    "    样；Sigmoid 把输出压到 0～1 像概率；Tanh 给出 -1～1 的“冷暖色\n",
    "    调”。这些非线性让网络能拼装出千变万化的口味，拟合真实世界的复杂\n",
    "    关系\n",
    "\n",
    "通俗例子：\n",
    "\n",
    "  1. 房价预测：面积越大越贵，但90平到100平涨幅和30平到40平不一样；\n",
    "     有阳台、朝向、学区等都会突变式影响价格。非线性激活让模型在不同\n",
    "     因素组合下给出区别对待。\n",
    "  2. 图像识别：像素只是线性叠加没法认出猫狗；通过卷积后叠加 ReLU，\n",
    "     网络能在不同区域捕捉边缘、纹理、形状的复杂组合。\n",
    "\n",
    "  3. 语音情感分析：语速、语调、停顿等特征之间关系非常复杂，没有非线\n",
    "     性，模型只能得出“音量越大越激动”这种线性结论；加了激活后可以组\n",
    "     合出“语速快 + 音调高但音量低”表示兴奋等丰富逻辑\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "855ee2745ff8b3cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "###  sigmoid 激活函数\n",
    "  - 作用：将输入映射到 (0, 1) 范围，常用于二分类问题的输出层。\n",
    "  - 公式：σ(x) = 1 / (1 + exp(-x))\n",
    "  - 性质：\n",
    "    - 输出恒在 (0, 1) 之间，可解释为概率。\n",
    "    - 对输入的微小变化敏感，导数在 (0, 0.25) 范围内。\n",
    "    -  非单调，对输入的变化不敏感。\n",
    "\n",
    ". 非线性变换\n",
    "中心点: sigmoid(0) = 0.5\n",
    "对称性: 输入值越大，输出越接近1；输入值越小，输出越接近0\n",
    "饱和性: 极端值被压缩到接近0或1\n",
    "\n",
    "> 优点:\n",
    "```\n",
    "    _输出范围固定，便于解释\n",
    "    平滑连续，数学性质良好_\n",
    "```\n",
    "\n",
    "> 缺点:\n",
    "```\n",
    "    梯度消失问题（在深度网络中）\n",
    "    计算成本较高\n",
    "    可能导致训练困难\n",
    "```"
   ],
   "id": "274c8e640bfe69b7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归一化层（Normalization）\n",
    "\n",
    "  - 目的：让每层的输入保持相似尺度，缓解梯度消失并加速收敛。\n",
    "  - 基本公式：y = gamma * (x - mean) / sqrt(var + eps) + beta，通过可学习的 gamma/beta 恢复表达能力。\n",
    "  - 常见形式：\n",
    "    - BatchNorm：对每个通道在一个 mini-batch 内统计均值方差，卷积网络首选。\n",
    "    - LayerNorm：沿最后一个特征维度归一化，序列/Transformer 中更稳。\n",
    "    - GroupNorm 与 InstanceNorm：batch 极小或风格迁移时保持稳定性。\n",
    "  - 训练与推理：训练期使用当前批次统计量，推理期使用滑动平均，确保输出一致。\n"
   ],
   "id": "8d51e1fc6d39bd28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 构造 3 个样本、3 个特征的小批量\n",
    "torch.manual_seed(0)\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [2.0, 4.0, 6.0],\n",
    "                  [3.0, 6.0, 9.0]])\n",
    "\n",
    "bn = nn.BatchNorm1d(num_features=3)\n",
    "bn.train()  # 训练模式：使用当前批次的统计量\n",
    "y = bn(x)\n",
    "\n",
    "print('原始每列均值:', x.mean(dim=0))\n",
    "print('归一化后均值≈0:', y.mean(dim=0).round(decimals=5))\n",
    "print('归一化后方差≈1:', y.var(dim=0, unbiased=False).round(decimals=5))\n"
   ],
   "id": "eb7ed8b624e2ee12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 运行结果显示：归一化层把批量内每个通道的分布拉回“零均值、单位方差”，\n",
    "> 训练更稳，也为后续可学习参数保留扩展空间。\n"
   ],
   "id": "7b7c089fb00c09d3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络层Recurrent（RNN 系列）\n",
    "\n",
    "  - 通过隐藏状态 h_t 记忆时间上下文：h_t = f(x_t, h_{t-1})。\n",
    "  - 基础 nn.RNN 结构简单但易梯度消失，LSTM/GRU 通过门控结构控制信息流。\n",
    "  - 常见接口：\n",
    "    - nn.RNN：最简循环层，可堆叠多层、设置双向。\n",
    "    - nn.LSTM：输入/遗忘/输出门 + 细胞状态，适合长期依赖。\n",
    "    - nn.GRU：合并门控，参数更少，训练更快。\n",
    "  - 输出包括整段隐藏状态序列和最终状态，可接分类/注意力等后续模块。\n",
    "  - 应用：文本生成、语音识别、时间序列预测等顺序数据建模。\n"
   ],
   "id": "2b7fa06dadb1f2f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 构造 4 个时间步的简单序列，每步 1 维特征\n",
    "seq = torch.tensor([[0.1], [0.2], [0.5], [0.9]])\n",
    "seq = seq.unsqueeze(1)  # 形状：时间步 × batch × 特征\n",
    "\n",
    "gru = nn.GRU(input_size=1, hidden_size=2, batch_first=False)\n",
    "out, h_last = gru(seq)\n",
    "\n",
    "print('每个时间步的隐藏状态:')\n",
    "print(out.squeeze(1).round(decimals=4))\n",
    "print('最后时间步的摘要特征:', h_last.squeeze(0).round(decimals=4))\n"
   ],
   "id": "85e1186ee50227b6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 隐藏状态像“记忆条”，随时间步滚动更新；最后一个状态可以直接送入分类头，\n",
    "> 帮助网络理解“前后文”之间的顺序关系。\n"
   ],
   "id": "e0706d44862afc0a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 层\n",
    "  - 作用：序列建模的强大工具，基于注意力机制并行处理长序列。\n",
    "  - 结构：\n",
    "    - 编码器（Encoder）：自注意力 + 前馈网络，堆叠多层。\n",
    "    - 解码器（Decoder）：自注意力 + 编码器-解码器注意力 + 前馈网络，堆叠多层。\n",
    "\n",
    "  - 核心由自注意力（Self-Attention）组成：基于 Query/Key/Value 计算加权和捕捉全局依赖。\n",
    "  - 多头注意力将注意力拆成多个子空间并行学习，最后拼接映射。\n",
    "  - 每个子层采用残差连接 + LayerNorm，保持梯度流动和数值稳定。\n",
    "  - 前馈网络（Position-wise FFN）逐位置施加非线性变换，提升表达力。\n",
    "  - 位置编码提供序列顺序感知，整体结构可并行处理长序列，适合 NLP/CV/语音等任务。\n"
   ],
   "id": "cac531bc36177c6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "layer = nn.TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=8)\n",
    "\n",
    "# 3 个时间步，假设 batch=1，特征维度 d_model=4\n",
    "tokens = torch.arange(12, dtype=torch.float32).reshape(3, 1, 4)\n",
    "encoded = layer(tokens)\n",
    "\n",
    "print('原始输入（时间步 × 特征）:')\n",
    "print(tokens.squeeze(1))\n",
    "print('自注意力后输出:')\n",
    "print(encoded.squeeze(1).round(decimals=4))\n"
   ],
   "id": "a87d8056a22c6a74"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 每个时间步的新向量都混合了全局信息：哪怕输入只是一串递增数字，\n",
    "> 自注意力也会根据相似度重新分配权重，形成“全局上下文”的表示。\n"
   ],
   "id": "6540a83cd8c68c71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f693086273573926"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1a7dcdd7c4187806"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性层（Linear Layer）\n",
    "\n",
    "  - 计算公式：y = Wx + b，本质是对输入做一次仿射变换，提炼出加权组合。\n",
    "  - 每个神经元学习一组权重，适合处理已展平的特征或上一层的输出。\n",
    "  - 在深度网络中常作为“信息汇总层”，与非线性激活交替堆叠。\n"
   ],
   "id": "8199f0ad35ffbd02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "layer = nn.Linear(in_features=2, out_features=1)\n",
    "with torch.no_grad():\n",
    "    layer.weight.copy_(torch.tensor([[0.5, -0.25]]))\n",
    "    layer.bias.fill_(0.1)\n",
    "\n",
    "samples = torch.tensor([[2.0, 1.0],\n",
    "                         [0.5, -0.5]])\n",
    "outputs = layer(samples)\n",
    "print('输入样本:', samples)\n",
    "print('线性层输出 = 0.5*x1 - 0.25*x2 + 0.1:', outputs)\n"
   ],
   "id": "d1aafce5f8a66f82"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 线性层相当于对特征做“权重叠加 + 偏置”，示例中可以直接对照公式计算结果，\n",
    "> 便于理解其作为最基本的特征变换单元。\n"
   ],
   "id": "7ce46c2953fc7ff9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout 层\n",
    "\n",
    "  - 训练期随机“关闭”部分神经元（置零），强迫网络不过度依赖单一通路。\n",
    "  - 推理期关闭随机性，按保留概率缩放权重，确保输出期望与训练一致。\n",
    "  - 常用在全连接层末尾或多头注意力后，减轻过拟合。\n"
   ],
   "id": "2cc95a3d00de8dad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.ones(6)\n",
    "drop = nn.Dropout(p=0.5)\n",
    "\n",
    "drop.train()  # 训练模式：随机丢弃\n",
    "train_out = drop(x)\n",
    "drop.eval()   # 推理模式：关闭丢弃\n",
    "eval_out = drop(x)\n",
    "\n",
    "print('训练模式输出（部分元素变 0，同时其他元素放大 1/(1-p)）:', train_out)\n",
    "print('推理模式输出（全部保留，数值不再缩放）:', eval_out)\n"
   ],
   "id": "408b5d5c771b2984"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 多次运行训练模式会得到不同掩码，像是在“摇骰子”帮助模型学习更稳健的表示。\n"
   ],
   "id": "fadb7f752b80b05e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 稀疏层（Sparse Layer）\n",
    "\n",
    "  - 面对“高维但大多数为 0”的输入，稀疏层只保存、更新必要连接，节省内存与计算。\n",
    "  - 典型例子是词嵌入（Embedding）：仅根据词索引查表，无需构造完整 one-hot 向量。\n",
    "  - PyTorch 中可通过 `nn.Embedding(..., sparse=True)` 启用稀疏梯度，只更新被访问的行。\n"
   ],
   "id": "8d7b2e80a4bbcc93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "embedding = nn.Embedding(num_embeddings=5, embedding_dim=3, sparse=True)\n",
    "indices = torch.tensor([0, 2, 2, 4])  # 访问少量词索引\n",
    "picked = embedding(indices)\n",
    "print('查询到的嵌入向量:', picked)\n",
    "\n",
    "loss = picked.sum()\n",
    "loss.backward()\n",
    "print('梯度仅在使用到的行非零:', embedding.weight.grad)\n"
   ],
   "id": "a2352a35d6bf97be"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 稀疏更新意味着词表再大也只对被访问的行求梯度，是处理海量离散特征的关键手段。\n"
   ],
   "id": "339e5668cd401f86"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 距离函数（Distance Functions）\n",
    "\n",
    "  - 用来衡量两个向量、分布或样本之间的相似度/差异度，是聚类、最近邻、对比学习等任务的基础。\n",
    "  - 常见形式：\n",
    "    - 欧氏距离（L2）：sqrt(Σ (x_i - y_i)^2)，关注整体差异。\n",
    "    - 曼哈顿距离（L1）：Σ |x_i - y_i|，对离群值更稳健。\n",
    "    - 余弦距离：1 - cos(x, y)，更看重方向相似度。\n",
    "  - 在深度学习里，距离函数常被嵌入到损失中，引导特征空间的几何结构。\n"
   ],
   "id": "3e7966cc619a8c56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([0.0, 1.0, 2.0])\n",
    "y = torch.tensor([1.0, 1.0, 1.0])\n",
    "\n",
    "euclidean = torch.dist(x, y, p=2)   # 欧氏距离\n",
    "manhattan = torch.dist(x, y, p=1)   # 曼哈顿距离\n",
    "cosine = 1 - torch.nn.functional.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0))\n",
    "\n",
    "print('欧氏距离:', euclidean.item())\n",
    "print('曼哈顿距离:', manhattan.item())\n",
    "print('余弦距离:', cosine.item())\n"
   ],
   "id": "7a67b15a182203e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 不同距离函数关注的侧重点不同：同样的样本对，余弦距离更看方向，欧氏/曼哈顿更看数值差异。\n"
   ],
   "id": "38167171c2249808"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数（Loss Functions）\n",
    "\n",
    "  - 用来衡量模型预测与目标之间的差异，是训练时反向传播的根本信号。\n",
    "  - 常见类别：\n",
    "    - 回归：均方误差（MSE）、L1 Loss。\n",
    "    - 分类：交叉熵（CrossEntropyLoss）、二元交叉熵（BCEWithLogitsLoss）。\n",
    "    - 对比/度量学习：Triplet Loss、InfoNCE 等。\n",
    "  - 损失函数往往会融合距离函数或概率分布差异（KL 散度等），以反映任务目标。\n"
   ],
   "id": "5d7693b7041c19e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "pred = torch.tensor([[2.0, -1.0, 0.5]])          # logits\n",
    "target = torch.tensor([0])                         # 真值标签\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "ce_loss = loss_fn(pred, target)\n",
    "\n",
    "reg_pred = torch.tensor([2.5, 3.0, 4.0])\n",
    "reg_target = torch.tensor([3.0, 2.0, 4.5])\n",
    "mse = nn.functional.mse_loss(reg_pred, reg_target)\n",
    "\n",
    "print('交叉熵损失:', ce_loss.item())\n",
    "print('均方误差:', mse.item())\n"
   ],
   "id": "8e3b99f286961427"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 分类里常用交叉熵惩罚错误类别的高置信度；回归则用均方误差度量预测值与目标值的差异。\n"
   ],
   "id": "52135da887497192"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 交叉熵理解 :\n",
   "id": "474c66e423f9d2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "  # 多分类交叉熵损失 (Cross Entropy Loss)\n",
    "  # L = -∑(i=1 to C) y_i * log(p_i)\n",
    "\n",
    "  # 其中:\n",
    "  # C = 类别总数\n",
    "  # y_i = 真实标签的one-hot编码 (只有正确类别为1,其他为0)\n",
    "  # p_i = 模型预测的概率分布 (softmax后的输出)\n",
    "\n",
    "\n"
   ],
   "id": "93684caf9d335f97"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
