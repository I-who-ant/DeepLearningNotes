{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "### 通道理解\n",
    "\n",
    "  - 对输入图像：RGB 彩图就是典型例子，每个像素有红、绿、蓝三个数值，对应三个输入通道；灰度图则只有一个通道。\n",
    "  - 对中间特征图：某一层输出往往包含很多特征图，每个特征图就是一个通道，用来表示模型从不同角度提取的局部模式（边缘、纹理、形状等）。\n",
    "\n",
    "  换句话说，通道是“在同样的高×宽坐标上，堆叠的独立信息层”。卷积核在处理多通道输入时，会为每个通道准备一套 kH×kW 权重，\n",
    "  并把所有通道的局部结果加总，生成一个输出通道；输出通道数由我们配置，代表想提取的特征类型数量。"
   ],
   "id": "a9ba59b97472bdcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 张量形状与通道的关系\n",
    "形状 (C, H, W) 表示：通道数 × 高度 × 宽度\n",
    "三个通道共享同一个张量，不是三个独立的张量\n",
    "所有通道的形状相同（都是 H×W），但值不同\n",
    "通道是张量的第一个维度切片：tensor[0]=红色通道，tensor[1]=绿色通道，tensor[2]=蓝色通道\n",
    "\n",
    "\n"
   ],
   "id": "415c171a29c214fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 图像格式转换的可逆性\n",
    "PIL.Image → NumPy数组\n",
    "→ PyTorch张量 → NumPy数组 → PIL.Image\n",
    "\n",
    "PIL.Image ↔ NumPy数组：直接的数据转换\n",
    "NumPy数组 ↔ PyTorch张量：内存共享的高效转换\n",
    "整个过程是可逆的：图像可以无损（或接近无损）地来回转换"
   ],
   "id": "a556e3f9f5fa01f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 核心理解要点：\n",
    "形状相同，值不同 - 通道关系\n",
    "归一化提高稳定性 - 数据预处理\n",
    "格式转换可逆 - 图像与张量的双向转换\n",
    "张量=图像数据 - 深度学习的基础\n"
   ],
   "id": "5a359002b3e59685"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 卷积神经网络中通道数\n",
    "\n",
    "-------------------------------------\n",
    "> 输入通道 (in_channels)\n",
    "\n",
    "由数据决定：RGB图像=3，灰度图=1\n",
    "不可改变：必须匹配输入数据的通道数\n",
    "示例：in_channels=3（RGB图像）\n",
    "\n",
    "-------------------------------------\n",
    "> 输出通道 (out_channels)\n",
    "\n",
    "设计选择：人为设定的超参数\n",
    "代表特征数量：每个通道学习一种特征模式\n",
    "示例：out_channels=64（学习64种特征）\n",
    "\n",
    "----------------------------------\n",
    "> 通道数变化的意义\n",
    "        (输入: (batch, 3, H, W) → 卷积层 → 输出: (batch, 64, H', W')\n",
    "- 增加输出通道数：模型学习更多特征，表达能力更强\n",
    "- 减少输出通道数：模型学习 fewer features，表达能力更弱\n",
    "- 保持通道数不变：模型不学习新特征，仅对输入进行简单变换\n",
    "\n",
    "\n",
    "----------------------------------\n",
    "> 通道数变化的意义\n",
    "\n",
    "任务复杂度：复杂任务需要更多特征通道\n",
    "计算资源：更多通道=更多计算和内存\n",
    "过拟合风险：通道太多可能过拟合训练数据\n",
    "经验法则：通常使用2的幂次（16,32,64,128...）\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "\n"
   ],
   "id": "2feeb37c8124958"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 卷积层输出尺寸的完整公式\n",
    "\n",
    "H_out = ⌊(H + 2p - kH) / s⌋ + 1\n",
    "W_out = ⌊(W + 2p - kW) / s⌋ + 1\n",
    "\n",
    "> 其中：\n",
    "\n",
    "> H_out = 输出高度\n",
    "\n",
    "> W_out = 输出宽度\n",
    "\n",
    "> H = 输入高度\n",
    "\n",
    "> W = 输入宽度\n",
    "\n",
    "> kH = 卷积核高度\n",
    "\n",
    "> kW = 卷积核宽度\n",
    "\n",
    "> p = 填充（padding）\n",
    "\n",
    "> s = 步幅（stride）\n",
    "\n",
    "> ⌊x⌋ 表示向下取整"
   ],
   "id": "bb306871f65a1dc2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 池化层的基本概念\n",
    "池化层（Pooling Layer）是卷积神经网络（CNN）中常用的一种层类型，主要用于特征压缩和降维，同时保留重要特征。\n",
    "池化层的基本概念包括：\n",
    "\n",
    "1. 池化核（Pooling Kernel）：与卷积核类似，池化核在输入特征图上滑动，对每个位置的小区域进行操作。\n",
    "2. 操作方式：常用的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）。\n",
    "\n",
    "Max Pooling：在每个小区域内取最大值，保留最强特征。\n",
    "Average Pooling：在每个小区域内取平均值，平滑特征。\n",
    "\n",
    "\n",
    "> 池化层的\"跳跃\"机制 :\n",
    "--------------------------------------------\n",
    "池化层的设计理念是信息压缩，而不是信息重复利用：\n",
    "\n",
    "每个区域只取一个代表值（最大值或平均值）\n",
    "步长决定了跳跃距离\n",
    "目的是降维，不是保留所有细节\n",
    "\n"
   ],
   "id": "ac3efdea9b0b21a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "(后续卷积层正是处理这份压缩特征：它们在更低分辨率上继续学习更抽象的模式，既节省计算，又扩大感受视野。",
   "id": "201a74fe199a6ab1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "## 池化层输出尺寸的完整公式\n",
    "\n",
    "H_out = ⌊(H - kH) / s⌋ + 1\n",
    "W_out = ⌊(W - kW) / s⌋ + 1\n",
    "\n",
    "> 其中：\n",
    "\n",
    "> H_out = 输出高度\n",
    "\n",
    "> W_out = 输出宽度\n",
    "\n",
    "> H = 输入高度\n",
    "\n",
    "> W = 输入宽度\n",
    "\n",
    "> kH = 池化核高度\n",
    "\n",
    "> kW = 池化核宽度\n",
    "\n",
    "> s = 步幅（stride）\n",
    "\n",
    "> ⌊x⌋ 表示向下取整\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "81f20c6095a73597"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8902acbef23af4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 非线性激活层\n",
    "(在神经网络中打破简单的线性映射，赋予模型拟合复杂函数的能力)\n",
    "\n",
    "具体来说：\n",
    "\n",
    "  - 引入非线性表达：若仅堆叠线性层，整体仍是线性变换，只能拟合\n",
    "    超平面。激活函数在每层的加权和后施加非线性，使网络能逼近任\n",
    "    意复杂的非线性关系。\n",
    "  - 分段特征提取：ReLU、LeakyReLU、GELU 等在不同输入区间呈现不\n",
    "    同响应，帮助网络学习多样化的特征模式。\n",
    "  - 梯度传播调节：某些激活（如Sigmoid、Tanh）在早期被用来平衡梯\n",
    "    度，现代网络常选ReLU类以减轻梯度消失并加速收敛。\n",
    "  - 概率或门控解释：Sigmoid 在二分类中可输出概率；LSTM/GRU 的门\n",
    "    控结构依赖激活函数调节信息通过量。\n",
    "\n",
    "  总之，激活函数是深度模型由“线性叠加”跃迁为“可表达复杂模式”的\n",
    "  关键，使网络具备强非线性拟合能力。\n",
    "\n",
    "\n",
    "把神经网络想成一台“自动调味机”：\n",
    "\n",
    "  - 只用线性层（不加非线性）\n",
    "    就像只会按比例混合盐和糖。无论你堆多少层，最后的味道仍旧是“咸甜\n",
    "    线性组合”，做不出酸辣、苦甜这些复杂口味，复杂数据就学不会。例如\n",
    "    经典的 XOR（异或）逻辑问题，没有非线性层怎么堆都解不了。\n",
    "  - 加上非线性激活（ReLU、Sigmoid 等）\n",
    "    等于给每层加了不同的“调味决策”，某些输入激活、某些不激活，从而\n",
    "    在不同区域呈现不同反应：ReLU 遇到负数直接归零、遇到正数保持原\n",
    "    样；Sigmoid 把输出压到 0～1 像概率；Tanh 给出 -1～1 的“冷暖色\n",
    "    调”。这些非线性让网络能拼装出千变万化的口味，拟合真实世界的复杂\n",
    "    关系\n",
    "\n",
    "通俗例子：\n",
    "\n",
    "  1. 房价预测：面积越大越贵，但90平到100平涨幅和30平到40平不一样；\n",
    "     有阳台、朝向、学区等都会突变式影响价格。非线性激活让模型在不同\n",
    "     因素组合下给出区别对待。\n",
    "  2. 图像识别：像素只是线性叠加没法认出猫狗；通过卷积后叠加 ReLU，\n",
    "     网络能在不同区域捕捉边缘、纹理、形状的复杂组合。\n",
    "\n",
    "  3. 语音情感分析：语速、语调、停顿等特征之间关系非常复杂，没有非线\n",
    "     性，模型只能得出“音量越大越激动”这种线性结论；加了激活后可以组\n",
    "     合出“语速快 + 音调高但音量低”表示兴奋等丰富逻辑\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "855ee2745ff8b3cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "###  sigmoid 激活函数\n",
    "  - 作用：将输入映射到 (0, 1) 范围，常用于二分类问题的输出层。\n",
    "  - 公式：σ(x) = 1 / (1 + exp(-x))\n",
    "  - 性质：\n",
    "    - 输出恒在 (0, 1) 之间，可解释为概率。\n",
    "    - 对输入的微小变化敏感，导数在 (0, 0.25) 范围内。\n",
    "    -  非单调，对输入的变化不敏感。\n",
    "\n",
    ". 非线性变换\n",
    "中心点: sigmoid(0) = 0.5\n",
    "对称性: 输入值越大，输出越接近1；输入值越小，输出越接近0\n",
    "饱和性: 极端值被压缩到接近0或1\n",
    "\n",
    "> 优点:\n",
    "```\n",
    "    _输出范围固定，便于解释\n",
    "    平滑连续，数学性质良好_\n",
    "```\n",
    "\n",
    "> 缺点:\n",
    "```\n",
    "    梯度消失问题（在深度网络中）\n",
    "    计算成本较高\n",
    "    可能导致训练困难\n",
    "```"
   ],
   "id": "274c8e640bfe69b7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归一化层（Normalization）\n",
    "\n",
    "  - 目的：让每层的输入保持相似尺度，缓解梯度消失并加速收敛。\n",
    "  - 基本公式：y = gamma * (x - mean) / sqrt(var + eps) + beta，通过可学习的 gamma/beta 恢复表达能力。\n",
    "  - 常见形式：\n",
    "    - BatchNorm：对每个通道在一个 mini-batch 内统计均值方差，卷积网络首选。\n",
    "    - LayerNorm：沿最后一个特征维度归一化，序列/Transformer 中更稳。\n",
    "    - GroupNorm 与 InstanceNorm：batch 极小或风格迁移时保持稳定性。\n",
    "  - 训练与推理：训练期使用当前批次统计量，推理期使用滑动平均，确保输出一致。\n"
   ],
   "id": "8d51e1fc6d39bd28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 构造 3 个样本、3 个特征的小批量\n",
    "torch.manual_seed(0)\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [2.0, 4.0, 6.0],\n",
    "                  [3.0, 6.0, 9.0]])\n",
    "\n",
    "bn = nn.BatchNorm1d(num_features=3)\n",
    "bn.train()  # 训练模式：使用当前批次的统计量\n",
    "y = bn(x)\n",
    "\n",
    "print('原始每列均值:', x.mean(dim=0))\n",
    "print('归一化后均值≈0:', y.mean(dim=0).round(decimals=5))\n",
    "print('归一化后方差≈1:', y.var(dim=0, unbiased=False).round(decimals=5))\n"
   ],
   "id": "eb7ed8b624e2ee12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 运行结果显示：归一化层把批量内每个通道的分布拉回“零均值、单位方差”，\n",
    "> 训练更稳，也为后续可学习参数保留扩展空间。\n"
   ],
   "id": "7b7c089fb00c09d3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络层Recurrent（RNN 系列）\n",
    "\n",
    "  - 通过隐藏状态 h_t 记忆时间上下文：h_t = f(x_t, h_{t-1})。\n",
    "  - 基础 nn.RNN 结构简单但易梯度消失，LSTM/GRU 通过门控结构控制信息流。\n",
    "  - 常见接口：\n",
    "    - nn.RNN：最简循环层，可堆叠多层、设置双向。\n",
    "    - nn.LSTM：输入/遗忘/输出门 + 细胞状态，适合长期依赖。\n",
    "    - nn.GRU：合并门控，参数更少，训练更快。\n",
    "  - 输出包括整段隐藏状态序列和最终状态，可接分类/注意力等后续模块。\n",
    "  - 应用：文本生成、语音识别、时间序列预测等顺序数据建模。\n"
   ],
   "id": "2b7fa06dadb1f2f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 构造 4 个时间步的简单序列，每步 1 维特征\n",
    "seq = torch.tensor([[0.1], [0.2], [0.5], [0.9]])\n",
    "seq = seq.unsqueeze(1)  # 形状：时间步 × batch × 特征\n",
    "\n",
    "gru = nn.GRU(input_size=1, hidden_size=2, batch_first=False)\n",
    "out, h_last = gru(seq)\n",
    "\n",
    "print('每个时间步的隐藏状态:')\n",
    "print(out.squeeze(1).round(decimals=4))\n",
    "print('最后时间步的摘要特征:', h_last.squeeze(0).round(decimals=4))\n"
   ],
   "id": "85e1186ee50227b6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 隐藏状态像“记忆条”，随时间步滚动更新；最后一个状态可以直接送入分类头，\n",
    "> 帮助网络理解“前后文”之间的顺序关系。\n"
   ],
   "id": "e0706d44862afc0a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 层\n",
    "  - 作用：序列建模的强大工具，基于注意力机制并行处理长序列。\n",
    "  - 结构：\n",
    "    - 编码器（Encoder）：自注意力 + 前馈网络，堆叠多层。\n",
    "    - 解码器（Decoder）：自注意力 + 编码器-解码器注意力 + 前馈网络，堆叠多层。\n",
    "\n",
    "  - 核心由自注意力（Self-Attention）组成：基于 Query/Key/Value 计算加权和捕捉全局依赖。\n",
    "  - 多头注意力将注意力拆成多个子空间并行学习，最后拼接映射。\n",
    "  - 每个子层采用残差连接 + LayerNorm，保持梯度流动和数值稳定。\n",
    "  - 前馈网络（Position-wise FFN）逐位置施加非线性变换，提升表达力。\n",
    "  - 位置编码提供序列顺序感知，整体结构可并行处理长序列，适合 NLP/CV/语音等任务。\n"
   ],
   "id": "cac531bc36177c6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "layer = nn.TransformerEncoderLayer(d_model=4, nhead=2, dim_feedforward=8)\n",
    "\n",
    "# 3 个时间步，假设 batch=1，特征维度 d_model=4\n",
    "tokens = torch.arange(12, dtype=torch.float32).reshape(3, 1, 4)\n",
    "encoded = layer(tokens)\n",
    "\n",
    "print('原始输入（时间步 × 特征）:')\n",
    "print(tokens.squeeze(1))\n",
    "print('自注意力后输出:')\n",
    "print(encoded.squeeze(1).round(decimals=4))\n"
   ],
   "id": "a87d8056a22c6a74"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 每个时间步的新向量都混合了全局信息：哪怕输入只是一串递增数字，\n",
    "> 自注意力也会根据相似度重新分配权重，形成“全局上下文”的表示。\n"
   ],
   "id": "6540a83cd8c68c71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f693086273573926"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1a7dcdd7c4187806"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性层（Linear Layer）\n",
    "\n",
    "  - 计算公式：y = Wx + b，本质是对输入做一次仿射变换，提炼出加权组合。\n",
    "  - 每个神经元学习一组权重，适合处理已展平的特征或上一层的输出。\n",
    "  - 在深度网络中常作为“信息汇总层”，与非线性激活交替堆叠。\n"
   ],
   "id": "8199f0ad35ffbd02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "layer = nn.Linear(in_features=2, out_features=1)\n",
    "with torch.no_grad():\n",
    "    layer.weight.copy_(torch.tensor([[0.5, -0.25]]))\n",
    "    layer.bias.fill_(0.1)\n",
    "\n",
    "samples = torch.tensor([[2.0, 1.0],\n",
    "                         [0.5, -0.5]])\n",
    "outputs = layer(samples)\n",
    "print('输入样本:', samples)\n",
    "print('线性层输出 = 0.5*x1 - 0.25*x2 + 0.1:', outputs)\n"
   ],
   "id": "d1aafce5f8a66f82"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 线性层相当于对特征做“权重叠加 + 偏置”，示例中可以直接对照公式计算结果，\n",
    "> 便于理解其作为最基本的特征变换单元。\n"
   ],
   "id": "7ce46c2953fc7ff9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout 层\n",
    "\n",
    "  - 训练期随机“关闭”部分神经元（置零），强迫网络不过度依赖单一通路。\n",
    "  - 推理期关闭随机性，按保留概率缩放权重，确保输出期望与训练一致。\n",
    "  - 常用在全连接层末尾或多头注意力后，减轻过拟合。\n"
   ],
   "id": "2cc95a3d00de8dad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x = torch.ones(6)\n",
    "drop = nn.Dropout(p=0.5)\n",
    "\n",
    "drop.train()  # 训练模式：随机丢弃\n",
    "train_out = drop(x)\n",
    "drop.eval()   # 推理模式：关闭丢弃\n",
    "eval_out = drop(x)\n",
    "\n",
    "print('训练模式输出（部分元素变 0，同时其他元素放大 1/(1-p)）:', train_out)\n",
    "print('推理模式输出（全部保留，数值不再缩放）:', eval_out)\n"
   ],
   "id": "408b5d5c771b2984"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 多次运行训练模式会得到不同掩码，像是在“摇骰子”帮助模型学习更稳健的表示。\n"
   ],
   "id": "fadb7f752b80b05e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 稀疏层（Sparse Layer）\n",
    "\n",
    "  - 面对“高维但大多数为 0”的输入，稀疏层只保存、更新必要连接，节省内存与计算。\n",
    "  - 典型例子是词嵌入（Embedding）：仅根据词索引查表，无需构造完整 one-hot 向量。\n",
    "  - PyTorch 中可通过 `nn.Embedding(..., sparse=True)` 启用稀疏梯度，只更新被访问的行。\n"
   ],
   "id": "8d7b2e80a4bbcc93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "embedding = nn.Embedding(num_embeddings=5, embedding_dim=3, sparse=True)\n",
    "indices = torch.tensor([0, 2, 2, 4])  # 访问少量词索引\n",
    "picked = embedding(indices)\n",
    "print('查询到的嵌入向量:', picked)\n",
    "\n",
    "loss = picked.sum()\n",
    "loss.backward()\n",
    "print('梯度仅在使用到的行非零:', embedding.weight.grad)\n"
   ],
   "id": "a2352a35d6bf97be"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 稀疏更新意味着词表再大也只对被访问的行求梯度，是处理海量离散特征的关键手段。\n"
   ],
   "id": "339e5668cd401f86"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 距离函数（Distance Functions）\n",
    "\n",
    "  - 用来衡量两个向量、分布或样本之间的相似度/差异度，是聚类、最近邻、对比学习等任务的基础。\n",
    "  - 常见形式：\n",
    "    - 欧氏距离（L2）：sqrt(Σ (x_i - y_i)^2)，关注整体差异。\n",
    "    - 曼哈顿距离（L1）：Σ |x_i - y_i|，对离群值更稳健。\n",
    "    - 余弦距离：1 - cos(x, y)，更看重方向相似度。\n",
    "  - 在深度学习里，距离函数常被嵌入到损失中，引导特征空间的几何结构。\n"
   ],
   "id": "3e7966cc619a8c56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([0.0, 1.0, 2.0])\n",
    "y = torch.tensor([1.0, 1.0, 1.0])\n",
    "\n",
    "euclidean = torch.dist(x, y, p=2)   # 欧氏距离\n",
    "manhattan = torch.dist(x, y, p=1)   # 曼哈顿距离\n",
    "cosine = 1 - torch.nn.functional.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0))\n",
    "\n",
    "print('欧氏距离:', euclidean.item())\n",
    "print('曼哈顿距离:', manhattan.item())\n",
    "print('余弦距离:', cosine.item())\n"
   ],
   "id": "7a67b15a182203e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 不同距离函数关注的侧重点不同：同样的样本对，余弦距离更看方向，欧氏/曼哈顿更看数值差异。\n"
   ],
   "id": "38167171c2249808"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数（Loss Functions）\n",
    "\n",
    "  - 用来衡量模型预测与目标之间的差异，是训练时反向传播的根本信号。\n",
    "  - 常见类别：\n",
    "    - 回归：均方误差（MSE）、L1 Loss。\n",
    "    - 分类：交叉熵（CrossEntropyLoss）、二元交叉熵（BCEWithLogitsLoss）。\n",
    "    - 对比/度量学习：Triplet Loss、InfoNCE 等。\n",
    "  - 损失函数往往会融合距离函数或概率分布差异（KL 散度等），以反映任务目标。\n"
   ],
   "id": "5d7693b7041c19e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "pred = torch.tensor([[2.0, -1.0, 0.5]])          # logits\n",
    "target = torch.tensor([0])                         # 真值标签\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "ce_loss = loss_fn(pred, target)\n",
    "\n",
    "reg_pred = torch.tensor([2.5, 3.0, 4.0])\n",
    "reg_target = torch.tensor([3.0, 2.0, 4.5])\n",
    "mse = nn.functional.mse_loss(reg_pred, reg_target)\n",
    "\n",
    "print('交叉熵损失:', ce_loss.item())\n",
    "print('均方误差:', mse.item())\n"
   ],
   "id": "8e3b99f286961427"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 分类里常用交叉熵惩罚错误类别的高置信度；回归则用均方误差度量预测值与目标值的差异。\n"
   ],
   "id": "52135da887497192"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 交叉熵理解 :\n",
   "id": "474c66e423f9d2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "  # 多分类交叉熵损失 (Cross Entropy Loss)\n",
    "  # L = -∑(i=1 to C) y_i * log(p_i)\n",
    "\n",
    "  # 其中:\n",
    "  # C = 类别总数\n",
    "  # y_i = 真实标签的one-hot编码 (只有正确类别为1,其他为0)\n",
    "  # p_i = 模型预测的概率分布 (softmax后的输出)\n",
    "\n",
    "\n"
   ],
   "id": "93684caf9d335f97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "  ⎿ ██████████████████████████████████████████████████████████████████████\n",
    "                   从图像到 Logits 的完整解析\n",
    "    ██████████████████████████████████████████████████████████████████████\n",
    "\n",
    "======================================================================\n",
    ">    理解 logits:从1024维特征到10类打分\n",
    "\n",
    "======================================================================\n",
    "\n",
    "\n",
    "    【最后一层的权重矩阵】\n",
    "    形状: (10, 64) - 10行代表10个类别,64列对应64个输入特征\n",
    "\n",
    "    输入特征向量 (64维):\n",
    "      形状: torch.Size([1, 64])\n",
    "      前5个值: [0.08768323808908463, 1.0937845706939697, -0.1177205815911293, -0.29864323139190674,\n",
    "    1.5193016529083252]\n",
    "\n",
    "    计算过程: logits = 权重矩阵 @ 特征向量 + 偏置\n",
    "\n",
    "    输出 logits (10维):\n",
    "      形状: torch.Size([1, 10])\n",
    "\n",
    "    各类别的打分(logits):\n",
    "      类别0 ( 飞机):   1.8846\n",
    "      类别1 ( 汽车):   0.2688\n",
    "      类别2 (  鸟):   0.5111\n",
    "      类别3 (  猫):  -0.9009\n",
    "      类别4 (  鹿):  -0.7383\n",
    "      类别5 (  狗):  -0.4083\n",
    "      类别6 ( 青蛙):  -0.4371\n",
    "      类别7 (  马):   0.3352\n",
    "      类别8 (  船):  -0.0817\n",
    "      类别9 ( 卡车):  -0.4138\n",
    "\n",
    "    ======================================================================\n",
    "    关键理解:\n",
    "    ======================================================================\n",
    "    1. 每个 logit 值是该类别的'原始评分'\n",
    "    2. 评分通过特征向量和该类别权重的点积计算:\n",
    "       logit[i] = Σ(feature[j] * weight[i,j]) + bias[i]\n",
    "    3. 正值 → 模型认为'可能是'这个类别\n",
    "       负值 → 模型认为'不太可能是'这个类别\n",
    "    4. logits 之间可以比较:分数越高,模型越倾向该类别\n",
    "    ======================================================================\n",
    "\n",
    "    通过 softmax 转换为概率:\n",
    "       飞机: 0.4468 (44.68%) ██████████████████████\n",
    "       汽车: 0.0888 ( 8.88%) ████\n",
    "        鸟: 0.1131 (11.31%) █████\n",
    "        猫: 0.0276 ( 2.76%) █\n",
    "        鹿: 0.0324 ( 3.24%) █\n",
    "        狗: 0.0451 ( 4.51%) ██\n",
    "       青蛙: 0.0438 ( 4.38%) ██\n",
    "        马: 0.0949 ( 9.49%) ████\n",
    "        船: 0.0625 ( 6.25%) ███\n",
    "       卡车: 0.0449 ( 4.49%) ██\n",
    "\n",
    "    概率和: 1.000000 (必定为1.0)\n",
    "\n",
    "    ======================================================================\n",
    "    深度解析:权重如何编码'类别特征'\n",
    "    ======================================================================\n",
    "\n",
    "    假设简化场景:只有3个特征,3个类别\n",
    "\n",
    "    输入特征: [0.8, 0.2, 0.1]\n",
    "      特征0 = 0.8 (强) - 可能表示'有翅膀'\n",
    "      特征1 = 0.2 (弱) - 可能表示'有轮子'\n",
    "      特征2 = 0.1 (弱) - 可能表示'有鳍'\n",
    "\n",
    "    权重矩阵:\n",
    "             特征0  特征1  特征2\n",
    "      类别0: [ 1.0, -0.5, -0.3]  (飞机:需要'翅膀',不要'轮子/鳍')\n",
    "      类别1: [-0.2,  1.5, -0.1]  (汽车:需要'轮子',不要'翅膀/鳍')\n",
    "      类别2: [-0.3, -0.2,  2.0]  (鱼  :需要'鳍',  不要'翅膀/轮子')\n",
    "\n",
    "    计算过程:\n",
    "      类别0 logit = 0.8×  1.0 + 0.2× -0.5 + 0.1× -0.3\n",
    "                  = 0.800 + -0.100 + -0.030\n",
    "                  = 0.670\n",
    "\n",
    "      类别1 logit = 0.8× -0.2 + 0.2×  1.5 + 0.1× -0.1\n",
    "                  = -0.160 + 0.300 + -0.010\n",
    "                  = 0.130\n",
    "\n",
    "      类别2 logit = 0.8× -0.3 + 0.2× -0.2 + 0.1×  2.0\n",
    "                  = -0.240 + -0.040 + 0.200\n",
    "                  = -0.080\n",
    "\n",
    "    结果解释:\n",
    "      类别0(飞机) = 0.670 ← 最高分!因为'翅膀'特征强\n",
    "      类别1(汽车) = 0.130\n",
    "      类别2(鱼)   = -0.080\n",
    "\n",
    "    💡 权重学习的本质:\n",
    "      - 训练过程中,权重会自动调整\n",
    "      - 让能代表某类别的特征获得高权重\n",
    "      - 让不相关的特征获得低/负权重\n",
    "      - 这样 logit = 特征·权重 就能衡量'相似度'\n",
    "\n",
    "    ======================================================================\n",
    "    真实模型演示\n",
    "    ======================================================================\n",
    "\n",
    "    ======================================================================\n",
    "    前向传播详细过程\n",
    "    ======================================================================\n",
    "    输入图像: torch.Size([1, 3, 32, 32]) - 3通道32×32的RGB图像\n",
    "      ↓ Conv1+ReLU: torch.Size([1, 32, 32, 32]) - 提取32种低级特征(边缘/纹理)\n",
    "      ↓ Pool1:      torch.Size([1, 32, 16, 16]) - 降低空间分辨率\n",
    "      ↓ Conv2+ReLU: torch.Size([1, 32, 16, 16]) - 组合特征(角点/简单形状)\n",
    "      ↓ Pool2:      torch.Size([1, 32, 8, 8])\n",
    "      ↓ Conv3+ReLU: torch.Size([1, 64, 8, 8]) - 抽象特征(物体部件)\n",
    "      ↓ Pool3:      torch.Size([1, 64, 4, 4]) - 64个特征图,每个4×4\n",
    "\n",
    "      展平前: torch.Size([1, 64, 4, 4])\n",
    "      展平后: torch.Size([1, 1024]) - 将所有特征连接成向量\n",
    "              这是64个特征图×16个位置 = 1024维特征向量\n",
    "      ↓ FC1+ReLU:   torch.Size([1, 64]) - 压缩到64维语义特征\n",
    "              (这64个数编码了图像的高级语义信息)\n",
    "      ↓ FC2(输出):  torch.Size([1, 10]) - 10类打分(logits)\n",
    "    ======================================================================\n",
    "\n",
    "    输出分析:\n",
    "      Logits: [0.10276348143815994, -0.026738209649920464, -0.16858941316604614, -0.1435030698776245,\n",
    "    0.04232097789645195, -0.007952742278575897, 0.11912526190280914, -0.012702954933047295,\n",
    "    -0.04540777951478958, -0.14113347232341766]\n",
    "\n",
    "      最高分: 0.1191 (类别6: 青蛙)\n",
    "\n",
    "      转换为概率后:\n",
    "         飞机: 11.35%\n",
    "         汽车:  9.97%\n",
    "          鸟:  8.65%\n",
    "          猫:  8.87%\n",
    "          鹿: 10.68%\n",
    "          狗: 10.16%\n",
    "         青蛙: 11.53% 👈\n",
    "          马: 10.11%\n",
    "          船:  9.78%\n",
    "         卡车:  8.89%\n",
    "\n",
    "    ======================================================================\n",
    "    总结\n",
    "    ======================================================================\n",
    "    Logits 的本质:\n",
    "      1. 是模型的'原始判断'\n",
    "      2. 通过学习到的权重,将特征映射为类别评分\n",
    "      3. 未归一化,但可以比较大小\n",
    "      4. 通过 softmax 转为概率,用于交叉熵计算\n",
    "    ======================================================================\n"
   ],
   "id": "6653210d629dabf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 反向传播理解\n",
    "  核心理解:\n",
    "\n",
    "  1. 权重(weight) = 模型的可学习参数\n",
    "  2. 梯度(gradient) = loss对参数的导数 = \"斜率\"\n",
    "  3. 反向传播(backward) = 自动计算所有参数的梯度\n",
    "  4. 梯度下降(gradient descent) = 沿负梯度方向更新参数\n",
    "\n",
    "  为什么叫\"反向\"?\n",
    "\n",
    "  前向传播: 输入 → 层1 → 层2 → ... → 输出 → loss\n",
    "  反向传播: loss ← 层2 ← 层1 ← ... ← 输出 ← 梯度\n",
    "           (从loss开始,逆着计算图传播梯度)\n"
   ],
   "id": "21b639ea26e698b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " # # ===== 完整流程 =====\n",
    " #\n",
    " #  # 步骤1: 前向传播(构建计算图)\n",
    " #  images = torch.randn(4, 3, 32, 32)\n",
    " #  labels = torch.tensor([3, 7, 2, 5])\n",
    " #\n",
    " #  # PyTorch会记录每一步操作!\n",
    " #  logits = model(images)  # 内部记录: logits 依赖于 model.parameters()\n",
    " #  loss = criterion(logits, labels)  # 记录: loss 依赖于 logits\n",
    " #\n",
    " #  # 此时PyTorch内部的计算图:\n",
    " #  # model.weight → conv1 → relu → conv2 → ... → logits → loss\n",
    " #  #      ↑                                         ↑\n",
    " #  #    我们的参数                              我们的目标\n",
    " #\n",
    " #  # 步骤2: 反向传播(计算梯度)\n",
    " #  loss.backward()  # 一行代码完成所有梯度计算!\n",
    " #\n",
    " #  # 这一行做了什么?\n",
    " #  # 1. 从 loss 开始\n",
    " #  # 2. 沿着计算图反向走\n",
    " #  # 3. 用链式法则逐层计算梯度\n",
    " #  # 4. 把梯度存到 param.grad 里\n",
    " #\n",
    " #  # 步骤3: 查看梯度\n",
    " #  print(model.classifier[3].weight.grad.shape)  # torch.Size([10, 64])\n",
    " #  # 梯度的形状和权重完全一样!\n",
    " #\n",
    " #  3. 链式法则详解\n",
    " #\n",
    " #  # 简化的例子:\n",
    " #  #\n",
    " #  # 前向: x → w·x+b=y → (y-label)²=loss\n",
    " #  #       输入  线性变换  均方误差\n",
    " #  #\n",
    " #  # 反向: 计算 dL/dw\n",
    " #  #\n",
    " #  # 使用链式法则:\n",
    " #  # dL/dw = dL/dy × dy/dw\n",
    " #  #         ↑       ↑\n",
    " #  #       \"loss对y  \"y对w\n",
    " #  #        的敏感度\" 的敏感度\"\n",
    " #  #\n",
    " #  # 具体计算:\n",
    " #  # dL/dy = d/dy[(y-label)²] = 2(y-label)\n",
    " #  # dy/dw = d/dw[w·x+b] = x\n",
    " #  #\n",
    " #  # 因此:\n",
    " #  # dL/dw = 2(y-label) × x\n",
    " #\n",
    " #  # 多层网络的链式法则:\n",
    " #  #\n",
    " #  # 前向: x → Layer1 → a1 → Layer2 → a2 → loss\n",
    " #  #\n",
    " #  # 反向计算 Layer1 权重的梯度:\n",
    " #  # dL/dw1 = dL/da2 × da2/da1 × da1/dw1\n",
    " #  #          ↑        ↑         ↑\n",
    " #  #        从后往前   逐层传播    最终得到\n",
    " #\n",
    " #  4. 为什么梯度是\"斜率\"?\n",
    " #\n",
    " #  从运行结果可以看到:\n",
    " #\n",
    " #  # 例子: L(w) = w²\n",
    " #\n",
    " #  # 当 w = 2.0 时:\n",
    " #  # L(w) = 4.0\n",
    " #  # dL/dw = 4.0  ← 这是切线的斜率!\n",
    " #\n",
    " #  # 物理意义:\n",
    " #  # \"当w=2时,w每增加1,loss会增加约4\"\n",
    " #  # \"当w=2时,w每减少1,loss会减少约4\"\n",
    " #\n",
    " #  # 可视化:\n",
    " #  #     L(w)\n",
    " #  #      ↑\n",
    " #  #   4  |     ●  ← 当前点(w=2, L=4)\n",
    " #  #      |    /|\n",
    " #  #   3  |   / |  斜率=4 (梯度)\n",
    " #  #      |  /  |\n",
    " #  #   1  | /   |\n",
    " #  #      |/    |\n",
    " #  #   0  +-----+--→ w\n",
    " #  #      0  1  2  3\n",
    " #\n",
    " #  # 💡 梯度告诉我们两件事:\n",
    " #  # 1. 方向: 正值 → 函数在上升; 负值 → 函数在下降\n",
    " #  # 2. 大小: 绝对值大 → 上升/下降快; 绝对值小 → 上升/下降慢\n",
    " #\n",
    " #  5. 为什么\"参数 = 参数 - 学习率×梯度\"能减小loss?\n",
    " #\n",
    " #  # 数学证明(一维情况):\n",
    " #\n",
    " #  # 假设当前 w = w₀, loss = L(w₀)\n",
    " #  # 梯度 g = dL/dw|_{w=w₀}\n",
    " #\n",
    " #  # 泰勒展开:\n",
    " #  # L(w₀ - α·g) ≈ L(w₀) - α·g·g + O(α²)\n",
    " #  #                      ↑\n",
    " #  #                   这一项是负的!(因为g²>0)\n",
    " #\n",
    " #  # 因此当 α 足够小时:\n",
    " #  # L(w₀ - α·g) < L(w₀)  ✓\n",
    " #  #\n",
    " #  # 即:沿着负梯度方向移动,loss一定会减小!\n",
    " #\n",
    " #  # 直观理解:\n",
    " #  # - 梯度指向\"loss增长最快\"的方向\n",
    " #  # - 负梯度指向\"loss下降最快\"的方向\n",
    " #  # - 所以我们减去梯度!\n",
    " #\n",
    " #  # 实例:\n",
    " #  # w = 2.0, L = 4.0, dL/dw = 4.0\n",
    " #  #\n",
    " #  # 如果增大w: w → 2.1\n",
    " #  #   L ≈ 4.0 + 4.0×0.1 = 4.4  (变大了 ❌)\n",
    " #  #\n",
    " #  # 如果减小w: w → 1.9\n",
    " #  #   L ≈ 4.0 - 4.0×0.1 = 3.6  (变小了 ✅)\n",
    " #  #\n",
    " #  # 更新公式: w_new = 2.0 - 0.01×4.0 = 1.96\n",
    " #\n",
    " #  📊 完整的训练循环\n",
    " #\n",
    " #  model = seeback()\n",
    " #  criterion = nn.CrossEntropyLoss()\n",
    " #  optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    " #\n",
    " #  for epoch in range(100):\n",
    " #      for images, labels in dataloader:\n",
    " #\n",
    " #          # 【步骤1】前向传播\n",
    " #          logits = model(images)\n",
    " #          # PyTorch记录: logits依赖于model的所有参数\n",
    " #\n",
    " #          loss = criterion(logits, labels)\n",
    " #          # PyTorch记录: loss依赖于logits\n",
    " #\n",
    " #          # 【步骤2】反向传播 - 计算梯度\n",
    " #          optimizer.zero_grad()  # 清空之前的梯度\n",
    " #          loss.backward()        # 自动计算所有参数的梯度!\n",
    " #\n",
    " #          # 现在每个参数都有 .grad 属性了:\n",
    " #          # model.conv1.weight.grad ✓\n",
    " #          # model.conv2.weight.grad ✓\n",
    " #          # model.fc.weight.grad ✓\n",
    " #          # ...\n",
    " #\n",
    " #          # 【步骤3】参数更新 - 梯度下降\n",
    " #          optimizer.step()  # 执行: param -= lr * param.grad\n",
    " #\n",
    " #          # 相当于:\n",
    " #          # for param in model.parameters():\n",
    " #          #     param.data -= lr * param.grad\n",
    "\n"
   ],
   "id": "927362a3ec8983a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 优化器理解",
   "id": "de75a57f7cdad5f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "#      ██████████████████████████████████████████████████████████████████████\n",
    "#                          优化器完全指南\n",
    "#      ██████████████████████████████████████████████████████████████████████\n",
    "#      ======================================================================\n",
    "#      Part 1: 优化器是什么?\n",
    "#      ======================================================================\n",
    "#\n",
    "#      💡 优化器的核心作用:\n",
    "#        输入: 参数的梯度 (param.grad)\n",
    "#        输出: 参数的更新量 (如何调整参数)\n",
    "#        目标: 让 loss 尽快下降到最小值\n",
    "#\n",
    "#      基础公式:\n",
    "#        参数更新 = 参数_old - 学习率 × 梯度\n",
    "#        param_new = param_old - lr × grad\n",
    "#\n",
    "#      ======================================================================\n",
    "#      示例:手动实现优化器\n",
    "#      ======================================================================\n",
    "#\n",
    "#      初始参数: 2.0000\n",
    "#      计算得到梯度: 4.0000\n",
    "#      学习率: 0.1\n",
    "#      更新后参数: 1.6000\n",
    "#\n",
    "#      💡 优化器做的就是这件事!\n",
    "#        只不过更聪明:不是简单地减去梯度,而是用各种技巧加速收敛\n",
    "#\n",
    "#\n",
    "#      ======================================================================\n",
    "#      Part 2: SGD家族优化器详解\n",
    "#      ======================================================================\n",
    "#\n",
    "#      测试函数: Rosenbrock函数\n",
    "#      起始点: (-1.0, -1.0)\n",
    "#      目标最小值点: (1, 1)\n",
    "#\n",
    "#      ======================================================================\n",
    "#      1. 基础SGD (Stochastic Gradient Descent)\n",
    "#      ======================================================================\n",
    "#\n",
    "#      原理:\n",
    "#        param_new = param_old - lr × grad\n",
    "#\n",
    "#      特点:\n",
    "#        - 最简单的优化器\n",
    "#        - 严格按照梯度方向更新\n",
    "#        - 容易陷入局部最优\n",
    "#        - 震荡较大\n",
    "#\n",
    "#      代码:\n",
    "#        optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "#\n",
    "#        步骤0: 位置=(-0.1960, -0.6000), loss=404.0000\n",
    "#        步骤4: 位置=(-0.0797, -0.2354), loss=10.5059\n",
    "#\n",
    "#      📊 观察:\n",
    "#        - 每步严格按梯度方向移动\n",
    "#        - 没有'记忆',每步独立决策\n",
    "#\n",
    "#      ======================================================================\n",
    "#      2. SGD + Momentum (动量)\n",
    "#      ======================================================================\n",
    "#\n",
    "#      原理:\n",
    "#        velocity = momentum × velocity_old + grad\n",
    "#        param_new = param_old - lr × velocity\n",
    "#\n",
    "#      💡 类比:\n",
    "#        想象一个球从山坡滚下来:\n",
    "#        - 不仅受当前坡度影响(梯度)\n",
    "#        - 还保持之前的速度(动量)\n",
    "#        - 能冲过小山丘(避免局部最优)\n",
    "#\n",
    "#      代码:\n",
    "#        optimizer = optim.SGD(model.parameters(),\n",
    "#                              lr=0.001,\n",
    "#                              momentum=0.9)  # 保持90%的历史速度\n",
    "#\n",
    "#        步骤0: 位置=(-0.1960, -0.6000), loss=404.0000\n",
    "#        步骤4: 位置=(1.0812, 1.7860), loss=24.5593\n",
    "#\n",
    "#      参数解释:\n",
    "#        momentum ∈ [0, 1]:\n",
    "#          - 0.0: 退化为基础SGD,无动量\n",
    "#          - 0.9: 保持90%的历史速度(常用值)\n",
    "#          - 0.99: 保持99%的历史速度(用于大batch)\n",
    "#\n",
    "#      优点:\n",
    "#        ✓ 加速收敛(利用历史信息)\n",
    "#        ✓ 减少震荡(平滑梯度)\n",
    "#        ✓ 更容易跳出局部最优\n",
    "#\n",
    "#      ======================================================================\n",
    "#      3. SGD + Nesterov Momentum (Nesterov加速梯度)\n",
    "#      ======================================================================\n",
    "#\n",
    "#      原理:\n",
    "#        1. 先按动量移动到'预测位置'\n",
    "#        2. 在预测位置计算梯度\n",
    "#        3. 根据预测位置的梯度修正方向\n",
    "#\n",
    "#      💡 类比:\n",
    "#        普通动量: 看当前位置的路标\n",
    "#        Nesterov: 先往前看一步,看前面的路标(更聪明!)\n",
    "#\n",
    "#      代码:\n",
    "#        optimizer = optim.SGD(model.parameters(),\n",
    "#                              lr=0.001,\n",
    "#                              momentum=0.9,\n",
    "#                              nesterov=True)  # 启用Nesterov\n",
    "#\n",
    "#      ======================================================================\n",
    "#      4. Weight Decay (权重衰减 = L2正则化)\n",
    "#      ======================================================================\n",
    "#\n",
    "#      原理:\n",
    "#        grad_new = grad + weight_decay × param\n",
    "#        param_new = param_old - lr × grad_new\n",
    "#\n",
    "#      💡 作用:\n",
    "#        - 防止权重过大\n",
    "#        - 相当于给损失函数加上 λ||w||²\n",
    "#        - 防止过拟合\n",
    "#\n",
    "#      代码:\n",
    "#        optimizer = optim.SGD(model.parameters(),\n",
    "#                              lr=0.001,\n",
    "#                              weight_decay=1e-4)  # L2正则化系数\n",
    "#\n",
    "#      参数解释:\n",
    "#        weight_decay ∈ [0, ∞):\n",
    "#          - 0: 无正则化\n",
    "#          - 1e-5 ~ 1e-3: 常用范围\n",
    "#          - 过大: 权重被压制得太小,欠拟合\n",
    "#\n",
    "#\n",
    "#\n",
    "#      ======================================================================\n",
    "#      Part 3: 自适应学习率优化器\n",
    "#      ======================================================================\n",
    "#\n",
    "#      💡 核心思想:\n",
    "#        不同参数应该用不同的学习率!\n",
    "#        - 梯度大的参数 → 用小学习率(防止震荡)\n",
    "#        - 梯度小的参数 → 用大学习率(加速收敛)\n",
    "#\n",
    "#      ======================================================================\n",
    "#      1. Adagrad (Adaptive Gradient)\n",
    "#      ======================================================================\n",
    "#\n",
    "#      原理:\n",
    "#        sum_squared_grad += grad²\n",
    "#        adjusted_lr = lr / sqrt(sum_squared_grad + ε)\n",
    "#        param_new = param_old - adjusted_lr × grad\n",
    "#\n",
    "#      💡 特点:\n",
    "#        - 累积历史梯度的平方\n",
    "#        - 学习率会不断减小\n",
    "#        - 适合稀疏梯度(如NLP)\n",
    "#\n",
    "#      代码:\n",
    "#        optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "#\n",
    "#      优点:\n",
    "#        ✓ 自动调整学习率\n",
    "#        ✓ 适合处理稀疏数据\n",
    "#\n",
    "#      缺点:\n",
    "#        ✗ 学习率单调递减\n",
    "#        ✗ 训练后期可能太慢\n",
    "#\n",
    "#      ======================================================================\n",
    "#      2. RMSprop (Root Mean Square Propagation)\n",
    "#      ======================================================================\n",
    "#\n",
    "#      原理:\n",
    "#        squared_avg = α × squared_avg + (1-α) × grad²\n",
    "#        adjusted_lr = lr / sqrt(squared_avg + ε)\n",
    "#        param_new = param_old - adjusted_lr × grad\n",
    "#\n",
    "#      💡 改进:\n",
    "#        - 用指数移动平均代替累积和\n",
    "#        - 学习率不会无限减小\n",
    "#        - 适合RNN训练\n",
    "#\n",
    "#      代码:\n",
    "#        optimizer = optim.RMSprop(model.parameters(),\n",
    "#                                  lr=0.01,\n",
    "#                                  alpha=0.99)  # 移动平均系数\n",
    "#\n",
    "#      参数解释:\n",
    "#        alpha ∈ [0, 1]:\n",
    "#          - 0.99: 保留99%的历史信息(常用)\n",
    "#          - 0.9:  更快适应新梯度\n",
    "#\n",
    "#      ======================================================================\n",
    "#      3. Adam (Adaptive Moment Estimation) ⭐最常用⭐\n",
    "#      ======================================================================\n",
    "#\n",
    "#      原理:结合Momentum和RMSprop\n",
    "#        m = β₁ × m + (1-β₁) × grad           # 一阶矩(动量)\n",
    "#        v = β₂ × v + (1-β₂) × grad²          # 二阶矩(方差)\n",
    "#        m_hat = m / (1 - β₁^t)                # 偏差修正\n",
    "#        v_hat = v / (1 - β₂^t)\n",
    "#        param_new = param_old - lr × m_hat / (sqrt(v_hat) + ε)\n",
    "#\n",
    "#      💡 集大成者:\n",
    "#        - 有动量(利用历史梯度方向)\n",
    "#        - 有自适应学习率(根据梯度大小调整)\n",
    "#        - 有偏差修正(训练初期更准确)\n",
    "#\n",
    "#      代码:\n",
    "#        optimizer = optim.Adam(model.parameters(),\n",
    "#                               lr=0.001,\n",
    "#                               betas=(0.9, 0.999),  # (β₁, β₂)\n",
    "#                               eps=1e-8,\n",
    "#                               weight_decay=0)\n",
    "#\n",
    "#      参数解释:\n",
    "#        lr: 学习率\n",
    "#          - 0.001: 默认值,适合大多数情况\n",
    "#          - 0.0001: 微调时使用\n",
    "#\n",
    "#        betas = (β₁, β₂):\n",
    "#          - β₁=0.9: 一阶矩(动量)衰减率\n",
    "#          - β₂=0.999: 二阶矩(方差)衰减率\n",
    "#          - 通常不需要改\n",
    "#\n",
    "#        eps: 数值稳定性\n",
    "#          - 防止除零\n",
    "#          - 默认1e-8即可\n",
    "#\n",
    "#      优点:\n",
    "#        ✓ 收敛快\n",
    "#        ✓ 鲁棒性好\n",
    "#        ✓ 超参数默认值就很好用\n",
    "#        ✓ 适用范围广\n",
    "#\n",
    "#      缺点:\n",
    "#        ✗ 可能过拟合\n",
    "#        ✗ 有时泛化性不如SGD+Momentum\n",
    "#\n",
    "#      ======================================================================\n",
    "#      4. AdamW (Adam with Weight Decay) ⭐推荐⭐\n",
    "#      ======================================================================\n",
    "#\n",
    "#      原理:\n",
    "#        - Adam的改进版本\n",
    "#        - 修正了weight_decay的实现方式\n",
    "#        - 更好的泛化性能\n",
    "#\n",
    "#      代码:\n",
    "#        optimizer = optim.AdamW(model.parameters(),\n",
    "#                                lr=0.001,\n",
    "#                                weight_decay=0.01)  # 常用值\n",
    "#\n",
    "#      💡 AdamW vs Adam:\n",
    "#        Adam:   grad = grad + weight_decay × param  (错误的L2)\n",
    "#        AdamW:  param = param × (1 - weight_decay)  (正确的L2)\n",
    "#\n",
    "#      📊 现代最佳实践:\n",
    "#        - 大多数情况优先选择AdamW\n",
    "#        - weight_decay=0.01~0.1\n",
    "#\n",
    "#\n",
    "#\n",
    "#      ======================================================================\n",
    "#      Part 4: 实战对比 - 训练简单模型\n",
    "#      ======================================================================\n",
    "#\n",
    "#      任务: 100个样本,10维输入,3分类\n",
    "#      模型: 10→50→3的全连接网络\n",
    "#\n",
    "#      SGD            : 初始loss=1.1845, 最终loss=1.1569, 下降=0.0276\n",
    "#      SGD+Momentum   : 初始loss=1.1493, 最终loss=1.0858, 下降=0.0634\n",
    "#      RMSprop        : 初始loss=1.1028, 最终loss=0.5969, 下降=0.5059\n",
    "#      Adam           : 初始loss=1.1327, 最终loss=0.8924, 下降=0.2403\n",
    "#      AdamW          : 初始loss=1.1340, 最终loss=0.9212, 下降=0.2129\n",
    "#\n",
    "#      📊 观察:\n",
    "#        - Adam/AdamW通常收敛最快\n",
    "#        - SGD+Momentum比纯SGD快\n",
    "#        - 但SGD+Momentum长期训练可能泛化更好\n",
    "#\n",
    "#\n",
    "#      ======================================================================\n",
    "#      Part 5: 优化器参数完全指南\n",
    "#      ======================================================================\n",
    "#\n",
    "#      【1. 学习率 (lr / learning_rate)】\n",
    "#        最重要的超参数!\n",
    "#\n",
    "#        作用: 控制参数更新的步长\n",
    "#        param_new = param_old - lr × grad\n",
    "#\n",
    "#        选择指南:\n",
    "#          - 太大: 震荡,不收敛,loss爆炸\n",
    "#          - 太小: 收敛慢,容易卡住\n",
    "#          - 合适: 稳定下降\n",
    "#\n",
    "#        常用范围:\n",
    "#          SGD:     0.01 ~ 0.1\n",
    "#          SGD+Momentum: 0.01 ~ 0.1\n",
    "#          Adam:    0.0001 ~ 0.001\n",
    "#          AdamW:   0.0001 ~ 0.001\n",
    "#\n",
    "#        调参技巧:\n",
    "#          1. 从小开始(如1e-4)\n",
    "#          2. 观察loss曲线\n",
    "#          3. 逐步增大,直到出现震荡\n",
    "#          4. 选择震荡前的最大值\n",
    "#\n",
    "#      【2. 动量 (momentum)】\n",
    "#        用于SGD\n",
    "#\n",
    "#        作用: 保留历史梯度信息,加速收敛\n",
    "#\n",
    "#        常用值:\n",
    "#          - 0.9: 标准选择\n",
    "#          - 0.95~0.99: 大batch size时\n",
    "#\n",
    "#      【3. 权重衰减 (weight_decay)】\n",
    "#        L2正则化\n",
    "#\n",
    "#        作用: 防止过拟合,限制权重大小\n",
    "#\n",
    "#        常用范围:\n",
    "#          - 0: 无正则化\n",
    "#          - 1e-5 ~ 1e-3: 小数据集\n",
    "#          - 0.01 ~ 0.1: 大模型(如Transformer)\n",
    "#\n",
    "#        注意:\n",
    "#          - Adam配合weight_decay效果不好\n",
    "#          - 推荐用AdamW\n",
    "#\n",
    "#      【4. Betas (β₁, β₂)】\n",
    "#        用于Adam/AdamW\n",
    "#\n",
    "#        β₁: 一阶矩(动量)衰减率\n",
    "#        β₂: 二阶矩(方差)衰减率\n",
    "#\n",
    "#        默认值: (0.9, 0.999)\n",
    "#        通常不需要调整!\n",
    "#\n",
    "#        特殊情况:\n",
    "#          - NLP/大batch: β₁=0.9, β₂=0.98\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#     ======================================================================\n",
    "#      Part 6: 优化器选择实用指南\n",
    "#      ======================================================================\n",
    "#\n",
    "#      🎯 快速决策树:\n",
    "#\n",
    "#      1. 你在做什么任务?\n",
    "#\n",
    "#         【计算机视觉 (CNN)】\n",
    "#           首选: SGD + Momentum\n",
    "#             optimizer = optim.SGD(model.parameters(),\n",
    "#                                   lr=0.1,\n",
    "#                                   momentum=0.9,\n",
    "#                                   weight_decay=1e-4)\n",
    "#           原因: 泛化性能最好,业界验证\n",
    "#\n",
    "#           备选: AdamW (快速原型)\n",
    "#             optimizer = optim.AdamW(model.parameters(),\n",
    "#                                     lr=0.001,\n",
    "#                                     weight_decay=0.01)\n",
    "#\n",
    "#         【自然语言处理 (Transformer)】\n",
    "#           首选: AdamW\n",
    "#             optimizer = optim.AdamW(model.parameters(),\n",
    "#                                     lr=1e-4,\n",
    "#                                     betas=(0.9, 0.98),\n",
    "#                                     weight_decay=0.01)\n",
    "#           原因: 处理稀疏梯度好,训练稳定\n",
    "#\n",
    "#         【强化学习】\n",
    "#           首选: Adam 或 RMSprop\n",
    "#             optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "#           原因: 梯度噪声大,需要自适应学习率\n",
    "#\n",
    "#         【GAN】\n",
    "#           首选: Adam\n",
    "#             optimizer_G = optim.Adam(generator.parameters(),\n",
    "#                                      lr=0.0002, betas=(0.5, 0.999))\n",
    "#             optimizer_D = optim.Adam(discriminator.parameters(),\n",
    "#                                      lr=0.0002, betas=(0.5, 0.999))\n",
    "#           注意: β₁=0.5 (降低动量,增加稳定性)\n",
    "#\n",
    "#      ======================================================================\n",
    "#      📊 优化器对比总结表\n",
    "#      ======================================================================\n",
    "#\n",
    "#      优化器          收敛速度  泛化性能  超参数敏感度  适用场景\n",
    "#      ----------------------------------------------------------------------\n",
    "#      SGD             慢        优        高           CV大模型\n",
    "#      SGD+Momentum    中        优        中           CV通用,推荐\n",
    "#      Adagrad         中        中        低           稀疏数据\n",
    "#      RMSprop         快        中        低           RNN\n",
    "#      Adam            快        中        低           通用原型\n",
    "#      AdamW           快        优        低           NLP,大模型\n",
    "#\n",
    "#      ======================================================================\n",
    "#      🔧 调参建议\n",
    "#      ======================================================================\n",
    "#\n",
    "#      1. 学习率调整策略:\n",
    "#         - 使用学习率调度器(lr_scheduler)\n",
    "#         - 常用: StepLR, CosineAnnealingLR, ReduceLROnPlateau\n",
    "#\n",
    "#         示例:\n",
    "#           optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "#           scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "#                                                  step_size=30,\n",
    "#                                                  gamma=0.1)\n",
    "#           # 每30个epoch,学习率×0.1\n",
    "#\n",
    "#      2. 学习率预热(Warmup):\n",
    "#         - 训练初期用小学习率\n",
    "#         - 逐步增大到目标学习率\n",
    "#         - 对大batch size很重要\n",
    "#\n",
    "#      3. 梯度裁剪(Gradient Clipping):\n",
    "#         - 防止梯度爆炸\n",
    "#         - 特别是RNN/Transformer\n",
    "#\n",
    "#         代码:\n",
    "#           loss.backward()\n",
    "#           torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#           optimizer.step()\n",
    "#\n",
    "#\n",
    "#\n",
    "#      ======================================================================\n",
    "#      Part 7: 完整训练循环示例\n",
    "#      ======================================================================\n",
    "#\n",
    "#\n",
    "#      import torch\n",
    "#      import torch.nn as nn\n",
    "#      import torch.optim as optim\n",
    "#      from torch.optim.lr_scheduler import StepLR\n",
    "#\n",
    "#      # 1. 创建模型\n",
    "#      model = MyModel()\n",
    "#\n",
    "#      # 2. 选择优化器\n",
    "#      optimizer = optim.AdamW(\n",
    "#          model.parameters(),\n",
    "#          lr=0.001,           # 初始学习率\n",
    "#          weight_decay=0.01   # L2正则化\n",
    "#      )\n",
    "#\n",
    "#      # 3. 学习率调度器\n",
    "#      scheduler = StepLR(\n",
    "#          optimizer,\n",
    "#          step_size=10,  # 每10个epoch\n",
    "#          gamma=0.5      # 学习率×0.5\n",
    "#      )\n",
    "#\n",
    "#      # 4. 损失函数\n",
    "#      criterion = nn.CrossEntropyLoss()\n",
    "#\n",
    "#      # 5. 训练循环\n",
    "#      for epoch in range(100):\n",
    "#\n",
    "#          for batch_images, batch_labels in train_loader:\n",
    "#              # 5.1 前向传播\n",
    "#              outputs = model(batch_images)\n",
    "#              loss = criterion(outputs, batch_labels)\n",
    "#\n",
    "#              # 5.2 反向传播\n",
    "#              optimizer.zero_grad()  # 清空梯度\n",
    "#              loss.backward()         # 计算梯度\n",
    "#\n",
    "#              # 5.3 梯度裁剪(可选)\n",
    "#              torch.nn.utils.clip_grad_norm_(\n",
    "#                  model.parameters(),\n",
    "#                  max_norm=1.0\n",
    "#              )\n",
    "#\n",
    "#              # 5.4 参数更新\n",
    "#              optimizer.step()\n",
    "#\n",
    "#          # 5.5 学习率调整\n",
    "#          scheduler.step()\n",
    "#\n",
    "#          # 5.6 打印信息\n",
    "#          current_lr = optimizer.param_groups[0]['lr']\n",
    "#          print(f\"Epoch {epoch}: loss={loss.item():.4f}, lr={current_lr:.6f}\")\n",
    "#\n",
    "#      ======================================================================\n",
    "#      关键步骤解释:\n",
    "#      ======================================================================\n",
    "#\n",
    "#      1. optimizer.zero_grad()\n",
    "#         - 清空上一次的梯度\n",
    "#         - 必须在每次backward前调用!\n",
    "#\n",
    "#      2. loss.backward()\n",
    "#         - 计算所有参数的梯度\n",
    "#         - 梯度累积在param.grad中\n",
    "#\n",
    "#      3. optimizer.step()\n",
    "#         - 根据梯度更新参数\n",
    "#         - 实现具体的优化算法\n",
    "#\n",
    "#      4. scheduler.step()\n",
    "#         - 调整学习率\n",
    "#         - 在每个epoch结束后调用\n",
    "#\n",
    "#\n",
    "#      ======================================================================\n",
    "#      总结\n",
    "#      ======================================================================\n",
    "#\n",
    "#      🎯 记住这些关键点:\n",
    "#\n",
    "#      1. 优化器的本质\n",
    "#         - 输入: 梯度\n",
    "#         - 输出: 参数更新量\n",
    "#         - 目标: 快速找到loss最小值\n",
    "#\n",
    "#      2. 快速选择\n",
    "#         - CV任务: SGD + Momentum\n",
    "#         - NLP任务: AdamW\n",
    "#         - 快速原型: Adam\n",
    "#         - 不确定: 试试AdamW\n",
    "#\n",
    "#      3. 关键参数\n",
    "#         - lr: 最重要,需要仔细调\n",
    "#         - momentum: SGD用0.9\n",
    "#         - weight_decay: 0.01左右\n",
    "#         - Adam的betas: 用默认值\n",
    "#\n",
    "#      4. 训练技巧\n",
    "#         - 用学习率调度器\n",
    "#         - 大模型用warmup\n",
    "#         - RNN用梯度裁剪\n",
    "#         - 监控loss曲线\n",
    "#\n",
    "#      5. 记忆口诀\n",
    "#      \"梯度告诉方向,优化器决定步法\""
   ],
   "id": "c68bf196a982eaff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
